2019-03-01 10:05:36,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = clnode084.clemson.cloudlab.us/130.127.133.93
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-02-17T03:08Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-03-01 10:05:36,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-03-01 10:05:37,271 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/root/data
2019-03-01 10:05:37,477 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-03-01 10:05:37,553 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-03-01 10:05:37,553 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-03-01 10:05:37,818 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-03-01 10:05:37,822 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-03-01 10:05:37,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is clnode084.clemson.cloudlab.us
2019-03-01 10:05:37,827 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-03-01 10:05:37,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-03-01 10:05:37,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:9866
2019-03-01 10:05:37,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwidth is 10485760 bytes/s
2019-03-01 10:05:37,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-03-01 10:05:38,015 INFO org.eclipse.jetty.util.log: Logging initialized @1886ms
2019-03-01 10:05:38,133 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-03-01 10:05:38,136 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-03-01 10:05:38,141 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-03-01 10:05:38,144 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-03-01 10:05:38,144 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-03-01 10:05:38,144 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-03-01 10:05:38,168 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 41853
2019-03-01 10:05:38,169 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2019-03-01 10:05:38,204 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@48e92c5c{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-03-01 10:05:38,204 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@22356acd{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-03-01 10:05:38,275 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@366ac49b{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/datanode/,AVAILABLE}{/datanode}
2019-03-01 10:05:38,282 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@34cdeda2{HTTP/1.1,[http/1.1]}{localhost:41853}
2019-03-01 10:05:38,282 INFO org.eclipse.jetty.server.Server: Started @2154ms
2019-03-01 10:05:38,476 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864
2019-03-01 10:05:38,483 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-03-01 10:05:38,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2019-03-01 10:05:38,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-03-01 10:05:38,577 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-03-01 10:05:38,594 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9867
2019-03-01 10:05:38,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:9867
2019-03-01 10:05:38,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: mycluster
2019-03-01 10:05:38,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: mycluster
2019-03-01 10:05:38,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to node-0-link-0/10.10.1.1:8020 starting to offer service
2019-03-01 10:05:38,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to node-1-link-0/10.10.1.4:8020 starting to offer service
2019-03-01 10:05:38,687 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-03-01 10:05:38,687 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9867: starting
2019-03-01 10:05:39,771 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-01 10:05:39,771 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-01 10:05:40,771 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-01 10:05:40,771 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-01 10:05:41,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-01 10:05:41,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-01 10:05:42,773 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-01 10:05:42,773 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-01 10:05:43,773 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-01 10:05:43,773 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-01 10:05:43,996 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-03-01 10:05:44,031 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /root/data/in_use.lock acquired by nodename 803@clnode084.clemson.cloudlab.us
2019-03-01 10:05:44,033 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory with location [DISK]file:/root/data is not formatted for namespace 1814765448. Formatting...
2019-03-01 10:05:44,034 INFO org.apache.hadoop.hdfs.server.common.Storage: Generated new storageID DS-24815efc-5064-4fd1-a028-0738581a015d for directory /root/data 
2019-03-01 10:05:44,096 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-258834523-130.127.133.101-1551459924751
2019-03-01 10:05:44,096 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /root/data/current/BP-258834523-130.127.133.101-1551459924751
2019-03-01 10:05:44,097 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory for location [DISK]file:/root/data and block pool id BP-258834523-130.127.133.101-1551459924751 is not formatted. Formatting ...
2019-03-01 10:05:44,097 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-258834523-130.127.133.101-1551459924751 directory /root/data/current/BP-258834523-130.127.133.101-1551459924751/current
2019-03-01 10:05:44,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1814765448;bpid=BP-258834523-130.127.133.101-1551459924751;lv=-57;nsInfo=lv=-64;cid=CID-0840fdc4-95c6-43ff-a7dd-de4d6a452d43;nsid=1814765448;c=1551459924751;bpid=BP-258834523-130.127.133.101-1551459924751;dnuuid=null
2019-03-01 10:05:44,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID f33fa295-0841-426e-9ba0-92fb033d0f2d
2019-03-01 10:05:44,247 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-24815efc-5064-4fd1-a028-0738581a015d
2019-03-01 10:05:44,248 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - [DISK]file:/root/data, StorageType: DISK
2019-03-01 10:05:44,252 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-03-01 10:05:44,260 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for /root/data
2019-03-01 10:05:44,269 INFO org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker: Scheduled health check for volume /root/data
2019-03-01 10:05:44,271 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-258834523-130.127.133.101-1551459924751
2019-03-01 10:05:44,272 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-258834523-130.127.133.101-1551459924751 on volume /root/data...
2019-03-01 10:05:44,287 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-258834523-130.127.133.101-1551459924751 on /root/data: 16ms
2019-03-01 10:05:44,287 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-258834523-130.127.133.101-1551459924751: 16ms
2019-03-01 10:05:44,290 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-258834523-130.127.133.101-1551459924751 on volume /root/data...
2019-03-01 10:05:44,290 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /root/data/current/BP-258834523-130.127.133.101-1551459924751/current/replicas doesn't exist 
2019-03-01 10:05:44,290 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-258834523-130.127.133.101-1551459924751 on volume /root/data: 0ms
2019-03-01 10:05:44,291 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2019-03-01 10:05:44,292 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-258834523-130.127.133.101-1551459924751 on volume /root/data
2019-03-01 10:05:44,293 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/root/data, DS-24815efc-5064-4fd1-a028-0738581a015d): finished scanning block pool BP-258834523-130.127.133.101-1551459924751
2019-03-01 10:05:44,303 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 3/1/19 3:47 PM with interval of 21600000ms
2019-03-01 10:05:44,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-258834523-130.127.133.101-1551459924751 (Datanode Uuid f33fa295-0841-426e-9ba0-92fb033d0f2d) service to node-0-link-0/10.10.1.1:8020 beginning handshake with NN
2019-03-01 10:05:44,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-258834523-130.127.133.101-1551459924751 (Datanode Uuid f33fa295-0841-426e-9ba0-92fb033d0f2d) service to node-1-link-0/10.10.1.4:8020 beginning handshake with NN
2019-03-01 10:05:44,321 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/root/data, DS-24815efc-5064-4fd1-a028-0738581a015d): no suitable block pools found to scan.  Waiting 1814399971 ms.
2019-03-01 10:05:44,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-258834523-130.127.133.101-1551459924751 (Datanode Uuid f33fa295-0841-426e-9ba0-92fb033d0f2d) service to node-0-link-0/10.10.1.1:8020 successfully registered with NN
2019-03-01 10:05:44,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-258834523-130.127.133.101-1551459924751 (Datanode Uuid f33fa295-0841-426e-9ba0-92fb033d0f2d) service to node-1-link-0/10.10.1.4:8020 successfully registered with NN
2019-03-01 10:05:44,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode node-0-link-0/10.10.1.1:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-03-01 10:05:44,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode node-1-link-0/10.10.1.4:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-03-01 10:05:44,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xf7da3b2a3c184826,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 5 msec to generate and 61 msecs for RPC and NN processing. Got back no commands.
2019-03-01 10:05:44,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x7a72e8b7b277653e,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 5 msec to generate and 61 msecs for RPC and NN processing. Got back no commands.
2019-03-01 10:05:47,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-258834523-130.127.133.101-1551459924751 (Datanode Uuid f33fa295-0841-426e-9ba0-92fb033d0f2d) service to node-0-link-0/10.10.1.1:8020 trying to claim ACTIVE state with txid=1
2019-03-01 10:05:47,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-258834523-130.127.133.101-1551459924751 (Datanode Uuid f33fa295-0841-426e-9ba0-92fb033d0f2d) service to node-0-link-0/10.10.1.1:8020
2019-03-01 10:05:51,236 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741827_1003 src: /10.10.1.6:37936 dest: /10.10.1.2:9866
2019-03-01 10:05:51,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741832_1008 src: /10.10.1.6:37946 dest: /10.10.1.2:9866
2019-03-01 10:05:51,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741835_1011 src: /10.10.1.5:56716 dest: /10.10.1.2:9866
2019-03-01 10:05:51,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741839_1015 src: /10.10.1.6:37956 dest: /10.10.1.2:9866
2019-03-01 10:05:51,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741840_1016 src: /10.10.1.6:37964 dest: /10.10.1.2:9866
2019-03-01 10:05:51,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741837_1013 src: /10.10.1.6:37960 dest: /10.10.1.2:9866
2019-03-01 10:05:51,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741842_1018 src: /10.10.1.5:56724 dest: /10.10.1.2:9866
2019-03-01 10:05:51,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741838_1014 src: /10.10.1.6:37958 dest: /10.10.1.2:9866
2019-03-01 10:05:51,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741843_1019 src: /10.10.1.5:56726 dest: /10.10.1.2:9866
2019-03-01 10:05:51,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741841_1017 src: /10.10.1.3:46754 dest: /10.10.1.2:9866
2019-03-01 10:05:51,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741825_1001 src: /10.10.1.4:40138 dest: /10.10.1.2:9866
2019-03-01 10:05:51,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741829_1005 src: /10.10.1.4:40140 dest: /10.10.1.2:9866
2019-03-01 10:05:51,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741831_1007 src: /10.10.1.4:40139 dest: /10.10.1.2:9866
2019-03-01 10:05:52,993 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:37946, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-880067917_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741832_1008, duration(ns): 1604940174
2019-03-01 10:05:52,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741832_1008, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.4:9866] terminating
2019-03-01 10:05:53,027 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:37964, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1595759682_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741840_1016, duration(ns): 1642470450
2019-03-01 10:05:53,027 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741840_1016, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.4:9866] terminating
2019-03-01 10:05:53,034 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741845_1021 src: /10.10.1.6:37972 dest: /10.10.1.2:9866
2019-03-01 10:05:53,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:37958, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-310929577_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741838_1014, duration(ns): 1642705433
2019-03-01 10:05:53,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741838_1014, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.3:9866] terminating
2019-03-01 10:05:53,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40140, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_991528232_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741829_1005, duration(ns): 1646899944
2019-03-01 10:05:53,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741829_1005, type=LAST_IN_PIPELINE terminating
2019-03-01 10:05:53,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741848_1024 src: /10.10.1.6:37974 dest: /10.10.1.2:9866
2019-03-01 10:05:53,060 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741846_1022 src: /10.10.1.6:37978 dest: /10.10.1.2:9866
2019-03-01 10:05:53,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741847_1023 src: /10.10.1.4:40160 dest: /10.10.1.2:9866
2019-03-01 10:05:53,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:56726, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1281235977_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741843_1019, duration(ns): 1687335668
2019-03-01 10:05:53,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741843_1019, type=LAST_IN_PIPELINE terminating
2019-03-01 10:05:53,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40139, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-135442510_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741831_1007, duration(ns): 1669921237
2019-03-01 10:05:53,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741831_1007, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:05:53,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:37960, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-838579742_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741837_1013, duration(ns): 1701994969
2019-03-01 10:05:53,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741837_1013, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.5:9866] terminating
2019-03-01 10:05:53,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741850_1026 src: /10.10.1.5:56736 dest: /10.10.1.2:9866
2019-03-01 10:05:53,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741852_1028 src: /10.10.1.4:40164 dest: /10.10.1.2:9866
2019-03-01 10:05:53,138 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741853_1029 src: /10.10.1.6:37988 dest: /10.10.1.2:9866
2019-03-01 10:05:53,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:56716, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_605168184_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741835_1011, duration(ns): 2488927292
2019-03-01 10:05:53,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741835_1011, type=LAST_IN_PIPELINE terminating
2019-03-01 10:05:54,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:56724, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1672504390_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741842_1018, duration(ns): 2719843211
2019-03-01 10:05:54,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741842_1018, type=LAST_IN_PIPELINE terminating
2019-03-01 10:05:54,131 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741855_1031 src: /10.10.1.3:46768 dest: /10.10.1.2:9866
2019-03-01 10:05:54,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:37956, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-427296916_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741839_1015, duration(ns): 2835079887
2019-03-01 10:05:54,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741839_1015, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.4:9866] terminating
2019-03-01 10:05:54,241 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:37936, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1235593893_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741827_1003, duration(ns): 2856679747
2019-03-01 10:05:54,241 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741856_1032 src: /10.10.1.6:37994 dest: /10.10.1.2:9866
2019-03-01 10:05:54,241 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.4:9866] terminating
2019-03-01 10:05:54,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741857_1033 src: /10.10.1.3:46772 dest: /10.10.1.2:9866
2019-03-01 10:05:54,348 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46754, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-537978807_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741841_1017, duration(ns): 2957153936
2019-03-01 10:05:54,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741841_1017, type=LAST_IN_PIPELINE terminating
2019-03-01 10:05:54,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741860_1036 src: /10.10.1.3:46776 dest: /10.10.1.2:9866
2019-03-01 10:05:54,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741861_1037 src: /10.10.1.5:56744 dest: /10.10.1.2:9866
2019-03-01 10:05:54,478 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40164, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-135442510_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741852_1028, duration(ns): 1367764710
2019-03-01 10:05:54,478 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741852_1028, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:05:54,487 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741862_1038 src: /10.10.1.4:40176 dest: /10.10.1.2:9866
2019-03-01 10:05:54,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741863_1039 src: /10.10.1.6:38008 dest: /10.10.1.2:9866
2019-03-01 10:05:54,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:56736, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-533564245_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741850_1026, duration(ns): 1634006937
2019-03-01 10:05:54,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741850_1026, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:05:54,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:37988, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-838579742_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741853_1029, duration(ns): 1726166776
2019-03-01 10:05:54,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741853_1029, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.5:9866] terminating
2019-03-01 10:05:54,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741865_1041 src: /10.10.1.4:40180 dest: /10.10.1.2:9866
2019-03-01 10:05:55,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40138, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-932109594_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741825_1001, duration(ns): 3844265798
2019-03-01 10:05:55,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
2019-03-01 10:05:55,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741866_1042 src: /10.10.1.4:40182 dest: /10.10.1.2:9866
2019-03-01 10:05:55,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:37972, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-880067917_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741845_1021, duration(ns): 2242888995
2019-03-01 10:05:55,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741845_1021, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.3:9866] terminating
2019-03-01 10:05:55,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741867_1043 src: /10.10.1.6:38016 dest: /10.10.1.2:9866
2019-03-01 10:05:55,421 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:37978, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1595759682_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741846_1022, duration(ns): 2354527754
2019-03-01 10:05:55,421 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741846_1022, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.4:9866] terminating
2019-03-01 10:05:55,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741870_1046 src: /10.10.1.3:46786 dest: /10.10.1.2:9866
2019-03-01 10:05:55,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:56744, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1203371398_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741861_1037, duration(ns): 1179982026
2019-03-01 10:05:55,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741861_1037, type=LAST_IN_PIPELINE terminating
2019-03-01 10:05:55,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46768, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1672504390_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741855_1031, duration(ns): 1468015337
2019-03-01 10:05:55,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741855_1031, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.5:9866] terminating
2019-03-01 10:05:55,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40160, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-946481335_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741847_1023, duration(ns): 2572350320
2019-03-01 10:05:55,639 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741847_1023, type=LAST_IN_PIPELINE terminating
2019-03-01 10:05:55,648 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741872_1048 src: /10.10.1.6:38028 dest: /10.10.1.2:9866
2019-03-01 10:05:55,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:37974, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-310929577_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741848_1024, duration(ns): 2585340040
2019-03-01 10:05:55,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741848_1024, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.3:9866] terminating
2019-03-01 10:05:55,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741874_1050 src: /10.10.1.6:38030 dest: /10.10.1.2:9866
2019-03-01 10:05:55,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38008, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-135442510_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741863_1039, duration(ns): 1184214848
2019-03-01 10:05:55,698 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741863_1039, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.3:9866] terminating
2019-03-01 10:05:55,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741876_1052 src: /10.10.1.4:40192 dest: /10.10.1.2:9866
2019-03-01 10:05:55,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46776, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-537978807_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741860_1036, duration(ns): 1332102158
2019-03-01 10:05:55,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741860_1036, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.5:9866] terminating
2019-03-01 10:05:55,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741877_1053 src: /10.10.1.3:46796 dest: /10.10.1.2:9866
2019-03-01 10:05:55,950 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741878_1054 src: /10.10.1.3:46798 dest: /10.10.1.2:9866
2019-03-01 10:05:56,106 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741879_1055 src: /10.10.1.6:38040 dest: /10.10.1.2:9866
2019-03-01 10:05:56,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741880_1056 src: /10.10.1.3:46800 dest: /10.10.1.2:9866
2019-03-01 10:05:57,461 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40182, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-932109594_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741866_1042, duration(ns): 1349298124
2019-03-01 10:05:57,462 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741866_1042, type=LAST_IN_PIPELINE terminating
2019-03-01 10:05:57,462 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46786, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1235593893_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741870_1046, duration(ns): 1374691303
2019-03-01 10:05:57,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741870_1046, type=LAST_IN_PIPELINE terminating
2019-03-01 10:05:57,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46772, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-427296916_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741857_1033, duration(ns): 2909679029
2019-03-01 10:05:57,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741857_1033, type=LAST_IN_PIPELINE terminating
2019-03-01 10:05:57,464 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40192, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-135442510_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741876_1052, duration(ns): 1536516515
2019-03-01 10:05:57,464 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741876_1052, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:05:57,464 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46798, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_991528232_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741878_1054, duration(ns): 1477770468
2019-03-01 10:05:57,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741878_1054, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:05:57,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40180, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-838579742_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741865_1041, duration(ns): 2535455414
2019-03-01 10:05:57,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741865_1041, type=LAST_IN_PIPELINE terminating
2019-03-01 10:05:57,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741885_1061 src: /10.10.1.6:38052 dest: /10.10.1.2:9866
2019-03-01 10:05:57,500 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741882_1058 src: /10.10.1.6:38044 dest: /10.10.1.2:9866
2019-03-01 10:05:57,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46800, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1395422256_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741880_1056, duration(ns): 1502311714
2019-03-01 10:05:57,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741880_1056, type=LAST_IN_PIPELINE terminating
2019-03-01 10:05:57,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:37994, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_21269380_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741856_1032, duration(ns): 3578178720
2019-03-01 10:05:57,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741856_1032, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.5:9866] terminating
2019-03-01 10:05:57,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38030, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-946481335_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741874_1050, duration(ns): 2167500431
2019-03-01 10:05:57,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741874_1050, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.5:9866] terminating
2019-03-01 10:05:57,831 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38016, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-880067917_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741867_1043, duration(ns): 2500331934
2019-03-01 10:05:57,831 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741867_1043, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.5:9866] terminating
2019-03-01 10:05:57,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38028, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1672504390_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741872_1048, duration(ns): 2176512066
2019-03-01 10:05:57,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741872_1048, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.3:9866] terminating
2019-03-01 10:05:57,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741881_1057 src: /10.10.1.3:46806 dest: /10.10.1.2:9866
2019-03-01 10:05:57,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46796, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-537978807_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741877_1053, duration(ns): 2075967554
2019-03-01 10:05:57,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741877_1053, type=LAST_IN_PIPELINE terminating
2019-03-01 10:05:57,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741886_1062 src: /10.10.1.6:38054 dest: /10.10.1.2:9866
2019-03-01 10:05:57,855 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741892_1068 src: /10.10.1.6:38060 dest: /10.10.1.2:9866
2019-03-01 10:05:57,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741889_1065 src: /10.10.1.6:38072 dest: /10.10.1.2:9866
2019-03-01 10:05:57,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741896_1072 src: /10.10.1.4:40204 dest: /10.10.1.2:9866
2019-03-01 10:05:57,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741895_1071 src: /10.10.1.5:56768 dest: /10.10.1.2:9866
2019-03-01 10:05:57,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741891_1067 src: /10.10.1.5:56770 dest: /10.10.1.2:9866
2019-03-01 10:05:57,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741894_1070 src: /10.10.1.4:40208 dest: /10.10.1.2:9866
2019-03-01 10:05:57,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741897_1073 src: /10.10.1.3:46818 dest: /10.10.1.2:9866
2019-03-01 10:05:58,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38040, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-533564245_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741879_1055, duration(ns): 1956515331
2019-03-01 10:05:58,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741879_1055, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.5:9866] terminating
2019-03-01 10:05:58,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741898_1074 src: /10.10.1.6:38078 dest: /10.10.1.2:9866
2019-03-01 10:05:58,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741899_1075 src: /10.10.1.3:46820 dest: /10.10.1.2:9866
2019-03-01 10:05:58,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40176, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1281235977_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741862_1038, duration(ns): 3632424608
2019-03-01 10:05:58,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741862_1038, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.5:9866] terminating
2019-03-01 10:05:59,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38044, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-932109594_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741882_1058, duration(ns): 1882501403
2019-03-01 10:05:59,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741882_1058, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.4:9866] terminating
2019-03-01 10:05:59,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741901_1077 src: /10.10.1.4:40218 dest: /10.10.1.2:9866
2019-03-01 10:05:59,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38052, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-838579742_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741885_1061, duration(ns): 1671556033
2019-03-01 10:05:59,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741885_1061, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.3:9866] terminating
2019-03-01 10:05:59,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46806, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_991528232_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741881_1057, duration(ns): 1676252900
2019-03-01 10:05:59,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741881_1057, type=LAST_IN_PIPELINE terminating
2019-03-01 10:05:59,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741902_1078 src: /10.10.1.6:38088 dest: /10.10.1.2:9866
2019-03-01 10:05:59,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741903_1079 src: /10.10.1.4:40220 dest: /10.10.1.2:9866
2019-03-01 10:05:59,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:56770, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1203371398_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741891_1067, duration(ns): 1684969052
2019-03-01 10:05:59,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741891_1067, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:05:59,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:56768, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1395422256_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741895_1071, duration(ns): 1685694538
2019-03-01 10:05:59,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741895_1071, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:05:59,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38060, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1672504390_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741892_1068, duration(ns): 1687118595
2019-03-01 10:05:59,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741892_1068, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.5:9866] terminating
2019-03-01 10:05:59,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741908_1084 src: /10.10.1.6:38092 dest: /10.10.1.2:9866
2019-03-01 10:05:59,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741905_1081 src: /10.10.1.3:46826 dest: /10.10.1.2:9866
2019-03-01 10:05:59,582 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741906_1082 src: /10.10.1.3:46830 dest: /10.10.1.2:9866
2019-03-01 10:05:59,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46818, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-537978807_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741897_1073, duration(ns): 1713045200
2019-03-01 10:05:59,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741897_1073, type=LAST_IN_PIPELINE terminating
2019-03-01 10:05:59,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741909_1085 src: /10.10.1.4:40228 dest: /10.10.1.2:9866
2019-03-01 10:05:59,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741910_1086 src: /10.10.1.5:56786 dest: /10.10.1.2:9866
2019-03-01 10:05:59,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741911_1087 src: /10.10.1.4:40232 dest: /10.10.1.2:9866
2019-03-01 10:05:59,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46820, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-533564245_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741899_1075, duration(ns): 1688247438
2019-03-01 10:05:59,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741899_1075, type=LAST_IN_PIPELINE terminating
2019-03-01 10:05:59,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38078, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1595759682_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741898_1074, duration(ns): 1781311822
2019-03-01 10:05:59,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741898_1074, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.5:9866] terminating
2019-03-01 10:05:59,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741913_1089 src: /10.10.1.6:38108 dest: /10.10.1.2:9866
2019-03-01 10:06:00,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40208, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_171690136_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741894_1070, duration(ns): 2225861414
2019-03-01 10:06:00,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741894_1070, type=LAST_IN_PIPELINE terminating
2019-03-01 10:06:00,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40204, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_605168184_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741896_1072, duration(ns): 2240571221
2019-03-01 10:06:00,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741896_1072, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:06:00,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741914_1090 src: /10.10.1.3:46834 dest: /10.10.1.2:9866
2019-03-01 10:06:00,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741915_1091 src: /10.10.1.6:38110 dest: /10.10.1.2:9866
2019-03-01 10:06:00,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741916_1092 src: /10.10.1.6:38114 dest: /10.10.1.2:9866
2019-03-01 10:06:00,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:56786, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-537978807_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741910_1086, duration(ns): 1199703160
2019-03-01 10:06:00,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741910_1086, type=LAST_IN_PIPELINE terminating
2019-03-01 10:06:00,838 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741917_1093 src: /10.10.1.6:38116 dest: /10.10.1.2:9866
2019-03-01 10:06:00,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46826, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1203371398_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741905_1081, duration(ns): 1304593651
2019-03-01 10:06:00,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741905_1081, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.5:9866] terminating
2019-03-01 10:06:00,941 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741918_1094 src: /10.10.1.5:56790 dest: /10.10.1.2:9866
2019-03-01 10:06:00,972 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741919_1095 src: /10.10.1.5:56792 dest: /10.10.1.2:9866
2019-03-01 10:06:00,972 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40220, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-838579742_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741903_1079, duration(ns): 1431702229
2019-03-01 10:06:00,972 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741903_1079, type=LAST_IN_PIPELINE terminating
2019-03-01 10:06:01,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741920_1096 src: /10.10.1.6:38122 dest: /10.10.1.2:9866
2019-03-01 10:06:01,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38092, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-946481335_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741908_1084, duration(ns): 1483597198
2019-03-01 10:06:01,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741908_1084, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.5:9866] terminating
2019-03-01 10:06:01,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741921_1097 src: /10.10.1.6:38124 dest: /10.10.1.2:9866
2019-03-01 10:06:01,864 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 336ms (threshold=300ms), downstream DNs=[10.10.1.4:9866, 10.10.1.5:9866], blockId=1073741921
2019-03-01 10:06:03,498 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 1393ms (threshold=300ms), downstream DNs=[10.10.1.4:9866, 10.10.1.5:9866], blockId=1073741921
2019-03-01 10:06:03,500 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 1674ms (threshold=300ms), downstream DNs=[10.10.1.4:9866, 10.10.1.5:9866], blockId=1073741915
2019-03-01 10:06:04,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40218, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-932109594_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741901_1077, duration(ns): 2364810532
2019-03-01 10:06:04,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40228, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1235593893_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741909_1085, duration(ns): 2289593134
2019-03-01 10:06:04,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741901_1077, type=LAST_IN_PIPELINE terminating
2019-03-01 10:06:04,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741909_1085, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.5:9866] terminating
2019-03-01 10:06:04,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46834, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_171690136_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741914_1090, duration(ns): 1850998800
2019-03-01 10:06:04,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741914_1090, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.5:9866] terminating
2019-03-01 10:06:04,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46830, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1395422256_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741906_1082, duration(ns): 3910260833
2019-03-01 10:06:04,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741906_1082, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:06:04,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38108, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1595759682_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741913_1089, duration(ns): 4020042858
2019-03-01 10:06:04,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741913_1089, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.4:9866] terminating
2019-03-01 10:06:04,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38110, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_605168184_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741915_1091, duration(ns): 3807555357
2019-03-01 10:06:04,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38124, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-946481335_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741921_1097, duration(ns): 2851178156
2019-03-01 10:06:04,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741915_1091, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.5:9866] terminating
2019-03-01 10:06:04,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741921_1097, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.5:9866] terminating
2019-03-01 10:06:04,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741924_1100 src: /10.10.1.6:38132 dest: /10.10.1.2:9866
2019-03-01 10:06:06,223 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 4120ms (threshold=300ms), downstream DNs=[10.10.1.3:9866, 10.10.1.4:9866], blockId=1073741920
2019-03-01 10:06:06,223 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 4160ms (threshold=300ms), downstream DNs=[10.10.1.3:9866, 10.10.1.4:9866], blockId=1073741917
2019-03-01 10:06:06,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40232, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1281235977_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741911_1087, duration(ns): 6447143267
2019-03-01 10:06:06,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741911_1087, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:06:06,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38054, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-310929577_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741886_1062, duration(ns): 8371824347
2019-03-01 10:06:06,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741886_1062, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.5:9866] terminating
2019-03-01 10:06:06,224 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 4081ms (threshold=300ms), downstream DNs=[10.10.1.3:9866], blockId=1073741919
2019-03-01 10:06:06,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38114, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_21269380_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741916_1092, duration(ns): 5812345625
2019-03-01 10:06:06,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741916_1092, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.4:9866] terminating
2019-03-01 10:06:06,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38072, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-880067917_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741889_1065, duration(ns): 8364322321
2019-03-01 10:06:06,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741889_1065, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.4:9866] terminating
2019-03-01 10:06:06,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741922_1098 src: /10.10.1.4:40244 dest: /10.10.1.2:9866
2019-03-01 10:06:06,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741926_1102 src: /10.10.1.3:46846 dest: /10.10.1.2:9866
2019-03-01 10:06:06,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741925_1101 src: /10.10.1.3:46848 dest: /10.10.1.2:9866
2019-03-01 10:06:06,268 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741930_1106 src: /10.10.1.6:38138 dest: /10.10.1.2:9866
2019-03-01 10:06:06,268 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741929_1105 src: /10.10.1.6:38136 dest: /10.10.1.2:9866
2019-03-01 10:06:06,268 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741928_1104 src: /10.10.1.6:38142 dest: /10.10.1.2:9866
2019-03-01 10:06:06,268 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741931_1107 src: /10.10.1.6:38146 dest: /10.10.1.2:9866
2019-03-01 10:06:06,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741933_1109 src: /10.10.1.6:38152 dest: /10.10.1.2:9866
2019-03-01 10:06:06,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741935_1111 src: /10.10.1.3:46854 dest: /10.10.1.2:9866
2019-03-01 10:06:06,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741927_1103 src: /10.10.1.3:46852 dest: /10.10.1.2:9866
2019-03-01 10:06:06,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741932_1108 src: /10.10.1.3:46850 dest: /10.10.1.2:9866
2019-03-01 10:06:06,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741934_1110 src: /10.10.1.3:46856 dest: /10.10.1.2:9866
2019-03-01 10:06:06,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38088, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-427296916_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741902_1078, duration(ns): 6940101301
2019-03-01 10:06:06,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741902_1078, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.3:9866] terminating
2019-03-01 10:06:06,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741923_1099 src: /10.10.1.4:40246 dest: /10.10.1.2:9866
2019-03-01 10:06:06,517 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741937_1113 src: /10.10.1.6:38154 dest: /10.10.1.2:9866
2019-03-01 10:06:12,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:56792, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-533564245_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741919_1095, duration(ns): 6095216850
2019-03-01 10:06:12,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:56790, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1203371398_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741918_1094, duration(ns): 6653446762
2019-03-01 10:06:12,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741919_1095, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:06:12,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741918_1094, type=LAST_IN_PIPELINE terminating
2019-03-01 10:06:12,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38142, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1291260705_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741928_1104, duration(ns): 1224653991
2019-03-01 10:06:12,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40244, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-135442510_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741922_1098, duration(ns): 1492061015
2019-03-01 10:06:12,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741928_1104, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.3:9866] terminating
2019-03-01 10:06:12,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741922_1098, type=LAST_IN_PIPELINE terminating
2019-03-01 10:06:12,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46848, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-946481335_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741925_1101, duration(ns): 1799662071
2019-03-01 10:06:12,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741925_1101, type=LAST_IN_PIPELINE terminating
2019-03-01 10:06:12,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46846, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_605168184_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741926_1102, duration(ns): 1768793764
2019-03-01 10:06:12,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741926_1102, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.5:9866] terminating
2019-03-01 10:06:12,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40246, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1235593893_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741923_1099, duration(ns): 1861650167
2019-03-01 10:06:12,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741923_1099, type=LAST_IN_PIPELINE terminating
2019-03-01 10:06:12,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46850, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1395422256_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741932_1108, duration(ns): 1906346549
2019-03-01 10:06:12,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741932_1108, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.5:9866] terminating
2019-03-01 10:06:12,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38152, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1672504390_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741933_1109, duration(ns): 1929442537
2019-03-01 10:06:12,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741933_1109, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.5:9866] terminating
2019-03-01 10:06:12,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38146, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-932109594_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741931_1107, duration(ns): 2516934146
2019-03-01 10:06:12,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741931_1107, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.5:9866] terminating
2019-03-01 10:06:12,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38138, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_171690136_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741930_1106, duration(ns): 2534396869
2019-03-01 10:06:12,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741930_1106, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.3:9866] terminating
2019-03-01 10:06:12,629 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46856, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1281235977_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741934_1110, duration(ns): 4478652640
2019-03-01 10:06:12,629 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741934_1110, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:06:12,629 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38132, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1595759682_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741924_1100, duration(ns): 4276889504
2019-03-01 10:06:12,629 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741924_1100, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.5:9866] terminating
2019-03-01 10:06:12,629 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46852, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-310929577_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741927_1103, duration(ns): 4479938040
2019-03-01 10:06:12,629 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741927_1103, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:06:12,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46854, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-880067917_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741935_1111, duration(ns): 4480289099
2019-03-01 10:06:12,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741935_1111, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:06:12,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38136, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_21269380_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741929_1105, duration(ns): 4276060654
2019-03-01 10:06:12,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741929_1105, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.4:9866] terminating
2019-03-01 10:06:12,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38116, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-537978807_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741917_1093, duration(ns): 10845388663
2019-03-01 10:06:12,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741917_1093, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.4:9866] terminating
2019-03-01 10:06:12,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38154, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-427296916_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741937_1113, duration(ns): 4900996760
2019-03-01 10:06:12,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741937_1113, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.4:9866] terminating
2019-03-01 10:06:12,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38122, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-838579742_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741920_1096, duration(ns): 10683306908
2019-03-01 10:06:12,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741920_1096, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.4:9866] terminating
2019-03-01 10:06:12,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741947_1123 src: /10.10.1.6:38180 dest: /10.10.1.2:9866
2019-03-01 10:06:12,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741951_1127 src: /10.10.1.6:38188 dest: /10.10.1.2:9866
2019-03-01 10:06:12,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741941_1117 src: /10.10.1.6:38184 dest: /10.10.1.2:9866
2019-03-01 10:06:12,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741944_1120 src: /10.10.1.6:38194 dest: /10.10.1.2:9866
2019-03-01 10:06:12,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741949_1125 src: /10.10.1.6:38196 dest: /10.10.1.2:9866
2019-03-01 10:06:12,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741954_1130 src: /10.10.1.6:38174 dest: /10.10.1.2:9866
2019-03-01 10:06:12,658 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741943_1119 src: /10.10.1.3:46876 dest: /10.10.1.2:9866
2019-03-01 10:06:12,658 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741952_1128 src: /10.10.1.6:38198 dest: /10.10.1.2:9866
2019-03-01 10:06:12,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741945_1121 src: /10.10.1.3:46882 dest: /10.10.1.2:9866
2019-03-01 10:06:12,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741948_1124 src: /10.10.1.5:56810 dest: /10.10.1.2:9866
2019-03-01 10:06:13,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:56810, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_21269380_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741948_1124, duration(ns): 416308009
2019-03-01 10:06:13,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741948_1124, type=LAST_IN_PIPELINE terminating
2019-03-01 10:06:13,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46882, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_171690136_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741945_1121, duration(ns): 423135607
2019-03-01 10:06:13,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741945_1121, type=LAST_IN_PIPELINE terminating
2019-03-01 10:06:13,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741955_1131 src: /10.10.1.3:46886 dest: /10.10.1.2:9866
2019-03-01 10:06:14,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741953_1129 src: /10.10.1.4:40250 dest: /10.10.1.2:9866
2019-03-01 10:06:14,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741958_1134 src: /10.10.1.6:38206 dest: /10.10.1.2:9866
2019-03-01 10:06:14,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741957_1133 src: /10.10.1.6:38207 dest: /10.10.1.2:9866
2019-03-01 10:06:15,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741960_1136 src: /10.10.1.6:38212 dest: /10.10.1.2:9866
2019-03-01 10:06:15,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741962_1138 src: /10.10.1.4:40270 dest: /10.10.1.2:9866
2019-03-01 10:06:15,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741963_1139 src: /10.10.1.4:40272 dest: /10.10.1.2:9866
2019-03-01 10:06:15,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741961_1137 src: /10.10.1.4:40274 dest: /10.10.1.2:9866
2019-03-01 10:06:15,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741964_1140 src: /10.10.1.6:38220 dest: /10.10.1.2:9866
2019-03-01 10:06:15,600 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741965_1141 src: /10.10.1.5:56818 dest: /10.10.1.2:9866
2019-03-01 10:06:15,747 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741966_1142 src: /10.10.1.4:40276 dest: /10.10.1.2:9866
2019-03-01 10:06:16,897 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:2616ms (threshold=300ms), volume=file:/root/data/, blockId=1073741949
2019-03-01 10:06:16,898 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:2261ms (threshold=300ms), volume=file:/root/data/, blockId=1073741951
2019-03-01 10:06:16,897 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:2615ms (threshold=300ms), volume=file:/root/data/, blockId=1073741955
2019-03-01 10:06:16,897 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:2259ms (threshold=300ms), volume=file:/root/data/, blockId=1073741952
2019-03-01 10:06:16,898 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:2618ms (threshold=300ms), volume=file:/root/data/, blockId=1073741947
2019-03-01 10:06:16,898 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:2260ms (threshold=300ms), volume=file:/root/data/, blockId=1073741954
2019-03-01 10:06:16,898 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:2617ms (threshold=300ms), volume=file:/root/data/, blockId=1073741943
2019-03-01 10:06:16,898 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:2618ms (threshold=300ms), volume=file:/root/data/, blockId=1073741941
2019-03-01 10:06:16,897 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:2616ms (threshold=300ms), volume=file:/root/data/, blockId=1073741944
2019-03-01 10:06:17,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:56818, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1291260705_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741965_1141, duration(ns): 1065644892
2019-03-01 10:06:17,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741965_1141, type=LAST_IN_PIPELINE terminating
2019-03-01 10:06:18,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741967_1143 src: /10.10.1.6:38228 dest: /10.10.1.2:9866
2019-03-01 10:06:19,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46886, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_21269380_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741955_1131, duration(ns): 4734991812
2019-03-01 10:06:19,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46876, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1395422256_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741943_1119, duration(ns): 4736521885
2019-03-01 10:06:19,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741955_1131, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:06:19,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741943_1119, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:06:19,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38198, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-932109594_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741952_1128, duration(ns): 4378868870
2019-03-01 10:06:19,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741952_1128, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.5:9866] terminating
2019-03-01 10:06:19,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38194, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1203371398_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741944_1120, duration(ns): 4736400414
2019-03-01 10:06:19,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741944_1120, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.3:9866] terminating
2019-03-01 10:06:19,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741971_1147 src: /10.10.1.6:38236 dest: /10.10.1.2:9866
2019-03-01 10:06:19,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741968_1144 src: /10.10.1.3:46896 dest: /10.10.1.2:9866
2019-03-01 10:06:19,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741969_1145 src: /10.10.1.3:46898 dest: /10.10.1.2:9866
2019-03-01 10:06:25,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40272, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-880067917_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741963_1139, duration(ns): 2212200527
2019-03-01 10:06:25,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40274, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1595759682_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741961_1137, duration(ns): 2229059163
2019-03-01 10:06:25,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40276, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1235593893_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741966_1142, duration(ns): 2263053116
2019-03-01 10:06:25,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741963_1139, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:06:25,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741966_1142, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:06:25,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741961_1137, type=LAST_IN_PIPELINE terminating
2019-03-01 10:06:25,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38196, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-427296916_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741949_1125, duration(ns): 4988117898
2019-03-01 10:06:25,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741949_1125, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.4:9866] terminating
2019-03-01 10:06:25,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40270, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_991528232_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741962_1138, duration(ns): 2422319795
2019-03-01 10:06:25,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741962_1138, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:06:25,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38212, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_605168184_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741960_1136, duration(ns): 2424226614
2019-03-01 10:06:25,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741960_1136, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.3:9866] terminating
2019-03-01 10:06:25,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38206, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-135442510_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741958_1134, duration(ns): 2447095204
2019-03-01 10:06:25,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741958_1134, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.5:9866] terminating
2019-03-01 10:06:25,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38188, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-310929577_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741951_1127, duration(ns): 4716156105
2019-03-01 10:06:25,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741951_1127, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.5:9866] terminating
2019-03-01 10:06:25,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38180, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1281235977_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741947_1123, duration(ns): 5075396014
2019-03-01 10:06:25,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741947_1123, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.3:9866] terminating
2019-03-01 10:06:25,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38207, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-946481335_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741957_1133, duration(ns): 2451321920
2019-03-01 10:06:25,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741957_1133, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.4:9866] terminating
2019-03-01 10:06:25,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38220, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_171690136_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741964_1140, duration(ns): 2479311106
2019-03-01 10:06:25,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741964_1140, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.3:9866] terminating
2019-03-01 10:06:25,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40250, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-838579742_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741953_1129, duration(ns): 2495354760
2019-03-01 10:06:25,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741953_1129, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:06:25,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38184, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1672504390_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741941_1117, duration(ns): 5181320909
2019-03-01 10:06:25,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741941_1117, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.3:9866] terminating
2019-03-01 10:06:25,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38174, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-537978807_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741954_1130, duration(ns): 4870849940
2019-03-01 10:06:25,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741954_1130, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.5:9866] terminating
2019-03-01 10:06:25,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38228, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1291260705_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741967_1143, duration(ns): 691968222
2019-03-01 10:06:25,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741967_1143, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.5:9866] terminating
2019-03-01 10:06:27,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741979_1155 src: /10.10.1.6:38272 dest: /10.10.1.2:9866
2019-03-01 10:06:32,699 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1149ms (threshold=300ms), volume=file:/root/data/, blockId=1073741971
2019-03-01 10:06:32,699 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1151ms (threshold=300ms), volume=file:/root/data/, blockId=1073741969
2019-03-01 10:06:32,699 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1151ms (threshold=300ms), volume=file:/root/data/, blockId=1073741968
2019-03-01 10:06:33,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46896, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-932109594_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741968_1144, duration(ns): 7461698277
2019-03-01 10:06:33,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741968_1144, type=LAST_IN_PIPELINE terminating
2019-03-01 10:06:33,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:46898, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_21269380_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741969_1145, duration(ns): 7505069012
2019-03-01 10:06:33,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741969_1145, type=LAST_IN_PIPELINE terminating
2019-03-01 10:06:40,554 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38272, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-880067917_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741979_1155, duration(ns): 4960882592
2019-03-01 10:06:40,554 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:38236, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1395422256_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073741971_1147, duration(ns): 6817473979
2019-03-01 10:06:40,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741979_1155, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.5:9866] terminating
2019-03-01 10:06:40,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073741971_1147, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.3:9866] terminating
2019-03-01 10:06:41,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741982_1158 src: /10.10.1.3:46916 dest: /10.10.1.2:9866
2019-03-01 10:06:41,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741983_1159 src: /10.10.1.3:46918 dest: /10.10.1.2:9866
2019-03-01 10:06:44,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073741984_1160 src: /10.10.1.5:56832 dest: /10.10.1.2:9866
2019-03-01 10:06:46,801 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1818ms (threshold=300ms), volume=file:/root/data/, blockId=1073741983
2019-03-01 10:06:46,801 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1906ms (threshold=300ms), volume=file:/root/data/, blockId=1073741982
2019-03-01 10:06:47,220 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-03-01 10:06:47,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at clnode084.clemson.cloudlab.us/130.127.133.93
************************************************************/
2019-03-01 10:06:49,314 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = clnode084.clemson.cloudlab.us/130.127.133.93
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-02-17T03:08Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-03-01 10:06:49,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-03-01 10:06:49,898 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/root/data
2019-03-01 10:06:50,110 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-03-01 10:06:50,184 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-03-01 10:06:50,184 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-03-01 10:06:50,452 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-03-01 10:06:50,455 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-03-01 10:06:50,461 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is clnode084.clemson.cloudlab.us
2019-03-01 10:06:50,461 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-03-01 10:06:50,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-03-01 10:06:50,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:9866
2019-03-01 10:06:50,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwidth is 10485760 bytes/s
2019-03-01 10:06:50,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-03-01 10:06:50,612 INFO org.eclipse.jetty.util.log: Logging initialized @1826ms
2019-03-01 10:06:50,725 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-03-01 10:06:50,728 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-03-01 10:06:50,733 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-03-01 10:06:50,735 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-03-01 10:06:50,735 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-03-01 10:06:50,735 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-03-01 10:06:50,759 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 41080
2019-03-01 10:06:50,760 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2019-03-01 10:06:50,794 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@48e92c5c{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-03-01 10:06:50,795 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@22356acd{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-03-01 10:06:50,865 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@366ac49b{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/datanode/,AVAILABLE}{/datanode}
2019-03-01 10:06:50,871 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@34cdeda2{HTTP/1.1,[http/1.1]}{localhost:41080}
2019-03-01 10:06:50,871 INFO org.eclipse.jetty.server.Server: Started @2085ms
2019-03-01 10:06:51,068 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864
2019-03-01 10:06:51,075 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-03-01 10:06:51,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2019-03-01 10:06:51,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-03-01 10:06:51,164 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-03-01 10:06:51,180 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9867
2019-03-01 10:06:51,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:9867
2019-03-01 10:06:51,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: mycluster
2019-03-01 10:06:51,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: mycluster
2019-03-01 10:06:51,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to node-0-link-0/10.10.1.1:8020 starting to offer service
2019-03-01 10:06:51,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to node-1-link-0/10.10.1.4:8020 starting to offer service
2019-03-01 10:06:51,271 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-03-01 10:06:51,271 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9867: starting
2019-03-01 10:06:51,414 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-03-01 10:06:55,246 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38530. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:06:56,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38542. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:06:56,318 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38548. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:06:59,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38576. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:06:59,622 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38582. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:06:59,782 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /root/data/in_use.lock acquired by nodename 1352@clnode084.clemson.cloudlab.us
2019-03-01 10:06:59,935 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38594. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:03,110 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38624. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:04,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38634. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:05,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38640. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:05,670 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38644. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:06,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38646. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:07,392 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-258834523-130.127.133.101-1551459924751
2019-03-01 10:07:07,392 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /root/data/current/BP-258834523-130.127.133.101-1551459924751
2019-03-01 10:07:10,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1814765448;bpid=BP-258834523-130.127.133.101-1551459924751;lv=-57;nsInfo=lv=-64;cid=CID-0840fdc4-95c6-43ff-a7dd-de4d6a452d43;nsid=1814765448;c=1551459924751;bpid=BP-258834523-130.127.133.101-1551459924751;dnuuid=f33fa295-0841-426e-9ba0-92fb033d0f2d
2019-03-01 10:07:10,472 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-24815efc-5064-4fd1-a028-0738581a015d
2019-03-01 10:07:10,473 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - [DISK]file:/root/data, StorageType: DISK
2019-03-01 10:07:10,477 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-03-01 10:07:10,484 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for /root/data
2019-03-01 10:07:10,494 INFO org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker: Scheduled health check for volume /root/data
2019-03-01 10:07:10,495 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-258834523-130.127.133.101-1551459924751
2019-03-01 10:07:10,496 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-258834523-130.127.133.101-1551459924751 on volume /root/data...
2019-03-01 10:07:10,508 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /root/data/current/BP-258834523-130.127.133.101-1551459924751/current: 14879318786
2019-03-01 10:07:10,511 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-258834523-130.127.133.101-1551459924751 on /root/data: 14ms
2019-03-01 10:07:10,511 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-258834523-130.127.133.101-1551459924751: 16ms
2019-03-01 10:07:10,514 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-258834523-130.127.133.101-1551459924751 on volume /root/data...
2019-03-01 10:07:10,514 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /root/data/current/BP-258834523-130.127.133.101-1551459924751/current/replicas doesn't exist 
2019-03-01 10:07:10,554 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-258834523-130.127.133.101-1551459924751 on volume /root/data: 41ms
2019-03-01 10:07:10,555 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 41ms
2019-03-01 10:07:10,588 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/root/data, DS-24815efc-5064-4fd1-a028-0738581a015d): no suitable block pools found to scan.  Waiting 1814313704 ms.
2019-03-01 10:07:10,596 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 3/1/19 2:42 PM with interval of 21600000ms
2019-03-01 10:07:10,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool BP-258834523-130.127.133.101-1551459924751 (Datanode Uuid f33fa295-0841-426e-9ba0-92fb033d0f2d) service to node-0-link-0/10.10.1.1:8020
2019-03-01 10:07:10,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-258834523-130.127.133.101-1551459924751 (Datanode Uuid f33fa295-0841-426e-9ba0-92fb033d0f2d) service to node-0-link-0/10.10.1.1:8020 beginning handshake with NN
2019-03-01 10:07:10,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-258834523-130.127.133.101-1551459924751 (Datanode Uuid f33fa295-0841-426e-9ba0-92fb033d0f2d) service to node-1-link-0/10.10.1.4:8020 beginning handshake with NN
2019-03-01 10:07:10,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-258834523-130.127.133.101-1551459924751 (Datanode Uuid f33fa295-0841-426e-9ba0-92fb033d0f2d) service to node-0-link-0/10.10.1.1:8020 successfully registered with NN
2019-03-01 10:07:10,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-258834523-130.127.133.101-1551459924751 (Datanode Uuid f33fa295-0841-426e-9ba0-92fb033d0f2d) service to node-1-link-0/10.10.1.4:8020 successfully registered with NN
2019-03-01 10:07:10,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode node-0-link-0/10.10.1.1:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-03-01 10:07:10,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode node-1-link-0/10.10.1.4:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-03-01 10:07:10,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x9206d464d8364ddf,  containing 1 storage report(s), of which we sent 1. The reports had 113 total blocks and used 1 RPC(s). This took 6 msec to generate and 29 msecs for RPC and NN processing. Got back no commands.
2019-03-01 10:07:10,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x151ba11977516508,  containing 1 storage report(s), of which we sent 1. The reports had 113 total blocks and used 1 RPC(s). This took 6 msec to generate and 30 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-03-01 10:07:10,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-258834523-130.127.133.101-1551459924751
2019-03-01 10:07:11,718 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38660. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:14,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38670. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:15,952 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38682. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:16,993 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38692. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:17,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38694. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:18,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38712. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:19,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38718. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:19,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38724. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:20,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38732. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:22,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38756. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:23,195 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38762. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:24,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38772. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:26,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38790. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:26,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38796. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:28,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38804. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:32,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38820. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:33,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38832. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:34,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38838. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:35,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38854. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:37,928 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38866. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:38,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38870. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:39,892 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38884. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:41,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38896. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:41,486 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38904. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:43,494 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38922. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:44,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38930. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:45,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38938. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:45,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38942. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:46,308 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38950. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:49,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38964. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:50,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:38972. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:54,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39000. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:54,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39002. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:54,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39006. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:56,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39020. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:57,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39024. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:58,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39028. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:59,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39050. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:07:59,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39056. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:01,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39068. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:03,383 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39084. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:03,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39088. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:03,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39090. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:07,305 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39106. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:07,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39108. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:10,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39140. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:11,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39150. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:12,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39154. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:12,736 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39166. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:14,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39174. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:16,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39192. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:16,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39196. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:16,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39202. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:19,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39230. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:20,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39232. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:24,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39248. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:25,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39250. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:26,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39262. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:29,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39292. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:29,731 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39300. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:30,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39304. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:31,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39316. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:32,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39326. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:32,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39342. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:32,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39348. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:33,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39352. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:34,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39370. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:36,116 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39386. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:37,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39400. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:37,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39412. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:38,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39414. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:39,116 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39426. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:39,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39428. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:41,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39446. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5178 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:42,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39452. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:46,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39496. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:46,762 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39506. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:47,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39510. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:48,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39516. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:48,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39518. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:50,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39530. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:53,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39540. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:55,398 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39554. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:56,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39566. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:56,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39572. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:08:57,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39576. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:00,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39604. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:00,946 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39606. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:01,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39610. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:03,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39636. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:04,313 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39646. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:05,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39650. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:06,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39668. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:07,476 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39672. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:08,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39678. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:10,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39688. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:10,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39694. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:10,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39696. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:13,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39716. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:14,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39732. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5177 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:16,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39750. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:16,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39756. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:17,600 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39760. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:20,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39778. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:21,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39800. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:22,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39802. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:23,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39806. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:25,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39828. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:26,135 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39834. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:27,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39846. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:28,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39858. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5178 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:30,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39868. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:30,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39874. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:31,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39884. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5178 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:33,727 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39900. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:33,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39904. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:34,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39914. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:37,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39928. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:38,852 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39938. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:40,654 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741921_1097 replica FinalizedReplica, blk_1073741921_1097, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741921 for deletion
2019-03-01 10:09:40,656 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741874_1050 replica FinalizedReplica, blk_1073741874_1050, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741874 for deletion
2019-03-01 10:09:40,657 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741908_1084 replica FinalizedReplica, blk_1073741908_1084, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741908 for deletion
2019-03-01 10:09:40,657 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741925_1101 replica FinalizedReplica, blk_1073741925_1101, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741925 for deletion
2019-03-01 10:09:40,657 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741957_1133 replica FinalizedReplica, blk_1073741957_1133, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741957 for deletion
2019-03-01 10:09:40,657 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741879_1055 replica FinalizedReplica, blk_1073741879_1055, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741879 for deletion
2019-03-01 10:09:40,658 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741847_1023 replica FinalizedReplica, blk_1073741847_1023, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741847 for deletion
2019-03-01 10:09:40,658 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741850_1026 replica FinalizedReplica, blk_1073741850_1026, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741850 for deletion
2019-03-01 10:09:40,658 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741899_1075 replica FinalizedReplica, blk_1073741899_1075, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741899 for deletion
2019-03-01 10:09:40,659 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741919_1095 replica FinalizedReplica, blk_1073741919_1095, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741919 for deletion
2019-03-01 10:09:40,678 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741921_1097 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741921
2019-03-01 10:09:40,701 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741874_1050 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741874
2019-03-01 10:09:40,722 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741908_1084 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741908
2019-03-01 10:09:40,743 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741925_1101 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741925
2019-03-01 10:09:40,764 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741957_1133 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741957
2019-03-01 10:09:40,785 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741879_1055 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741879
2019-03-01 10:09:40,809 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741847_1023 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741847
2019-03-01 10:09:40,831 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741850_1026 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741850
2019-03-01 10:09:40,852 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741899_1075 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741899
2019-03-01 10:09:40,873 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741919_1095 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741919
2019-03-01 10:09:42,801 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39956. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:44,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:39976. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:46,647 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741905_1081 replica FinalizedReplica, blk_1073741905_1081, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741905 for deletion
2019-03-01 10:09:46,647 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741891_1067 replica FinalizedReplica, blk_1073741891_1067, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741891 for deletion
2019-03-01 10:09:46,648 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 replica FinalizedReplica, blk_1073741827_1003, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741827 for deletion
2019-03-01 10:09:46,648 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741923_1099 replica FinalizedReplica, blk_1073741923_1099, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741923 for deletion
2019-03-01 10:09:46,648 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741861_1037 replica FinalizedReplica, blk_1073741861_1037, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741861 for deletion
2019-03-01 10:09:46,648 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741909_1085 replica FinalizedReplica, blk_1073741909_1085, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741909 for deletion
2019-03-01 10:09:46,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741944_1120 replica FinalizedReplica, blk_1073741944_1120, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741944 for deletion
2019-03-01 10:09:46,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741918_1094 replica FinalizedReplica, blk_1073741918_1094, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741918 for deletion
2019-03-01 10:09:46,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741870_1046 replica FinalizedReplica, blk_1073741870_1046, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741870 for deletion
2019-03-01 10:09:46,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741966_1142 replica FinalizedReplica, blk_1073741966_1142, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741966 for deletion
2019-03-01 10:09:46,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741905_1081 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741905
2019-03-01 10:09:46,689 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741891_1067 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741891
2019-03-01 10:09:46,713 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741827_1003 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741827
2019-03-01 10:09:46,734 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741923_1099 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741923
2019-03-01 10:09:46,757 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741861_1037 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741861
2019-03-01 10:09:46,779 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741909_1085 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741909
2019-03-01 10:09:46,801 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741944_1120 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741944
2019-03-01 10:09:46,823 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741918_1094 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741918
2019-03-01 10:09:46,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741870_1046 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741870
2019-03-01 10:09:46,868 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741966_1142 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741966
2019-03-01 10:09:49,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:40002. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:49,648 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741926_1102 replica FinalizedReplica, blk_1073741926_1102, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741926 for deletion
2019-03-01 10:09:49,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741896_1072 replica FinalizedReplica, blk_1073741896_1072, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741896 for deletion
2019-03-01 10:09:49,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741960_1136 replica FinalizedReplica, blk_1073741960_1136, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741960 for deletion
2019-03-01 10:09:49,650 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741835_1011 replica FinalizedReplica, blk_1073741835_1011, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741835 for deletion
2019-03-01 10:09:49,650 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741915_1091 replica FinalizedReplica, blk_1073741915_1091, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741915 for deletion
2019-03-01 10:09:49,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741926_1102 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741926
2019-03-01 10:09:49,690 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741896_1072 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741896
2019-03-01 10:09:49,712 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741960_1136 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741960
2019-03-01 10:09:49,735 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741835_1011 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741835
2019-03-01 10:09:49,756 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741915_1091 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741915
2019-03-01 10:09:51,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.5:56844. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50c2 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:52,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:40026. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:55,648 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741872_1048 replica FinalizedReplica, blk_1073741872_1048, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741872 for deletion
2019-03-01 10:09:55,648 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741842_1018 replica FinalizedReplica, blk_1073741842_1018, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741842 for deletion
2019-03-01 10:09:55,648 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741892_1068 replica FinalizedReplica, blk_1073741892_1068, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741892 for deletion
2019-03-01 10:09:55,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741941_1117 replica FinalizedReplica, blk_1073741941_1117, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741941 for deletion
2019-03-01 10:09:55,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741933_1109 replica FinalizedReplica, blk_1073741933_1109, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741933 for deletion
2019-03-01 10:09:55,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741855_1031 replica FinalizedReplica, blk_1073741855_1031, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741855 for deletion
2019-03-01 10:09:55,671 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741872_1048 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741872
2019-03-01 10:09:55,694 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741842_1018 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741842
2019-03-01 10:09:55,715 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741892_1068 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741892
2019-03-01 10:09:55,737 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741941_1117 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741941
2019-03-01 10:09:55,758 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741933_1109 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741933
2019-03-01 10:09:55,781 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741855_1031 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741855
2019-03-01 10:09:57,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.4:40350. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50e1 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:58,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:40044. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:58,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.5:56848. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50e3 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:58,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.5:56854. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50c2 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:58,983 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:40062. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:09:59,443 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.4:40354. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50c3 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:10:01,648 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741952_1128 replica FinalizedReplica, blk_1073741952_1128, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741952 for deletion
2019-03-01 10:10:01,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 replica FinalizedReplica, blk_1073741825_1001, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741825 for deletion
2019-03-01 10:10:01,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741954_1130 replica FinalizedReplica, blk_1073741954_1130, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741954 for deletion
2019-03-01 10:10:01,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741860_1036 replica FinalizedReplica, blk_1073741860_1036, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741860 for deletion
2019-03-01 10:10:01,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741897_1073 replica FinalizedReplica, blk_1073741897_1073, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741897 for deletion
2019-03-01 10:10:01,650 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741866_1042 replica FinalizedReplica, blk_1073741866_1042, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741866 for deletion
2019-03-01 10:10:01,650 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741931_1107 replica FinalizedReplica, blk_1073741931_1107, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741931 for deletion
2019-03-01 10:10:01,650 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741901_1077 replica FinalizedReplica, blk_1073741901_1077, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741901 for deletion
2019-03-01 10:10:01,650 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741968_1144 replica FinalizedReplica, blk_1073741968_1144, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741968 for deletion
2019-03-01 10:10:01,650 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741841_1017 replica FinalizedReplica, blk_1073741841_1017, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741841 for deletion
2019-03-01 10:10:01,651 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741877_1053 replica FinalizedReplica, blk_1073741877_1053, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741877 for deletion
2019-03-01 10:10:01,651 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741910_1086 replica FinalizedReplica, blk_1073741910_1086, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741910 for deletion
2019-03-01 10:10:01,651 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741882_1058 replica FinalizedReplica, blk_1073741882_1058, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741882 for deletion
2019-03-01 10:10:01,651 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741917_1093 replica FinalizedReplica, blk_1073741917_1093, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741917 for deletion
2019-03-01 10:10:01,651 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Error processing datanode Command
java.io.IOException: Failed to delete 1 (out of 15) replica(s):
0) Failed to delete replica blk_1073741982_1163: GenerationStamp not matched, existing replica is blk_1073741982_1158
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.invalidate(FsDatasetImpl.java:2108)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.invalidate(FsDatasetImpl.java:2007)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActive(BPOfferService.java:734)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActor(BPOfferService.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processCommand(BPServiceActor.java:875)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:675)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:841)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:10:01,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741952_1128 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741952
2019-03-01 10:10:01,693 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741825_1001 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741825
2019-03-01 10:10:01,714 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741954_1130 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741954
2019-03-01 10:10:01,737 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741860_1036 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741860
2019-03-01 10:10:01,759 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741897_1073 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741897
2019-03-01 10:10:01,783 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741866_1042 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741866
2019-03-01 10:10:01,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741931_1107 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741931
2019-03-01 10:10:01,825 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741901_1077 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741901
2019-03-01 10:10:01,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741968_1144 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741968
2019-03-01 10:10:01,869 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741841_1017 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741841
2019-03-01 10:10:01,890 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741877_1053 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741877
2019-03-01 10:10:01,912 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741910_1086 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741910
2019-03-01 10:10:01,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:40110. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:10:01,935 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741882_1058 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741882
2019-03-01 10:10:01,957 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741917_1093 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741917
2019-03-01 10:10:02,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:40120. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:10:03,167 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:40128. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:10:03,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:40136. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50f5 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:10:04,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:40150. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:10:06,025 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.3:46978. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50e2 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:10:06,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:40204. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:10:07,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741889_1065 replica FinalizedReplica, blk_1073741889_1065, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741889 for deletion
2019-03-01 10:10:07,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741843_1019 replica FinalizedReplica, blk_1073741843_1019, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741843 for deletion
2019-03-01 10:10:07,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741845_1021 replica FinalizedReplica, blk_1073741845_1021, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741845 for deletion
2019-03-01 10:10:07,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741862_1038 replica FinalizedReplica, blk_1073741862_1038, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741862 for deletion
2019-03-01 10:10:07,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741911_1087 replica FinalizedReplica, blk_1073741911_1087, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741911 for deletion
2019-03-01 10:10:07,650 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741832_1008 replica FinalizedReplica, blk_1073741832_1008, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741832 for deletion
2019-03-01 10:10:07,650 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741867_1043 replica FinalizedReplica, blk_1073741867_1043, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741867 for deletion
2019-03-01 10:10:07,650 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741963_1139 replica FinalizedReplica, blk_1073741963_1139, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741963 for deletion
2019-03-01 10:10:07,650 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741979_1155 replica FinalizedReplica, blk_1073741979_1155, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741979 for deletion
2019-03-01 10:10:07,650 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741947_1123 replica FinalizedReplica, blk_1073741947_1123, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741947 for deletion
2019-03-01 10:10:07,650 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741934_1110 replica FinalizedReplica, blk_1073741934_1110, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741934 for deletion
2019-03-01 10:10:07,651 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741935_1111 replica FinalizedReplica, blk_1073741935_1111, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741935 for deletion
2019-03-01 10:10:07,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741889_1065 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741889
2019-03-01 10:10:07,694 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741843_1019 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741843
2019-03-01 10:10:07,717 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741845_1021 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741845
2019-03-01 10:10:07,739 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741862_1038 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741862
2019-03-01 10:10:07,761 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741911_1087 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741911
2019-03-01 10:10:07,785 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741832_1008 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741832
2019-03-01 10:10:07,808 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741867_1043 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741867
2019-03-01 10:10:07,829 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741963_1139 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741963
2019-03-01 10:10:07,849 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741979_1155 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741979
2019-03-01 10:10:07,870 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741947_1123 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741947
2019-03-01 10:10:07,892 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741934_1110 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741934
2019-03-01 10:10:07,913 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741935_1111 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741935
2019-03-01 10:10:07,959 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.3:47000. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50c3 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:10:11,101 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-03-01 10:10:11,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at clnode084.clemson.cloudlab.us/130.127.133.93
************************************************************/
2019-03-01 10:10:13,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = clnode084.clemson.cloudlab.us/130.127.133.93
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-02-17T03:08Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-03-01 10:10:13,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-03-01 10:10:13,795 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/root/data
2019-03-01 10:10:14,005 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-03-01 10:10:14,084 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-03-01 10:10:14,084 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-03-01 10:10:14,350 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-03-01 10:10:14,354 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-03-01 10:10:14,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is clnode084.clemson.cloudlab.us
2019-03-01 10:10:14,360 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-03-01 10:10:14,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-03-01 10:10:14,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:9866
2019-03-01 10:10:14,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwidth is 10485760 bytes/s
2019-03-01 10:10:14,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-03-01 10:10:14,507 INFO org.eclipse.jetty.util.log: Logging initialized @1821ms
2019-03-01 10:10:14,617 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-03-01 10:10:14,620 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-03-01 10:10:14,625 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-03-01 10:10:14,627 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-03-01 10:10:14,627 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-03-01 10:10:14,627 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-03-01 10:10:14,650 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 44055
2019-03-01 10:10:14,652 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2019-03-01 10:10:14,686 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@48e92c5c{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-03-01 10:10:14,687 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@22356acd{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-03-01 10:10:14,757 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@366ac49b{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/datanode/,AVAILABLE}{/datanode}
2019-03-01 10:10:14,764 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@34cdeda2{HTTP/1.1,[http/1.1]}{localhost:44055}
2019-03-01 10:10:14,764 INFO org.eclipse.jetty.server.Server: Started @2079ms
2019-03-01 10:10:14,964 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864
2019-03-01 10:10:14,970 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-03-01 10:10:15,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2019-03-01 10:10:15,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-03-01 10:10:15,069 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-03-01 10:10:15,085 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9867
2019-03-01 10:10:15,127 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:9867
2019-03-01 10:10:15,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: mycluster
2019-03-01 10:10:15,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: mycluster
2019-03-01 10:10:15,167 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to node-0-link-0/10.10.1.1:8020 starting to offer service
2019-03-01 10:10:15,167 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to node-1-link-0/10.10.1.4:8020 starting to offer service
2019-03-01 10:10:15,175 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-03-01 10:10:15,175 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9867: starting
2019-03-01 10:10:15,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to node-0-link-0/10.10.1.1:8020
2019-03-01 10:10:15,324 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-03-01 10:10:15,355 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /root/data/in_use.lock acquired by nodename 1809@clnode084.clemson.cloudlab.us
2019-03-01 10:10:15,397 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-258834523-130.127.133.101-1551459924751
2019-03-01 10:10:15,397 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /root/data/current/BP-258834523-130.127.133.101-1551459924751
2019-03-01 10:10:15,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1814765448;bpid=BP-258834523-130.127.133.101-1551459924751;lv=-57;nsInfo=lv=-64;cid=CID-0840fdc4-95c6-43ff-a7dd-de4d6a452d43;nsid=1814765448;c=1551459924751;bpid=BP-258834523-130.127.133.101-1551459924751;dnuuid=f33fa295-0841-426e-9ba0-92fb033d0f2d
2019-03-01 10:10:15,487 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-24815efc-5064-4fd1-a028-0738581a015d
2019-03-01 10:10:15,488 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - [DISK]file:/root/data, StorageType: DISK
2019-03-01 10:10:15,491 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-03-01 10:10:15,499 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for /root/data
2019-03-01 10:10:15,508 INFO org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker: Scheduled health check for volume /root/data
2019-03-01 10:10:15,510 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-258834523-130.127.133.101-1551459924751
2019-03-01 10:10:15,511 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-258834523-130.127.133.101-1551459924751 on volume /root/data...
2019-03-01 10:10:15,521 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /root/data/current/BP-258834523-130.127.133.101-1551459924751/current: 7169139059
2019-03-01 10:10:15,524 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-258834523-130.127.133.101-1551459924751 on /root/data: 13ms
2019-03-01 10:10:15,524 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-258834523-130.127.133.101-1551459924751: 13ms
2019-03-01 10:10:15,527 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-258834523-130.127.133.101-1551459924751 on volume /root/data...
2019-03-01 10:10:15,527 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /root/data/current/BP-258834523-130.127.133.101-1551459924751/current/replicas doesn't exist 
2019-03-01 10:10:15,551 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-258834523-130.127.133.101-1551459924751 on volume /root/data: 24ms
2019-03-01 10:10:15,551 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 25ms
2019-03-01 10:10:15,584 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/root/data, DS-24815efc-5064-4fd1-a028-0738581a015d): no suitable block pools found to scan.  Waiting 1814128708 ms.
2019-03-01 10:10:15,592 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 3/1/19 10:45 AM with interval of 21600000ms
2019-03-01 10:10:15,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-258834523-130.127.133.101-1551459924751 (Datanode Uuid f33fa295-0841-426e-9ba0-92fb033d0f2d) service to node-0-link-0/10.10.1.1:8020 beginning handshake with NN
2019-03-01 10:10:15,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-258834523-130.127.133.101-1551459924751 (Datanode Uuid f33fa295-0841-426e-9ba0-92fb033d0f2d) service to node-1-link-0/10.10.1.4:8020 beginning handshake with NN
2019-03-01 10:10:15,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-258834523-130.127.133.101-1551459924751 (Datanode Uuid f33fa295-0841-426e-9ba0-92fb033d0f2d) service to node-1-link-0/10.10.1.4:8020 successfully registered with NN
2019-03-01 10:10:15,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-258834523-130.127.133.101-1551459924751 (Datanode Uuid f33fa295-0841-426e-9ba0-92fb033d0f2d) service to node-0-link-0/10.10.1.1:8020 successfully registered with NN
2019-03-01 10:10:15,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode node-1-link-0/10.10.1.4:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-03-01 10:10:15,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode node-0-link-0/10.10.1.1:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-03-01 10:10:15,672 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xdd281fab8dea356e,  containing 1 storage report(s), of which we sent 1. The reports had 56 total blocks and used 1 RPC(s). This took 5 msec to generate and 29 msecs for RPC and NN processing. Got back no commands.
2019-03-01 10:10:15,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x80942067271ebd63,  containing 1 storage report(s), of which we sent 1. The reports had 56 total blocks and used 1 RPC(s). This took 5 msec to generate and 29 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-03-01 10:10:15,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-258834523-130.127.133.101-1551459924751
2019-03-01 10:10:16,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742058_1237 src: /10.10.1.5:56946 dest: /10.10.1.2:9866
2019-03-01 10:10:18,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:56946, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1074057617_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742058_1237, duration(ns): 659817503
2019-03-01 10:10:18,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742058_1237, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:10:22,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742061_1240 src: /10.10.1.6:40406 dest: /10.10.1.2:9866
2019-03-01 10:10:22,757 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742062_1241 src: /10.10.1.4:40428 dest: /10.10.1.2:9866
2019-03-01 10:10:23,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:40406, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1214116148_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742061_1240, duration(ns): 475677025
2019-03-01 10:10:23,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742061_1240, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.5:9866] terminating
2019-03-01 10:10:23,323 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742063_1242 src: /10.10.1.4:40432 dest: /10.10.1.2:9866
2019-03-01 10:10:23,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40428, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1074057617_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742062_1241, duration(ns): 788210632
2019-03-01 10:10:23,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742062_1241, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:10:23,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742064_1243 src: /10.10.1.3:47032 dest: /10.10.1.2:9866
2019-03-01 10:10:23,775 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40432, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1214116148_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742063_1242, duration(ns): 450262051
2019-03-01 10:10:23,775 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742063_1242, type=LAST_IN_PIPELINE terminating
2019-03-01 10:10:23,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742065_1244 src: /10.10.1.4:40434 dest: /10.10.1.2:9866
2019-03-01 10:10:24,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47032, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1074057617_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742064_1243, duration(ns): 588497032
2019-03-01 10:10:24,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742064_1243, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.5:9866] terminating
2019-03-01 10:10:24,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742066_1245 src: /10.10.1.6:40428 dest: /10.10.1.2:9866
2019-03-01 10:10:24,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40434, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1214116148_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742065_1244, duration(ns): 473627438
2019-03-01 10:10:24,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742065_1244, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.5:9866] terminating
2019-03-01 10:10:24,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742067_1246 src: /10.10.1.5:56954 dest: /10.10.1.2:9866
2019-03-01 10:10:24,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741857_1033 replica FinalizedReplica, blk_1073741857_1033, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741857 for deletion
2019-03-01 10:10:24,621 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741937_1113 replica FinalizedReplica, blk_1073741937_1113, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741937 for deletion
2019-03-01 10:10:24,622 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741927_1103 replica FinalizedReplica, blk_1073741927_1103, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741927 for deletion
2019-03-01 10:10:24,622 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741848_1024 replica FinalizedReplica, blk_1073741848_1024, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741848 for deletion
2019-03-01 10:10:24,622 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741949_1125 replica FinalizedReplica, blk_1073741949_1125, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741949 for deletion
2019-03-01 10:10:24,622 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741902_1078 replica FinalizedReplica, blk_1073741902_1078, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741902 for deletion
2019-03-01 10:10:24,623 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741838_1014 replica FinalizedReplica, blk_1073741838_1014, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741838 for deletion
2019-03-01 10:10:24,623 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741886_1062 replica FinalizedReplica, blk_1073741886_1062, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741886 for deletion
2019-03-01 10:10:24,623 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741839_1015 replica FinalizedReplica, blk_1073741839_1015, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741839 for deletion
2019-03-01 10:10:24,624 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741951_1127 replica FinalizedReplica, blk_1073741951_1127, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741951 for deletion
2019-03-01 10:10:24,624 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Error processing datanode Command
java.io.IOException: Failed to delete 1 (out of 11) replica(s):
0) Failed to delete replica blk_1073741984_1161: GenerationStamp not matched, existing replica is blk_1073741984_1160
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.invalidate(FsDatasetImpl.java:2108)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.invalidate(FsDatasetImpl.java:2007)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActive(BPOfferService.java:734)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActor(BPOfferService.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processCommand(BPServiceActor.java:875)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:675)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:841)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:10:24,646 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741857_1033 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741857
2019-03-01 10:10:24,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741937_1113 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741937
2019-03-01 10:10:24,689 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741927_1103 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741927
2019-03-01 10:10:24,722 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741848_1024 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741848
2019-03-01 10:10:24,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:40428, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1074057617_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742066_1245, duration(ns): 473471077
2019-03-01 10:10:24,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742066_1245, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.4:9866] terminating
2019-03-01 10:10:24,751 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741949_1125 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741949
2019-03-01 10:10:24,772 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741902_1078 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741902
2019-03-01 10:10:24,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741838_1014 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741838
2019-03-01 10:10:24,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:56954, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1214116148_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742067_1246, duration(ns): 450540603
2019-03-01 10:10:24,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742067_1246, type=LAST_IN_PIPELINE terminating
2019-03-01 10:10:24,821 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741886_1062 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741886
2019-03-01 10:10:24,847 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741839_1015 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741839
2019-03-01 10:10:24,869 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741951_1127 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741951
2019-03-01 10:10:25,285 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742070_1249 src: /10.10.1.6:40440 dest: /10.10.1.2:9866
2019-03-01 10:10:25,450 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742071_1250 src: /10.10.1.6:40446 dest: /10.10.1.2:9866
2019-03-01 10:10:25,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:40440, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1074057617_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742070_1249, duration(ns): 455803531
2019-03-01 10:10:25,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742070_1249, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.4:9866] terminating
2019-03-01 10:10:25,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742072_1251 src: /10.10.1.5:56962 dest: /10.10.1.2:9866
2019-03-01 10:10:26,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:40446, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1214116148_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742071_1250, duration(ns): 704006964
2019-03-01 10:10:26,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742071_1250, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.5:9866] terminating
2019-03-01 10:10:26,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:56962, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1074057617_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742072_1251, duration(ns): 454294067
2019-03-01 10:10:26,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742072_1251, type=LAST_IN_PIPELINE terminating
2019-03-01 10:10:28,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742073_1252 src: /10.10.1.6:40478 dest: /10.10.1.2:9866
2019-03-01 10:10:28,731 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:40478, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1369072303_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742073_1252, duration(ns): 470966172
2019-03-01 10:10:28,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742073_1252, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.5:9866] terminating
2019-03-01 10:10:29,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742075_1254 src: /10.10.1.3:47046 dest: /10.10.1.2:9866
2019-03-01 10:10:29,804 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47046, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1369072303_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742075_1254, duration(ns): 444978112
2019-03-01 10:10:29,804 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742075_1254, type=LAST_IN_PIPELINE terminating
2019-03-01 10:10:29,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742076_1255 src: /10.10.1.3:47048 dest: /10.10.1.2:9866
2019-03-01 10:10:29,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742077_1256 src: /10.10.1.4:40442 dest: /10.10.1.2:9866
2019-03-01 10:10:30,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40442, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1369072303_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742077_1256, duration(ns): 432403068
2019-03-01 10:10:30,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742077_1256, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.5:9866] terminating
2019-03-01 10:10:30,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742078_1257 src: /10.10.1.5:56968 dest: /10.10.1.2:9866
2019-03-01 10:10:30,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47048, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1879160607_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742076_1255, duration(ns): 598272374
2019-03-01 10:10:30,414 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742076_1255, type=LAST_IN_PIPELINE terminating
2019-03-01 10:10:30,720 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:56968, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1369072303_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742078_1257, duration(ns): 382865858
2019-03-01 10:10:30,720 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742078_1257, type=LAST_IN_PIPELINE terminating
2019-03-01 10:10:30,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742080_1259 src: /10.10.1.4:40446 dest: /10.10.1.2:9866
2019-03-01 10:10:30,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742081_1260 src: /10.10.1.6:40508 dest: /10.10.1.2:9866
2019-03-01 10:10:31,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40446, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1369072303_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742080_1259, duration(ns): 379455674
2019-03-01 10:10:31,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742080_1259, type=LAST_IN_PIPELINE terminating
2019-03-01 10:10:31,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742082_1261 src: /10.10.1.5:56972 dest: /10.10.1.2:9866
2019-03-01 10:10:31,340 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:40508, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1879160607_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742081_1260, duration(ns): 427781770
2019-03-01 10:10:31,340 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742081_1260, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.5:9866] terminating
2019-03-01 10:10:31,383 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742083_1262 src: /10.10.1.3:47058 dest: /10.10.1.2:9866
2019-03-01 10:10:31,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:56972, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1369072303_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742082_1261, duration(ns): 444061612
2019-03-01 10:10:31,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742082_1261, type=LAST_IN_PIPELINE terminating
2019-03-01 10:10:31,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742084_1263 src: /10.10.1.3:47060 dest: /10.10.1.2:9866
2019-03-01 10:10:31,812 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47058, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1879160607_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742083_1262, duration(ns): 425891521
2019-03-01 10:10:31,812 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742083_1262, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.5:9866] terminating
2019-03-01 10:10:31,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742085_1264 src: /10.10.1.6:40524 dest: /10.10.1.2:9866
2019-03-01 10:10:32,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47060, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1369072303_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742084_1263, duration(ns): 378447953
2019-03-01 10:10:32,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742084_1263, type=LAST_IN_PIPELINE terminating
2019-03-01 10:10:32,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:40524, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1879160607_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742085_1264, duration(ns): 414479043
2019-03-01 10:10:32,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742085_1264, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.4:9866] terminating
2019-03-01 10:10:32,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742086_1265 src: /10.10.1.4:40450 dest: /10.10.1.2:9866
2019-03-01 10:10:32,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40450, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1879160607_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742086_1265, duration(ns): 377399240
2019-03-01 10:10:32,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742086_1265, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:10:33,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742088_1267 src: /10.10.1.3:47064 dest: /10.10.1.2:9866
2019-03-01 10:10:33,950 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47064, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1879160607_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742088_1267, duration(ns): 747795629
2019-03-01 10:10:33,950 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742088_1267, type=LAST_IN_PIPELINE terminating
2019-03-01 10:11:03,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741928_1104 replica FinalizedReplica, blk_1073741928_1104, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741928 for deletion
2019-03-01 10:11:03,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741965_1141 replica FinalizedReplica, blk_1073741965_1141, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741965 for deletion
2019-03-01 10:11:03,617 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741967_1143 replica FinalizedReplica, blk_1073741967_1143, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741967 for deletion
2019-03-01 10:11:03,637 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741928_1104 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741928
2019-03-01 10:11:03,658 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741965_1141 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741965
2019-03-01 10:11:03,677 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741967_1143 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741967
2019-03-01 10:11:09,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742089_1268 src: /10.10.1.6:40820 dest: /10.10.1.2:9866
2019-03-01 10:11:09,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741894_1070 replica FinalizedReplica, blk_1073741894_1070, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741894 for deletion
2019-03-01 10:11:09,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741945_1121 replica FinalizedReplica, blk_1073741945_1121, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741945 for deletion
2019-03-01 10:11:09,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741914_1090 replica FinalizedReplica, blk_1073741914_1090, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741914 for deletion
2019-03-01 10:11:09,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741930_1106 replica FinalizedReplica, blk_1073741930_1106, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741930 for deletion
2019-03-01 10:11:09,617 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741964_1140 replica FinalizedReplica, blk_1073741964_1140, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741964 for deletion
2019-03-01 10:11:14,766 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:3968ms (threshold=300ms), volume=file:/root/data/, blockId=1073742089
2019-03-01 10:11:14,766 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741894_1070 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741894
2019-03-01 10:11:14,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741945_1121 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741945
2019-03-01 10:11:14,809 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741914_1090 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741914
2019-03-01 10:11:14,831 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741930_1106 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741930
2019-03-01 10:11:14,852 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741964_1140 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741964
2019-03-01 10:11:16,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742090_1269 src: /10.10.1.4:40462 dest: /10.10.1.2:9866
2019-03-01 10:11:19,624 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:40820, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1490969395_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742089_1268, duration(ns): 5340675452
2019-03-01 10:11:19,624 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742089_1268, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.5:9866] terminating
2019-03-01 10:11:19,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742091_1270 src: /10.10.1.6:40886 dest: /10.10.1.2:9866
2019-03-01 10:11:21,617 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741920_1096 replica FinalizedReplica, blk_1073741920_1096, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741920 for deletion
2019-03-01 10:11:21,617 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741953_1129 replica FinalizedReplica, blk_1073741953_1129, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741953 for deletion
2019-03-01 10:11:21,617 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741895_1071 replica FinalizedReplica, blk_1073741895_1071, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741895 for deletion
2019-03-01 10:11:21,617 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741865_1041 replica FinalizedReplica, blk_1073741865_1041, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741865 for deletion
2019-03-01 10:11:21,618 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741932_1108 replica FinalizedReplica, blk_1073741932_1108, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741932 for deletion
2019-03-01 10:11:21,618 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741837_1013 replica FinalizedReplica, blk_1073741837_1013, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741837 for deletion
2019-03-01 10:11:21,618 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741903_1079 replica FinalizedReplica, blk_1073741903_1079, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741903 for deletion
2019-03-01 10:11:21,618 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741906_1082 replica FinalizedReplica, blk_1073741906_1082, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741906 for deletion
2019-03-01 10:11:21,618 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741971_1147 replica FinalizedReplica, blk_1073741971_1147, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741971 for deletion
2019-03-01 10:11:21,619 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741943_1119 replica FinalizedReplica, blk_1073741943_1119, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741943 for deletion
2019-03-01 10:11:21,619 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741880_1056 replica FinalizedReplica, blk_1073741880_1056, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741880 for deletion
2019-03-01 10:11:21,619 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741853_1029 replica FinalizedReplica, blk_1073741853_1029, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741853 for deletion
2019-03-01 10:11:21,619 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741885_1061 replica FinalizedReplica, blk_1073741885_1061, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741885 for deletion
2019-03-01 10:11:21,619 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Error processing datanode Command
java.io.IOException: Failed to delete 1 (out of 14) replica(s):
0) Failed to delete replica blk_1073741983_1162: GenerationStamp not matched, existing replica is blk_1073741983_1159
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.invalidate(FsDatasetImpl.java:2108)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.invalidate(FsDatasetImpl.java:2007)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActive(BPOfferService.java:734)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActor(BPOfferService.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processCommand(BPServiceActor.java:875)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:675)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:841)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:11:21,638 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741920_1096 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741920
2019-03-01 10:11:21,659 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741953_1129 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741953
2019-03-01 10:11:21,680 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741895_1071 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741895
2019-03-01 10:11:21,878 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741865_1041 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741865
2019-03-01 10:11:24,200 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741932_1108 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741932
2019-03-01 10:11:24,200 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1667ms (threshold=300ms), volume=file:/root/data/, blockId=1073742090
2019-03-01 10:11:24,226 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741837_1013 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741837
2019-03-01 10:11:24,248 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741903_1079 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741903
2019-03-01 10:11:24,269 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741906_1082 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741906
2019-03-01 10:11:24,290 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741971_1147 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741971
2019-03-01 10:11:24,312 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741943_1119 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741943
2019-03-01 10:11:24,334 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741880_1056 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741880
2019-03-01 10:11:24,358 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741853_1029 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741853
2019-03-01 10:11:24,380 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741885_1061 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741885
2019-03-01 10:11:24,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741840_1016 replica FinalizedReplica, blk_1073741840_1016, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741840 for deletion
2019-03-01 10:11:24,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741924_1100 replica FinalizedReplica, blk_1073741924_1100, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741924 for deletion
2019-03-01 10:11:24,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741846_1022 replica FinalizedReplica, blk_1073741846_1022, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741846 for deletion
2019-03-01 10:11:24,617 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741913_1089 replica FinalizedReplica, blk_1073741913_1089, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741913 for deletion
2019-03-01 10:11:24,617 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741961_1137 replica FinalizedReplica, blk_1073741961_1137, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741961 for deletion
2019-03-01 10:11:24,617 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741898_1074 replica FinalizedReplica, blk_1073741898_1074, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741898 for deletion
2019-03-01 10:11:24,640 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741840_1016 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741840
2019-03-01 10:11:24,660 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741924_1100 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741924
2019-03-01 10:11:24,683 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741846_1022 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741846
2019-03-01 10:11:24,703 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741913_1089 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741913
2019-03-01 10:11:24,725 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741961_1137 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741961
2019-03-01 10:11:24,746 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741898_1074 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741898
2019-03-01 10:11:27,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40462, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1061945758_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742090_1269, duration(ns): 6224003216
2019-03-01 10:11:27,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742090_1269, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.5:9866] terminating
2019-03-01 10:11:27,820 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741856_1032 replica FinalizedReplica, blk_1073741856_1032, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741856 for deletion
2019-03-01 10:11:27,821 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741969_1145 replica FinalizedReplica, blk_1073741969_1145, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741969 for deletion
2019-03-01 10:11:27,821 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741955_1131 replica FinalizedReplica, blk_1073741955_1131, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741955 for deletion
2019-03-01 10:11:27,821 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741929_1105 replica FinalizedReplica, blk_1073741929_1105, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741929 for deletion
2019-03-01 10:11:27,821 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741916_1092 replica FinalizedReplica, blk_1073741916_1092, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741916 for deletion
2019-03-01 10:11:27,821 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741948_1124 replica FinalizedReplica, blk_1073741948_1124, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741948 for deletion
2019-03-01 10:11:27,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741856_1032 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741856
2019-03-01 10:11:27,865 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741969_1145 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741969
2019-03-01 10:11:27,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742092_1271 src: /10.10.1.6:40944 dest: /10.10.1.2:9866
2019-03-01 10:11:27,886 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741955_1131 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741955
2019-03-01 10:11:27,907 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741929_1105 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741929
2019-03-01 10:11:27,928 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741916_1092 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741916
2019-03-01 10:11:27,949 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741948_1124 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741948
2019-03-01 10:11:29,794 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 3927ms (threshold=300ms), downstream DNs=[10.10.1.3:9866, 10.10.1.5:9866], blockId=1073742091
2019-03-01 10:11:33,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742094_1273 src: /10.10.1.5:56980 dest: /10.10.1.2:9866
2019-03-01 10:11:33,344 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 3274ms (threshold=300ms), downstream DNs=[10.10.1.3:9866, 10.10.1.5:9866], blockId=1073742091
2019-03-01 10:11:33,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742093_1272 src: /10.10.1.3:47068 dest: /10.10.1.2:9866
2019-03-01 10:11:33,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742095_1274 src: /10.10.1.4:40466 dest: /10.10.1.2:9866
2019-03-01 10:11:33,486 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 3677ms (threshold=300ms), downstream DNs=[10.10.1.4:9866, 10.10.1.3:9866], blockId=1073742092
2019-03-01 10:11:33,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47068, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1079160607_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742093_1272, duration(ns): 532548538
2019-03-01 10:11:33,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742093_1272, type=LAST_IN_PIPELINE terminating
2019-03-01 10:11:33,928 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742096_1275 src: /10.10.1.3:47070 dest: /10.10.1.2:9866
2019-03-01 10:11:33,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:40944, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1061945758_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742092_1271, duration(ns): 4181314649
2019-03-01 10:11:33,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742092_1271, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.3:9866] terminating
2019-03-01 10:11:34,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:56980, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_843036333_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742094_1273, duration(ns): 596729780
2019-03-01 10:11:34,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742094_1273, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:11:34,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47070, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1079160607_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742096_1275, duration(ns): 417837018
2019-03-01 10:11:34,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742096_1275, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:11:34,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742098_1277 src: /10.10.1.3:47072 dest: /10.10.1.2:9866
2019-03-01 10:11:38,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40466, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1025064869_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742095_1274, duration(ns): 4818681092
2019-03-01 10:11:38,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742097_1276 src: /10.10.1.5:56982 dest: /10.10.1.2:9866
2019-03-01 10:11:38,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742095_1274, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.5:9866] terminating
2019-03-01 10:11:38,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:40886, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1490969395_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742091_1270, duration(ns): 12443005722
2019-03-01 10:11:38,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742091_1270, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.5:9866] terminating
2019-03-01 10:11:38,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742099_1278 src: /10.10.1.4:40468 dest: /10.10.1.2:9866
2019-03-01 10:11:38,353 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742102_1281 src: /10.10.1.5:56986 dest: /10.10.1.2:9866
2019-03-01 10:11:38,353 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742100_1279 src: /10.10.1.5:56988 dest: /10.10.1.2:9866
2019-03-01 10:11:39,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:56986, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1490969395_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742102_1281, duration(ns): 1086678915
2019-03-01 10:11:39,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742102_1281, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:11:39,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47072, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1079160607_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742098_1277, duration(ns): 1137078156
2019-03-01 10:11:39,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742098_1277, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.5:9866] terminating
2019-03-01 10:11:39,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742103_1282 src: /10.10.1.3:47076 dest: /10.10.1.2:9866
2019-03-01 10:11:39,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742104_1283 src: /10.10.1.3:47078 dest: /10.10.1.2:9866
2019-03-01 10:11:39,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:56988, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_843036333_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742100_1279, duration(ns): 1222706343
2019-03-01 10:11:39,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742100_1279, type=LAST_IN_PIPELINE terminating
2019-03-01 10:11:39,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:56982, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1061945758_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742097_1276, duration(ns): 1366612459
2019-03-01 10:11:39,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742097_1276, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:11:39,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40468, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_415564948_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742099_1278, duration(ns): 1386415515
2019-03-01 10:11:39,698 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742099_1278, type=LAST_IN_PIPELINE terminating
2019-03-01 10:11:39,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742107_1286 src: /10.10.1.3:47084 dest: /10.10.1.2:9866
2019-03-01 10:11:39,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742108_1287 src: /10.10.1.4:40472 dest: /10.10.1.2:9866
2019-03-01 10:11:40,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47076, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1490969395_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742103_1282, duration(ns): 564496582
2019-03-01 10:11:40,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742103_1282, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:11:40,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742109_1288 src: /10.10.1.6:41058 dest: /10.10.1.2:9866
2019-03-01 10:11:40,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47084, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_415564948_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742107_1286, duration(ns): 546677198
2019-03-01 10:11:40,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742107_1286, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:11:40,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47078, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1079160607_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742104_1283, duration(ns): 795345194
2019-03-01 10:11:40,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742104_1283, type=LAST_IN_PIPELINE terminating
2019-03-01 10:11:40,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742111_1290 src: /10.10.1.6:41066 dest: /10.10.1.2:9866
2019-03-01 10:11:40,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742110_1289 src: /10.10.1.3:47090 dest: /10.10.1.2:9866
2019-03-01 10:11:40,540 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40472, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1025064869_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742108_1287, duration(ns): 675875342
2019-03-01 10:11:40,540 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742108_1287, type=LAST_IN_PIPELINE terminating
2019-03-01 10:11:40,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742112_1291 src: /10.10.1.6:41072 dest: /10.10.1.2:9866
2019-03-01 10:11:40,658 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742113_1292 src: /10.10.1.5:57000 dest: /10.10.1.2:9866
2019-03-01 10:11:41,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47090, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_415564948_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742110_1289, duration(ns): 593734820
2019-03-01 10:11:41,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742110_1289, type=LAST_IN_PIPELINE terminating
2019-03-01 10:11:41,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742115_1294 src: /10.10.1.6:41080 dest: /10.10.1.2:9866
2019-03-01 10:11:41,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57000, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_843036333_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742113_1292, duration(ns): 516120079
2019-03-01 10:11:41,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742113_1292, type=LAST_IN_PIPELINE terminating
2019-03-01 10:11:41,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:41066, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1079160607_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742111_1290, duration(ns): 740635225
2019-03-01 10:11:41,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742111_1290, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.4:9866] terminating
2019-03-01 10:11:41,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:41072, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1025064869_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742112_1291, duration(ns): 755347756
2019-03-01 10:11:41,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742112_1291, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.4:9866] terminating
2019-03-01 10:11:41,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742117_1296 src: /10.10.1.6:41090 dest: /10.10.1.2:9866
2019-03-01 10:11:41,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:41058, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1490969395_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742109_1288, duration(ns): 1396248901
2019-03-01 10:11:41,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742109_1288, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.5:9866] terminating
2019-03-01 10:11:41,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742116_1295 src: /10.10.1.3:47096 dest: /10.10.1.2:9866
2019-03-01 10:11:42,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47096, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1079160607_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742116_1295, duration(ns): 579970775
2019-03-01 10:11:42,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742116_1295, type=LAST_IN_PIPELINE terminating
2019-03-01 10:11:42,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742120_1299 src: /10.10.1.3:47100 dest: /10.10.1.2:9866
2019-03-01 10:11:42,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47100, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1079160607_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742120_1299, duration(ns): 689689531
2019-03-01 10:11:42,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742120_1299, type=LAST_IN_PIPELINE terminating
2019-03-01 10:11:44,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:41080, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_415564948_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742115_1294, duration(ns): 3139312392
2019-03-01 10:11:44,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742115_1294, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.3:9866] terminating
2019-03-01 10:11:44,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:41090, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1025064869_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742117_1296, duration(ns): 2859201242
2019-03-01 10:11:44,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742117_1296, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.3:9866] terminating
2019-03-01 10:11:44,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742122_1301 src: /10.10.1.5:57012 dest: /10.10.1.2:9866
2019-03-01 10:11:44,305 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742124_1303 src: /10.10.1.5:57014 dest: /10.10.1.2:9866
2019-03-01 10:11:44,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57012, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_415564948_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742122_1301, duration(ns): 601324390
2019-03-01 10:11:44,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742122_1301, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:11:45,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57014, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1061945758_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742124_1303, duration(ns): 931503435
2019-03-01 10:11:45,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742124_1303, type=LAST_IN_PIPELINE terminating
2019-03-01 10:11:50,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742125_1304 src: /10.10.1.6:41154 dest: /10.10.1.2:9866
2019-03-01 10:11:50,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742127_1306 src: /10.10.1.4:40504 dest: /10.10.1.2:9866
2019-03-01 10:11:50,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742126_1305 src: /10.10.1.5:57016 dest: /10.10.1.2:9866
2019-03-01 10:11:50,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742128_1307 src: /10.10.1.5:57018 dest: /10.10.1.2:9866
2019-03-01 10:11:51,096 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57018, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_415564948_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742128_1307, duration(ns): 698445011
2019-03-01 10:11:51,096 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742128_1307, type=LAST_IN_PIPELINE terminating
2019-03-01 10:11:51,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40504, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1061945758_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742127_1306, duration(ns): 1106146036
2019-03-01 10:11:51,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742127_1306, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.5:9866] terminating
2019-03-01 10:11:51,558 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57016, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_843036333_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742126_1305, duration(ns): 1161438793
2019-03-01 10:11:51,558 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742126_1305, type=LAST_IN_PIPELINE terminating
2019-03-01 10:11:52,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742131_1310 src: /10.10.1.6:41170 dest: /10.10.1.2:9866
2019-03-01 10:11:52,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742130_1309 src: /10.10.1.6:41172 dest: /10.10.1.2:9866
2019-03-01 10:11:52,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:41154, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1490969395_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742125_1304, duration(ns): 2204744222
2019-03-01 10:11:52,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742125_1304, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.5:9866] terminating
2019-03-01 10:11:52,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742133_1312 src: /10.10.1.6:41174 dest: /10.10.1.2:9866
2019-03-01 10:11:52,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742134_1313 src: /10.10.1.6:41178 dest: /10.10.1.2:9866
2019-03-01 10:11:53,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:41178, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1061945758_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742134_1313, duration(ns): 399457613
2019-03-01 10:11:53,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742134_1313, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.3:9866] terminating
2019-03-01 10:11:53,060 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:41170, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_843036333_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742131_1310, duration(ns): 458122431
2019-03-01 10:11:53,060 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742131_1310, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.4:9866] terminating
2019-03-01 10:11:53,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742135_1314 src: /10.10.1.6:41182 dest: /10.10.1.2:9866
2019-03-01 10:11:53,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:41182, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_843036333_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742135_1314, duration(ns): 363538212
2019-03-01 10:11:53,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742135_1314, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.4:9866] terminating
2019-03-01 10:11:58,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742132_1311 src: /10.10.1.5:57022 dest: /10.10.1.2:9866
2019-03-01 10:11:58,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:41174, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1025064869_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742133_1312, duration(ns): 608652242
2019-03-01 10:11:58,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742133_1312, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.5:9866] terminating
2019-03-01 10:11:58,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:41172, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_415564948_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742130_1309, duration(ns): 626078993
2019-03-01 10:11:58,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742130_1309, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.4:9866] terminating
2019-03-01 10:11:58,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742136_1315 src: /10.10.1.6:41234 dest: /10.10.1.2:9866
2019-03-01 10:11:59,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57022, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1490969395_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742132_1311, duration(ns): 761239855
2019-03-01 10:11:59,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742132_1311, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:12:04,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:41234, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_415564948_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742136_1315, duration(ns): 5603789129
2019-03-01 10:12:04,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742136_1315, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.5:9866] terminating
2019-03-01 10:12:33,617 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741922_1098 replica FinalizedReplica, blk_1073741922_1098, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741922 for deletion
2019-03-01 10:12:33,618 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741876_1052 replica FinalizedReplica, blk_1073741876_1052, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741876 for deletion
2019-03-01 10:12:33,618 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741958_1134 replica FinalizedReplica, blk_1073741958_1134, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741958 for deletion
2019-03-01 10:12:33,618 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 replica FinalizedReplica, blk_1073741831_1007, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741831 for deletion
2019-03-01 10:12:33,619 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741863_1039 replica FinalizedReplica, blk_1073741863_1039, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741863 for deletion
2019-03-01 10:12:33,619 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741852_1028 replica FinalizedReplica, blk_1073741852_1028, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741852 for deletion
2019-03-01 10:12:33,639 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741922_1098 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741922
2019-03-01 10:12:33,662 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741876_1052 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741876
2019-03-01 10:12:33,683 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741958_1134 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741958
2019-03-01 10:12:33,708 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741831_1007 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741831
2019-03-01 10:12:33,732 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741863_1039 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741863
2019-03-01 10:12:33,756 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741852_1028 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741852
2019-03-01 10:12:40,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742137_1316 src: /10.10.1.3:47122 dest: /10.10.1.2:9866
2019-03-01 10:12:41,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47122, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1459731084_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742137_1316, duration(ns): 372691532
2019-03-01 10:12:41,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742137_1316, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:12:41,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742138_1317 src: /10.10.1.5:57024 dest: /10.10.1.2:9866
2019-03-01 10:12:41,736 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57024, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1459731084_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742138_1317, duration(ns): 400920016
2019-03-01 10:12:41,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742138_1317, type=LAST_IN_PIPELINE terminating
2019-03-01 10:12:41,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742139_1318 src: /10.10.1.5:57026 dest: /10.10.1.2:9866
2019-03-01 10:12:42,168 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57026, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1459731084_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742139_1318, duration(ns): 393247681
2019-03-01 10:12:42,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742139_1318, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:12:45,618 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 replica FinalizedReplica, blk_1073741829_1005, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741829 for deletion
2019-03-01 10:12:45,619 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741878_1054 replica FinalizedReplica, blk_1073741878_1054, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741878 for deletion
2019-03-01 10:12:45,619 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741881_1057 replica FinalizedReplica, blk_1073741881_1057, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741881 for deletion
2019-03-01 10:12:45,619 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741962_1138 replica FinalizedReplica, blk_1073741962_1138, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741962 for deletion
2019-03-01 10:12:47,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742141_1320 src: /10.10.1.4:40520 dest: /10.10.1.2:9866
2019-03-01 10:12:49,671 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741829_1005 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741829
2019-03-01 10:12:49,693 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741878_1054 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741878
2019-03-01 10:12:49,715 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741881_1057 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741881
2019-03-01 10:12:49,736 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073741962_1138 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073741962
2019-03-01 10:12:50,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40520, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1459731084_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742141_1320, duration(ns): 1035465890
2019-03-01 10:12:50,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742141_1320, type=LAST_IN_PIPELINE terminating
2019-03-01 10:12:58,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742143_1322 src: /10.10.1.4:40524 dest: /10.10.1.2:9866
2019-03-01 10:13:00,560 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40524, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1459731084_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742143_1322, duration(ns): 415288080
2019-03-01 10:13:00,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742143_1322, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:13:01,461 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742146_1325 src: /10.10.1.6:41788 dest: /10.10.1.2:9866
2019-03-01 10:13:02,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:41788, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1459731084_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742146_1325, duration(ns): 437458865
2019-03-01 10:13:02,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742146_1325, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.5:9866] terminating
2019-03-01 10:13:09,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742149_1328 src: /10.10.1.4:40532 dest: /10.10.1.2:9866
2019-03-01 10:13:14,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40532, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-299731618_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742149_1328, duration(ns): 3433237990
2019-03-01 10:13:14,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742149_1328, type=LAST_IN_PIPELINE terminating
2019-03-01 10:13:19,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742150_1329 src: /10.10.1.3:47136 dest: /10.10.1.2:9866
2019-03-01 10:13:19,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47136, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-299731618_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742150_1329, duration(ns): 505546664
2019-03-01 10:13:19,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742150_1329, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.5:9866] terminating
2019-03-01 10:13:34,986 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-03-01 10:13:34,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at clnode084.clemson.cloudlab.us/130.127.133.93
************************************************************/
2019-03-01 10:13:37,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = clnode084.clemson.cloudlab.us/130.127.133.93
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-02-17T03:08Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-03-01 10:13:37,096 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-03-01 10:13:37,673 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/root/data
2019-03-01 10:13:37,882 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-03-01 10:13:37,957 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-03-01 10:13:37,957 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-03-01 10:13:38,220 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-03-01 10:13:38,223 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-03-01 10:13:38,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is clnode084.clemson.cloudlab.us
2019-03-01 10:13:38,229 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-03-01 10:13:38,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-03-01 10:13:38,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:9866
2019-03-01 10:13:38,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwidth is 10485760 bytes/s
2019-03-01 10:13:38,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-03-01 10:13:38,401 INFO org.eclipse.jetty.util.log: Logging initialized @1836ms
2019-03-01 10:13:38,512 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-03-01 10:13:38,515 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-03-01 10:13:38,521 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-03-01 10:13:38,524 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-03-01 10:13:38,524 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-03-01 10:13:38,524 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-03-01 10:13:38,549 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 35146
2019-03-01 10:13:38,550 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2019-03-01 10:13:38,584 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@48e92c5c{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-03-01 10:13:38,585 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@22356acd{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-03-01 10:13:38,655 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@366ac49b{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/datanode/,AVAILABLE}{/datanode}
2019-03-01 10:13:38,662 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@34cdeda2{HTTP/1.1,[http/1.1]}{localhost:35146}
2019-03-01 10:13:38,662 INFO org.eclipse.jetty.server.Server: Started @2096ms
2019-03-01 10:13:38,855 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864
2019-03-01 10:13:38,863 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-03-01 10:13:38,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2019-03-01 10:13:38,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-03-01 10:13:38,961 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-03-01 10:13:38,978 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9867
2019-03-01 10:13:39,027 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:9867
2019-03-01 10:13:39,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: mycluster
2019-03-01 10:13:39,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: mycluster
2019-03-01 10:13:39,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to node-0-link-0/10.10.1.1:8020 starting to offer service
2019-03-01 10:13:39,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to node-1-link-0/10.10.1.4:8020 starting to offer service
2019-03-01 10:13:39,077 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-03-01 10:13:39,078 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9867: starting
2019-03-01 10:13:39,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42132. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:13:39,220 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-03-01 10:13:39,256 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /root/data/in_use.lock acquired by nodename 2354@clnode084.clemson.cloudlab.us
2019-03-01 10:13:39,291 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-258834523-130.127.133.101-1551459924751
2019-03-01 10:13:39,291 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /root/data/current/BP-258834523-130.127.133.101-1551459924751
2019-03-01 10:13:39,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1814765448;bpid=BP-258834523-130.127.133.101-1551459924751;lv=-57;nsInfo=lv=-64;cid=CID-0840fdc4-95c6-43ff-a7dd-de4d6a452d43;nsid=1814765448;c=1551459924751;bpid=BP-258834523-130.127.133.101-1551459924751;dnuuid=f33fa295-0841-426e-9ba0-92fb033d0f2d
2019-03-01 10:13:39,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-24815efc-5064-4fd1-a028-0738581a015d
2019-03-01 10:13:39,376 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - [DISK]file:/root/data, StorageType: DISK
2019-03-01 10:13:39,380 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-03-01 10:13:39,388 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for /root/data
2019-03-01 10:13:39,397 INFO org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker: Scheduled health check for volume /root/data
2019-03-01 10:13:39,399 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-258834523-130.127.133.101-1551459924751
2019-03-01 10:13:39,399 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-258834523-130.127.133.101-1551459924751 on volume /root/data...
2019-03-01 10:13:39,410 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /root/data/current/BP-258834523-130.127.133.101-1551459924751/current: 9603932657
2019-03-01 10:13:39,413 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-258834523-130.127.133.101-1551459924751 on /root/data: 14ms
2019-03-01 10:13:39,413 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-258834523-130.127.133.101-1551459924751: 15ms
2019-03-01 10:13:39,416 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-258834523-130.127.133.101-1551459924751 on volume /root/data...
2019-03-01 10:13:39,416 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /root/data/current/BP-258834523-130.127.133.101-1551459924751/current/replicas doesn't exist 
2019-03-01 10:13:39,449 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-258834523-130.127.133.101-1551459924751 on volume /root/data: 32ms
2019-03-01 10:13:39,449 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 33ms
2019-03-01 10:13:39,482 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/root/data, DS-24815efc-5064-4fd1-a028-0738581a015d): no suitable block pools found to scan.  Waiting 1813924810 ms.
2019-03-01 10:13:39,490 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 3/1/19 12:09 PM with interval of 21600000ms
2019-03-01 10:13:39,494 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool BP-258834523-130.127.133.101-1551459924751 (Datanode Uuid f33fa295-0841-426e-9ba0-92fb033d0f2d) service to node-0-link-0/10.10.1.1:8020
2019-03-01 10:13:39,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-258834523-130.127.133.101-1551459924751 (Datanode Uuid f33fa295-0841-426e-9ba0-92fb033d0f2d) service to node-0-link-0/10.10.1.1:8020 beginning handshake with NN
2019-03-01 10:13:39,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-258834523-130.127.133.101-1551459924751 (Datanode Uuid f33fa295-0841-426e-9ba0-92fb033d0f2d) service to node-1-link-0/10.10.1.4:8020 beginning handshake with NN
2019-03-01 10:13:39,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-258834523-130.127.133.101-1551459924751 (Datanode Uuid f33fa295-0841-426e-9ba0-92fb033d0f2d) service to node-0-link-0/10.10.1.1:8020 successfully registered with NN
2019-03-01 10:13:39,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-258834523-130.127.133.101-1551459924751 (Datanode Uuid f33fa295-0841-426e-9ba0-92fb033d0f2d) service to node-1-link-0/10.10.1.4:8020 successfully registered with NN
2019-03-01 10:13:39,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode node-0-link-0/10.10.1.1:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-03-01 10:13:39,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode node-1-link-0/10.10.1.4:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-03-01 10:13:39,615 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x97a89908de95a499,  containing 1 storage report(s), of which we sent 1. The reports had 74 total blocks and used 1 RPC(s). This took 6 msec to generate and 29 msecs for RPC and NN processing. Got back no commands.
2019-03-01 10:13:39,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3a8db415c96c2782,  containing 1 storage report(s), of which we sent 1. The reports had 74 total blocks and used 1 RPC(s). This took 6 msec to generate and 29 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-03-01 10:13:39,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-258834523-130.127.133.101-1551459924751
2019-03-01 10:13:41,812 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42158. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:13:42,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42164. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:13:44,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42180. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:13:45,537 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742064_1243 replica FinalizedReplica, blk_1073742064_1243, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742064 for deletion
2019-03-01 10:13:45,539 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742066_1245 replica FinalizedReplica, blk_1073742066_1245, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742066 for deletion
2019-03-01 10:13:45,540 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742070_1249 replica FinalizedReplica, blk_1073742070_1249, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742070 for deletion
2019-03-01 10:13:45,540 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742072_1251 replica FinalizedReplica, blk_1073742072_1251, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742072 for deletion
2019-03-01 10:13:45,540 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742058_1237 replica FinalizedReplica, blk_1073742058_1237, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742058 for deletion
2019-03-01 10:13:45,540 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742062_1241 replica FinalizedReplica, blk_1073742062_1241, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742062 for deletion
2019-03-01 10:13:45,565 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742064_1243 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742064
2019-03-01 10:13:45,590 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742066_1245 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742066
2019-03-01 10:13:45,615 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742070_1249 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742070
2019-03-01 10:13:45,638 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742072_1251 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742072
2019-03-01 10:13:45,662 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742058_1237 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742058
2019-03-01 10:13:45,686 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742062_1241 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742062
2019-03-01 10:13:48,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42216. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:13:52,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42244. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:13:53,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42256. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:13:53,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42260. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:13:54,531 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742065_1244 replica FinalizedReplica, blk_1073742065_1244, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742065 for deletion
2019-03-01 10:13:54,532 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742067_1246 replica FinalizedReplica, blk_1073742067_1246, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742067 for deletion
2019-03-01 10:13:54,532 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742071_1250 replica FinalizedReplica, blk_1073742071_1250, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742071 for deletion
2019-03-01 10:13:54,532 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742061_1240 replica FinalizedReplica, blk_1073742061_1240, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742061 for deletion
2019-03-01 10:13:54,532 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742063_1242 replica FinalizedReplica, blk_1073742063_1242, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742063 for deletion
2019-03-01 10:13:54,554 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742065_1244 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742065
2019-03-01 10:13:54,579 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742067_1246 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742067
2019-03-01 10:13:54,601 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742071_1250 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742071
2019-03-01 10:13:54,625 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742061_1240 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742061
2019-03-01 10:13:54,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742063_1242 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742063
2019-03-01 10:13:56,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.5:57044. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50e2 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:13:57,531 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742080_1259 replica FinalizedReplica, blk_1073742080_1259, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742080 for deletion
2019-03-01 10:13:57,531 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742082_1261 replica FinalizedReplica, blk_1073742082_1261, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742082 for deletion
2019-03-01 10:13:57,532 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742084_1263 replica FinalizedReplica, blk_1073742084_1263, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742084 for deletion
2019-03-01 10:13:57,532 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742073_1252 replica FinalizedReplica, blk_1073742073_1252, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742073 for deletion
2019-03-01 10:13:57,532 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742075_1254 replica FinalizedReplica, blk_1073742075_1254, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742075 for deletion
2019-03-01 10:13:57,533 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742077_1256 replica FinalizedReplica, blk_1073742077_1256, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742077 for deletion
2019-03-01 10:13:57,533 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742078_1257 replica FinalizedReplica, blk_1073742078_1257, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742078 for deletion
2019-03-01 10:13:57,554 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742080_1259 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742080
2019-03-01 10:13:57,576 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742082_1261 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742082
2019-03-01 10:13:57,599 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742084_1263 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742084
2019-03-01 10:13:57,621 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742073_1252 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742073
2019-03-01 10:13:57,644 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742075_1254 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742075
2019-03-01 10:13:57,666 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742077_1256 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742077
2019-03-01 10:13:57,689 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742078_1257 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742078
2019-03-01 10:14:00,530 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742081_1260 replica FinalizedReplica, blk_1073742081_1260, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742081 for deletion
2019-03-01 10:14:00,530 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742083_1262 replica FinalizedReplica, blk_1073742083_1262, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742083 for deletion
2019-03-01 10:14:00,531 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742085_1264 replica FinalizedReplica, blk_1073742085_1264, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742085 for deletion
2019-03-01 10:14:00,531 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742086_1265 replica FinalizedReplica, blk_1073742086_1265, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742086 for deletion
2019-03-01 10:14:00,531 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742088_1267 replica FinalizedReplica, blk_1073742088_1267, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742088 for deletion
2019-03-01 10:14:00,531 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742076_1255 replica FinalizedReplica, blk_1073742076_1255, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742076 for deletion
2019-03-01 10:14:00,553 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742081_1260 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742081
2019-03-01 10:14:00,575 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742083_1262 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742083
2019-03-01 10:14:00,597 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742085_1264 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742085
2019-03-01 10:14:00,618 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742086_1265 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742086
2019-03-01 10:14:00,639 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742088_1267 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742088
2019-03-01 10:14:00,663 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742076_1255 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir0/blk_1073742076
2019-03-01 10:14:01,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42306. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50f3 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:14:01,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42310. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:14:01,846 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42314. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:14:03,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42332. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:14:04,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42340. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5178 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:14:06,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.3:47152. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50e2 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:14:08,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.3:47154. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50c2 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:14:10,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42396. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50f6 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:14:14,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42432. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:14:17,441 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42444. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:14:18,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42454. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:14:23,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42492. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5178 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:14:25,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42500. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:14:27,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42520. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50f5 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:14:28,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.5:57082. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50e1 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:14:28,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.4:40574. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50e3 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:14:29,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42560. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:14:36,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42626. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:14:36,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42630. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:14:39,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42660. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:14:42,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42704. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:14:48,121 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42734. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:14:53,461 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.3:47238. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50a5 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:14:54,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42784. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:14:56,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42816. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:15:02,464 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.3:47242. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50a5 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:15:03,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42858. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:15:05,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42868. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:15:06,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42878. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:15:06,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42882. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50f5 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:15:13,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42928. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:15:17,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:42956. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:15:26,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:43034. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:15:27,534 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742096_1275 replica FinalizedReplica, blk_1073742096_1275, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742096 for deletion
2019-03-01 10:15:27,534 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742098_1277 replica FinalizedReplica, blk_1073742098_1277, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742098 for deletion
2019-03-01 10:15:27,535 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742116_1295 replica FinalizedReplica, blk_1073742116_1295, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742116 for deletion
2019-03-01 10:15:27,535 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742104_1283 replica FinalizedReplica, blk_1073742104_1283, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742104 for deletion
2019-03-01 10:15:27,535 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742120_1299 replica FinalizedReplica, blk_1073742120_1299, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742120 for deletion
2019-03-01 10:15:27,535 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742093_1272 replica FinalizedReplica, blk_1073742093_1272, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742093 for deletion
2019-03-01 10:15:27,536 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742111_1290 replica FinalizedReplica, blk_1073742111_1290, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742111 for deletion
2019-03-01 10:15:27,558 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742096_1275 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742096
2019-03-01 10:15:27,584 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742098_1277 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742098
2019-03-01 10:15:27,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742116_1295 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742116
2019-03-01 10:15:27,630 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742104_1283 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742104
2019-03-01 10:15:27,652 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742120_1299 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742120
2019-03-01 10:15:27,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742093_1272 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742093
2019-03-01 10:15:27,702 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742111_1290 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742111
2019-03-01 10:15:28,236 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.5:57152. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50c2 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:15:33,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:43088. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:15:34,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:43100. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:15:37,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.3:47252. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50c3 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:15:42,535 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742112_1291 replica FinalizedReplica, blk_1073742112_1291, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742112 for deletion
2019-03-01 10:15:42,536 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742117_1296 replica FinalizedReplica, blk_1073742117_1296, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742117 for deletion
2019-03-01 10:15:42,536 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742133_1312 replica FinalizedReplica, blk_1073742133_1312, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742133 for deletion
2019-03-01 10:15:42,536 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742108_1287 replica FinalizedReplica, blk_1073742108_1287, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742108 for deletion
2019-03-01 10:15:42,536 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742095_1274 replica FinalizedReplica, blk_1073742095_1274, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742095 for deletion
2019-03-01 10:15:42,558 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742112_1291 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742112
2019-03-01 10:15:42,579 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742117_1296 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742117
2019-03-01 10:15:42,600 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742133_1312 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742133
2019-03-01 10:15:42,623 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742108_1287 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742108
2019-03-01 10:15:42,647 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742095_1274 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742095
2019-03-01 10:15:45,615 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.5:57164. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50c3 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:15:46,478 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:43190. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:15:48,127 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:43220. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:15:51,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.3:47272. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50e2 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:15:54,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:43294. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:15:58,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:43338. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517b instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:15:59,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:43344. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:16:16,975 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.4:40694. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50e3 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:16:18,534 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:43472. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:16:18,536 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742097_1276 replica FinalizedReplica, blk_1073742097_1276, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742097 for deletion
2019-03-01 10:16:18,536 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742134_1313 replica FinalizedReplica, blk_1073742134_1313, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742134 for deletion
2019-03-01 10:16:18,536 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742090_1269 replica FinalizedReplica, blk_1073742090_1269, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742090 for deletion
2019-03-01 10:16:18,537 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742092_1271 replica FinalizedReplica, blk_1073742092_1271, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742092 for deletion
2019-03-01 10:16:18,537 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742124_1303 replica FinalizedReplica, blk_1073742124_1303, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742124 for deletion
2019-03-01 10:16:18,537 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742127_1306 replica FinalizedReplica, blk_1073742127_1306, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742127 for deletion
2019-03-01 10:16:18,562 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742097_1276 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742097
2019-03-01 10:16:18,583 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742134_1313 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742134
2019-03-01 10:16:18,615 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742090_1269 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742090
2019-03-01 10:16:18,640 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742092_1271 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742092
2019-03-01 10:16:18,663 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742124_1303 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742124
2019-03-01 10:16:18,685 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742127_1306 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742127
2019-03-01 10:16:21,535 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742113_1292 replica FinalizedReplica, blk_1073742113_1292, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742113 for deletion
2019-03-01 10:16:21,536 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742131_1310 replica FinalizedReplica, blk_1073742131_1310, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742131 for deletion
2019-03-01 10:16:21,536 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742100_1279 replica FinalizedReplica, blk_1073742100_1279, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742100 for deletion
2019-03-01 10:16:21,536 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742135_1314 replica FinalizedReplica, blk_1073742135_1314, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742135 for deletion
2019-03-01 10:16:21,536 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742094_1273 replica FinalizedReplica, blk_1073742094_1273, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742094 for deletion
2019-03-01 10:16:21,536 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742126_1305 replica FinalizedReplica, blk_1073742126_1305, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742126 for deletion
2019-03-01 10:16:21,559 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742113_1292 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742113
2019-03-01 10:16:21,580 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742131_1310 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742131
2019-03-01 10:16:21,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742100_1279 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742100
2019-03-01 10:16:21,627 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742135_1314 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742135
2019-03-01 10:16:21,652 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742094_1273 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742094
2019-03-01 10:16:21,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742126_1305 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742126
2019-03-01 10:16:28,852 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:43524. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:16:30,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:43536. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50ef instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:16:33,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:43558. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50f2 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:16:36,037 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:43584. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c517a instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:16:45,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected encryption handshake from client at /10.10.1.6:43672. Perhaps the client is running an older version of Hadoop which does not support encryption
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5179 instead of deadbeef from client.
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:364)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getEncryptedStreams(SaslDataTransferServer.java:180)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:112)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:748)
2019-03-01 10:16:58,865 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-03-01 10:16:58,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at clnode084.clemson.cloudlab.us/130.127.133.93
************************************************************/
2019-03-01 10:17:00,972 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = clnode084.clemson.cloudlab.us/130.127.133.93
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-02-17T03:08Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-03-01 10:17:00,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-03-01 10:17:01,557 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/root/data
2019-03-01 10:17:01,778 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-03-01 10:17:01,855 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-03-01 10:17:01,855 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-03-01 10:17:02,119 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-03-01 10:17:02,123 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-03-01 10:17:02,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is clnode084.clemson.cloudlab.us
2019-03-01 10:17:02,128 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-03-01 10:17:02,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-03-01 10:17:02,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:9866
2019-03-01 10:17:02,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwidth is 10485760 bytes/s
2019-03-01 10:17:02,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-03-01 10:17:02,283 INFO org.eclipse.jetty.util.log: Logging initialized @1833ms
2019-03-01 10:17:02,392 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-03-01 10:17:02,396 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-03-01 10:17:02,401 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-03-01 10:17:02,403 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-03-01 10:17:02,403 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-03-01 10:17:02,403 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-03-01 10:17:02,426 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 45886
2019-03-01 10:17:02,428 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2019-03-01 10:17:02,462 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@48e92c5c{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-03-01 10:17:02,463 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@22356acd{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-03-01 10:17:02,534 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@366ac49b{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/datanode/,AVAILABLE}{/datanode}
2019-03-01 10:17:02,540 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@34cdeda2{HTTP/1.1,[http/1.1]}{localhost:45886}
2019-03-01 10:17:02,541 INFO org.eclipse.jetty.server.Server: Started @2091ms
2019-03-01 10:17:02,734 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864
2019-03-01 10:17:02,741 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-03-01 10:17:02,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2019-03-01 10:17:02,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-03-01 10:17:02,828 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-03-01 10:17:02,844 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9867
2019-03-01 10:17:02,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:9867
2019-03-01 10:17:02,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: mycluster
2019-03-01 10:17:02,915 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: mycluster
2019-03-01 10:17:02,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to node-0-link-0/10.10.1.1:8020 starting to offer service
2019-03-01 10:17:02,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to node-1-link-0/10.10.1.4:8020 starting to offer service
2019-03-01 10:17:02,933 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-03-01 10:17:02,933 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9867: starting
2019-03-01 10:17:03,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to node-0-link-0/10.10.1.1:8020
2019-03-01 10:17:03,076 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-03-01 10:17:03,113 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /root/data/in_use.lock acquired by nodename 2709@clnode084.clemson.cloudlab.us
2019-03-01 10:17:03,146 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-258834523-130.127.133.101-1551459924751
2019-03-01 10:17:03,146 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /root/data/current/BP-258834523-130.127.133.101-1551459924751
2019-03-01 10:17:03,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1814765448;bpid=BP-258834523-130.127.133.101-1551459924751;lv=-57;nsInfo=lv=-64;cid=CID-0840fdc4-95c6-43ff-a7dd-de4d6a452d43;nsid=1814765448;c=1551459924751;bpid=BP-258834523-130.127.133.101-1551459924751;dnuuid=f33fa295-0841-426e-9ba0-92fb033d0f2d
2019-03-01 10:17:03,228 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-24815efc-5064-4fd1-a028-0738581a015d
2019-03-01 10:17:03,228 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - [DISK]file:/root/data, StorageType: DISK
2019-03-01 10:17:03,232 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-03-01 10:17:03,239 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for /root/data
2019-03-01 10:17:03,248 INFO org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker: Scheduled health check for volume /root/data
2019-03-01 10:17:03,250 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-258834523-130.127.133.101-1551459924751
2019-03-01 10:17:03,251 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-258834523-130.127.133.101-1551459924751 on volume /root/data...
2019-03-01 10:17:03,261 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /root/data/current/BP-258834523-130.127.133.101-1551459924751/current: 3111149729
2019-03-01 10:17:03,264 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-258834523-130.127.133.101-1551459924751 on /root/data: 14ms
2019-03-01 10:17:03,265 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-258834523-130.127.133.101-1551459924751: 14ms
2019-03-01 10:17:03,267 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-258834523-130.127.133.101-1551459924751 on volume /root/data...
2019-03-01 10:17:03,267 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /root/data/current/BP-258834523-130.127.133.101-1551459924751/current/replicas doesn't exist 
2019-03-01 10:17:03,291 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-258834523-130.127.133.101-1551459924751 on volume /root/data: 23ms
2019-03-01 10:17:03,291 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 24ms
2019-03-01 10:17:03,324 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/root/data, DS-24815efc-5064-4fd1-a028-0738581a015d): no suitable block pools found to scan.  Waiting 1813720968 ms.
2019-03-01 10:17:03,332 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 3/1/19 1:45 PM with interval of 21600000ms
2019-03-01 10:17:03,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-258834523-130.127.133.101-1551459924751 (Datanode Uuid f33fa295-0841-426e-9ba0-92fb033d0f2d) service to node-1-link-0/10.10.1.4:8020 beginning handshake with NN
2019-03-01 10:17:03,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-258834523-130.127.133.101-1551459924751 (Datanode Uuid f33fa295-0841-426e-9ba0-92fb033d0f2d) service to node-0-link-0/10.10.1.1:8020 beginning handshake with NN
2019-03-01 10:17:03,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-258834523-130.127.133.101-1551459924751 (Datanode Uuid f33fa295-0841-426e-9ba0-92fb033d0f2d) service to node-1-link-0/10.10.1.4:8020 successfully registered with NN
2019-03-01 10:17:03,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-258834523-130.127.133.101-1551459924751 (Datanode Uuid f33fa295-0841-426e-9ba0-92fb033d0f2d) service to node-0-link-0/10.10.1.1:8020 successfully registered with NN
2019-03-01 10:17:03,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode node-1-link-0/10.10.1.4:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-03-01 10:17:03,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode node-0-link-0/10.10.1.1:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-03-01 10:17:03,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x8c4dfbcd5c624a1c,  containing 1 storage report(s), of which we sent 1. The reports had 26 total blocks and used 1 RPC(s). This took 5 msec to generate and 29 msecs for RPC and NN processing. Got back no commands.
2019-03-01 10:17:03,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xa681bba704816eb7,  containing 1 storage report(s), of which we sent 1. The reports had 26 total blocks and used 1 RPC(s). This took 5 msec to generate and 29 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-03-01 10:17:03,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-258834523-130.127.133.101-1551459924751
2019-03-01 10:17:09,379 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742146_1325 replica FinalizedReplica, blk_1073742146_1325, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742146 for deletion
2019-03-01 10:17:09,381 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742137_1316 replica FinalizedReplica, blk_1073742137_1316, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742137 for deletion
2019-03-01 10:17:09,382 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742138_1317 replica FinalizedReplica, blk_1073742138_1317, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742138 for deletion
2019-03-01 10:17:09,382 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742139_1318 replica FinalizedReplica, blk_1073742139_1318, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742139 for deletion
2019-03-01 10:17:09,382 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742141_1320 replica FinalizedReplica, blk_1073742141_1320, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742141 for deletion
2019-03-01 10:17:09,383 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742143_1322 replica FinalizedReplica, blk_1073742143_1322, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742143 for deletion
2019-03-01 10:17:09,404 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742146_1325 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742146
2019-03-01 10:17:09,432 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742137_1316 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742137
2019-03-01 10:17:09,457 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742138_1317 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742138
2019-03-01 10:17:09,481 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742139_1318 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742139
2019-03-01 10:17:09,509 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742141_1320 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742141
2019-03-01 10:17:09,532 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742143_1322 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742143
2019-03-01 10:17:11,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742288_1467 src: /10.10.1.3:47324 dest: /10.10.1.2:9866
2019-03-01 10:17:11,949 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-258834523-130.127.133.101-1551459924751:blk_1073742288_1467 src: /10.10.1.3:47324 dest: /10.10.1.2:9866 of size 134217728
2019-03-01 10:17:15,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742298_1477 src: /10.10.1.4:40744 dest: /10.10.1.2:9866
2019-03-01 10:17:16,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40744, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_709250910_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742298_1477, duration(ns): 420302196
2019-03-01 10:17:16,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742298_1477, type=LAST_IN_PIPELINE terminating
2019-03-01 10:17:16,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742299_1478 src: /10.10.1.3:47330 dest: /10.10.1.2:9866
2019-03-01 10:17:16,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47330, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_709250910_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742299_1478, duration(ns): 533955697
2019-03-01 10:17:16,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742299_1478, type=LAST_IN_PIPELINE terminating
2019-03-01 10:17:21,372 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742115_1294 replica FinalizedReplica, blk_1073742115_1294, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742115 for deletion
2019-03-01 10:17:21,373 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742089_1268 replica FinalizedReplica, blk_1073742089_1268, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742089 for deletion
2019-03-01 10:17:21,373 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742122_1301 replica FinalizedReplica, blk_1073742122_1301, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742122 for deletion
2019-03-01 10:17:21,373 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742091_1270 replica FinalizedReplica, blk_1073742091_1270, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742091 for deletion
2019-03-01 10:17:21,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742125_1304 replica FinalizedReplica, blk_1073742125_1304, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742125 for deletion
2019-03-01 10:17:21,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742128_1307 replica FinalizedReplica, blk_1073742128_1307, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742128 for deletion
2019-03-01 10:17:21,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742130_1309 replica FinalizedReplica, blk_1073742130_1309, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742130 for deletion
2019-03-01 10:17:21,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742099_1278 replica FinalizedReplica, blk_1073742099_1278, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742099 for deletion
2019-03-01 10:17:21,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742132_1311 replica FinalizedReplica, blk_1073742132_1311, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742132 for deletion
2019-03-01 10:17:21,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742102_1281 replica FinalizedReplica, blk_1073742102_1281, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742102 for deletion
2019-03-01 10:17:21,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742103_1282 replica FinalizedReplica, blk_1073742103_1282, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742103 for deletion
2019-03-01 10:17:21,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742136_1315 replica FinalizedReplica, blk_1073742136_1315, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742136 for deletion
2019-03-01 10:17:21,376 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742107_1286 replica FinalizedReplica, blk_1073742107_1286, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742107 for deletion
2019-03-01 10:17:21,376 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742109_1288 replica FinalizedReplica, blk_1073742109_1288, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742109 for deletion
2019-03-01 10:17:21,376 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742110_1289 replica FinalizedReplica, blk_1073742110_1289, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742110 for deletion
2019-03-01 10:17:21,394 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742115_1294 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742115
2019-03-01 10:17:21,418 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742089_1268 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742089
2019-03-01 10:17:21,439 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742122_1301 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742122
2019-03-01 10:17:21,464 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742091_1270 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742091
2019-03-01 10:17:21,486 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742125_1304 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742125
2019-03-01 10:17:21,507 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742128_1307 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742128
2019-03-01 10:17:21,528 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742130_1309 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742130
2019-03-01 10:17:21,552 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742099_1278 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742099
2019-03-01 10:17:21,573 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742132_1311 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742132
2019-03-01 10:17:21,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742102_1281 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742102
2019-03-01 10:17:21,619 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742103_1282 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742103
2019-03-01 10:17:21,640 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742136_1315 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742136
2019-03-01 10:17:21,663 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742107_1286 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742107
2019-03-01 10:17:21,686 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742109_1288 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742109
2019-03-01 10:17:21,708 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742110_1289 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742110
2019-03-01 10:17:26,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742302_1481 src: /10.10.1.6:44006 dest: /10.10.1.2:9866
2019-03-01 10:17:28,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:44006, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_709250910_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742302_1481, duration(ns): 1739587542
2019-03-01 10:17:28,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742302_1481, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.3:9866] terminating
2019-03-01 10:17:28,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742304_1483 src: /10.10.1.5:57246 dest: /10.10.1.2:9866
2019-03-01 10:17:29,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57246, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_709250910_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742304_1483, duration(ns): 483218978
2019-03-01 10:17:29,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742304_1483, type=LAST_IN_PIPELINE terminating
2019-03-01 10:17:32,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742308_1487 src: /10.10.1.6:44036 dest: /10.10.1.2:9866
2019-03-01 10:17:32,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:44036, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1403593281_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742308_1487, duration(ns): 379937291
2019-03-01 10:17:32,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742308_1487, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.5:9866] terminating
2019-03-01 10:17:32,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742309_1488 src: /10.10.1.6:44038 dest: /10.10.1.2:9866
2019-03-01 10:17:33,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742311_1490 src: /10.10.1.5:57254 dest: /10.10.1.2:9866
2019-03-01 10:17:33,598 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:44038, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1403593281_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742309_1488, duration(ns): 504580093
2019-03-01 10:17:33,598 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742309_1488, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.4:9866] terminating
2019-03-01 10:17:33,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742312_1491 src: /10.10.1.4:40764 dest: /10.10.1.2:9866
2019-03-01 10:17:34,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57254, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-824535980_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742311_1490, duration(ns): 435825123
2019-03-01 10:17:34,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742311_1490, type=LAST_IN_PIPELINE terminating
2019-03-01 10:17:34,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742313_1492 src: /10.10.1.6:44052 dest: /10.10.1.2:9866
2019-03-01 10:17:34,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40764, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1403593281_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742312_1491, duration(ns): 665484460
2019-03-01 10:17:34,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742312_1491, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:17:36,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:44052, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-824535980_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742313_1492, duration(ns): 1995089093
2019-03-01 10:17:36,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742313_1492, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.3:9866] terminating
2019-03-01 10:17:38,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742315_1494 src: /10.10.1.5:57256 dest: /10.10.1.2:9866
2019-03-01 10:17:39,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57256, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-824535980_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742315_1494, duration(ns): 1004884403
2019-03-01 10:17:39,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742315_1494, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:17:41,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742316_1495 src: /10.10.1.4:40770 dest: /10.10.1.2:9866
2019-03-01 10:17:42,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40770, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-824535980_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742316_1495, duration(ns): 665575140
2019-03-01 10:17:42,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742316_1495, type=LAST_IN_PIPELINE terminating
2019-03-01 10:17:44,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742318_1497 src: /10.10.1.5:57264 dest: /10.10.1.2:9866
2019-03-01 10:17:46,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57264, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1403593281_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742318_1497, duration(ns): 507115613
2019-03-01 10:17:46,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742318_1497, type=LAST_IN_PIPELINE terminating
2019-03-01 10:17:46,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742319_1498 src: /10.10.1.6:44178 dest: /10.10.1.2:9866
2019-03-01 10:17:46,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:44178, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1403593281_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742319_1498, duration(ns): 469658526
2019-03-01 10:17:46,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742319_1498, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.3:9866] terminating
2019-03-01 10:17:48,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742320_1499 src: /10.10.1.4:40786 dest: /10.10.1.2:9866
2019-03-01 10:17:49,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40786, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1403593281_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742320_1499, duration(ns): 667754830
2019-03-01 10:17:49,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742320_1499, type=LAST_IN_PIPELINE terminating
2019-03-01 10:18:06,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742149_1328 replica FinalizedReplica, blk_1073742149_1328, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742149 for deletion
2019-03-01 10:18:06,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742150_1329 replica FinalizedReplica, blk_1073742150_1329, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742150 for deletion
2019-03-01 10:18:06,397 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742149_1328 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742149
2019-03-01 10:18:06,419 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742150_1329 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742150
2019-03-01 10:18:11,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742321_1500 src: /10.10.1.6:44366 dest: /10.10.1.2:9866
2019-03-01 10:18:13,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:44366, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_730029814_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742321_1500, duration(ns): 507773915
2019-03-01 10:18:13,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742321_1500, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.5:9866] terminating
2019-03-01 10:18:13,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742322_1501 src: /10.10.1.4:40788 dest: /10.10.1.2:9866
2019-03-01 10:18:13,636 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40788, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_730029814_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742322_1501, duration(ns): 405001902
2019-03-01 10:18:13,636 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742322_1501, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.5:9866] terminating
2019-03-01 10:18:13,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742323_1502 src: /10.10.1.6:44394 dest: /10.10.1.2:9866
2019-03-01 10:18:14,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:44394, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_730029814_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742323_1502, duration(ns): 392454921
2019-03-01 10:18:14,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742323_1502, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.5:9866] terminating
2019-03-01 10:18:15,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742324_1503 src: /10.10.1.4:40792 dest: /10.10.1.2:9866
2019-03-01 10:18:21,395 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1177ms (threshold=300ms), volume=file:/root/data/, blockId=1073742324
2019-03-01 10:18:26,207 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40792, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_730029814_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742324_1503, duration(ns): 10449366948
2019-03-01 10:18:26,207 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742324_1503, type=LAST_IN_PIPELINE terminating
2019-03-01 10:18:27,230 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742326_1505 src: /10.10.1.5:57272 dest: /10.10.1.2:9866
2019-03-01 10:18:27,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57272, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_730029814_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742326_1505, duration(ns): 393413504
2019-03-01 10:18:27,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742326_1505, type=LAST_IN_PIPELINE terminating
2019-03-01 10:18:41,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742329_1508 src: /10.10.1.5:57276 dest: /10.10.1.2:9866
2019-03-01 10:18:42,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57276, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_28590724_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742329_1508, duration(ns): 566263845
2019-03-01 10:18:42,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742329_1508, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:18:42,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742330_1509 src: /10.10.1.6:44616 dest: /10.10.1.2:9866
2019-03-01 10:18:44,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742331_1510 src: /10.10.1.4:40800 dest: /10.10.1.2:9866
2019-03-01 10:18:52,049 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742333_1512 src: /10.10.1.3:47356 dest: /10.10.1.2:9866
2019-03-01 10:18:52,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:44616, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_28590724_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742330_1509, duration(ns): 5357747317
2019-03-01 10:18:52,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742330_1509, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.4:9866] terminating
2019-03-01 10:18:52,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40800, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1953040112_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742331_1510, duration(ns): 504220748
2019-03-01 10:18:52,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742331_1510, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.5:9866] terminating
2019-03-01 10:18:52,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742334_1513 src: /10.10.1.6:44670 dest: /10.10.1.2:9866
2019-03-01 10:18:52,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742335_1514 src: /10.10.1.6:44672 dest: /10.10.1.2:9866
2019-03-01 10:18:53,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47356, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1131378456_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742333_1512, duration(ns): 735838710
2019-03-01 10:18:53,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742333_1512, type=LAST_IN_PIPELINE terminating
2019-03-01 10:18:55,727 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:44672, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1953040112_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742335_1514, duration(ns): 2759939050
2019-03-01 10:18:55,727 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742335_1514, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.4:9866] terminating
2019-03-01 10:18:55,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742338_1517 src: /10.10.1.6:44694 dest: /10.10.1.2:9866
2019-03-01 10:18:56,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:44694, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1953040112_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742338_1517, duration(ns): 417077089
2019-03-01 10:18:56,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742338_1517, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.3:9866] terminating
2019-03-01 10:18:57,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:44670, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_28590724_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742334_1513, duration(ns): 4266214365
2019-03-01 10:18:57,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742334_1513, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.4:9866] terminating
2019-03-01 10:18:57,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742340_1519 src: /10.10.1.6:44722 dest: /10.10.1.2:9866
2019-03-01 10:18:57,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742341_1520 src: /10.10.1.4:40818 dest: /10.10.1.2:9866
2019-03-01 10:18:57,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742342_1521 src: /10.10.1.5:57286 dest: /10.10.1.2:9866
2019-03-01 10:18:57,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742343_1522 src: /10.10.1.4:40820 dest: /10.10.1.2:9866
2019-03-01 10:18:57,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:44722, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_28590724_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742340_1519, duration(ns): 658282968
2019-03-01 10:18:57,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742340_1519, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.4:9866] terminating
2019-03-01 10:18:57,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742344_1523 src: /10.10.1.4:40822 dest: /10.10.1.2:9866
2019-03-01 10:18:58,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40818, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1953040112_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742341_1520, duration(ns): 557572266
2019-03-01 10:18:58,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742341_1520, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:18:58,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40820, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1131378456_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742343_1522, duration(ns): 631217126
2019-03-01 10:18:58,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742343_1522, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:18:58,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40822, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_28590724_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742344_1523, duration(ns): 637184643
2019-03-01 10:18:58,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742344_1523, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:19:00,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57286, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1772450466_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742342_1521, duration(ns): 3010607278
2019-03-01 10:19:00,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742342_1521, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:19:00,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742346_1525 src: /10.10.1.3:47368 dest: /10.10.1.2:9866
2019-03-01 10:19:01,245 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47368, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1953040112_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742346_1525, duration(ns): 382845510
2019-03-01 10:19:01,245 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742346_1525, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:19:01,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742349_1528 src: /10.10.1.6:44766 dest: /10.10.1.2:9866
2019-03-01 10:19:05,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742347_1526 src: /10.10.1.5:57290 dest: /10.10.1.2:9866
2019-03-01 10:19:05,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57290, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_28590724_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742347_1526, duration(ns): 610700668
2019-03-01 10:19:05,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742347_1526, type=LAST_IN_PIPELINE terminating
2019-03-01 10:19:06,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742352_1531 src: /10.10.1.6:44792 dest: /10.10.1.2:9866
2019-03-01 10:19:10,791 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 1637ms (threshold=300ms), downstream DNs=[10.10.1.5:9866, 10.10.1.4:9866], blockId=1073742349
2019-03-01 10:19:10,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742353_1532 src: /10.10.1.3:47374 dest: /10.10.1.2:9866
2019-03-01 10:19:11,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47374, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_830062780_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742353_1532, duration(ns): 763515895
2019-03-01 10:19:11,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742353_1532, type=LAST_IN_PIPELINE terminating
2019-03-01 10:19:11,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742354_1533 src: /10.10.1.6:44824 dest: /10.10.1.2:9866
2019-03-01 10:19:11,736 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 809ms (threshold=300ms), downstream DNs=[10.10.1.4:9866, 10.10.1.5:9866], blockId=1073742352
2019-03-01 10:19:12,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:44792, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_969299202_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742352_1531, duration(ns): 1536091961
2019-03-01 10:19:12,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742352_1531, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.5:9866] terminating
2019-03-01 10:19:12,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:44766, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1953040112_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742349_1528, duration(ns): 3188056053
2019-03-01 10:19:12,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742349_1528, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.4:9866] terminating
2019-03-01 10:19:12,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742356_1535 src: /10.10.1.4:40830 dest: /10.10.1.2:9866
2019-03-01 10:19:12,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742357_1536 src: /10.10.1.5:57304 dest: /10.10.1.2:9866
2019-03-01 10:19:12,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742358_1537 src: /10.10.1.5:57306 dest: /10.10.1.2:9866
2019-03-01 10:19:12,582 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742359_1538 src: /10.10.1.5:57308 dest: /10.10.1.2:9866
2019-03-01 10:19:12,629 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:44824, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_830062780_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742354_1533, duration(ns): 1005889431
2019-03-01 10:19:12,629 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742354_1533, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.3:9866] terminating
2019-03-01 10:19:13,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57304, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1953040112_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742357_1536, duration(ns): 693594438
2019-03-01 10:19:13,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742357_1536, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:19:13,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57306, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1131378456_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742358_1537, duration(ns): 581701338
2019-03-01 10:19:13,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742358_1537, type=LAST_IN_PIPELINE terminating
2019-03-01 10:19:13,245 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57308, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1772450466_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742359_1538, duration(ns): 649888723
2019-03-01 10:19:13,245 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742359_1538, type=LAST_IN_PIPELINE terminating
2019-03-01 10:19:18,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40830, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_969299202_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742356_1535, duration(ns): 6443704032
2019-03-01 10:19:18,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742356_1535, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.5:9866] terminating
2019-03-01 10:19:18,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742364_1543 src: /10.10.1.6:44886 dest: /10.10.1.2:9866
2019-03-01 10:19:18,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742361_1540 src: /10.10.1.3:47384 dest: /10.10.1.2:9866
2019-03-01 10:19:18,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742363_1542 src: /10.10.1.4:40836 dest: /10.10.1.2:9866
2019-03-01 10:19:18,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742362_1541 src: /10.10.1.3:47386 dest: /10.10.1.2:9866
2019-03-01 10:19:19,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742366_1545 src: /10.10.1.6:44892 dest: /10.10.1.2:9866
2019-03-01 10:19:19,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47384, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_28590724_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742361_1540, duration(ns): 573590608
2019-03-01 10:19:19,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742361_1540, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.5:9866] terminating
2019-03-01 10:19:19,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47386, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1772450466_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742362_1541, duration(ns): 587598929
2019-03-01 10:19:19,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742362_1541, type=LAST_IN_PIPELINE terminating
2019-03-01 10:19:19,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40836, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1131378456_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742363_1542, duration(ns): 615350690
2019-03-01 10:19:19,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742363_1542, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.5:9866] terminating
2019-03-01 10:19:19,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742368_1547 src: /10.10.1.4:40844 dest: /10.10.1.2:9866
2019-03-01 10:19:19,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:44886, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_969299202_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742364_1543, duration(ns): 683719651
2019-03-01 10:19:19,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742364_1543, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.4:9866] terminating
2019-03-01 10:19:19,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:44892, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_830062780_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742366_1545, duration(ns): 452041297
2019-03-01 10:19:19,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742366_1545, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.5:9866] terminating
2019-03-01 10:19:20,823 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742370_1549 src: /10.10.1.4:40848 dest: /10.10.1.2:9866
2019-03-01 10:19:21,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40844, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1131378456_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742368_1547, duration(ns): 1543812495
2019-03-01 10:19:21,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742368_1547, type=LAST_IN_PIPELINE terminating
2019-03-01 10:19:21,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40848, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_830062780_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742370_1549, duration(ns): 983207546
2019-03-01 10:19:21,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742370_1549, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:19:21,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742371_1550 src: /10.10.1.6:44926 dest: /10.10.1.2:9866
2019-03-01 10:19:24,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742372_1551 src: /10.10.1.4:40850 dest: /10.10.1.2:9866
2019-03-01 10:19:24,956 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40850, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1053574486_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742372_1551, duration(ns): 413967039
2019-03-01 10:19:24,956 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742372_1551, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:19:24,996 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742373_1552 src: /10.10.1.4:40852 dest: /10.10.1.2:9866
2019-03-01 10:19:25,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742374_1553 src: /10.10.1.5:57320 dest: /10.10.1.2:9866
2019-03-01 10:19:25,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:44926, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_830062780_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742371_1550, duration(ns): 702017875
2019-03-01 10:19:25,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742371_1550, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.3:9866] terminating
2019-03-01 10:19:25,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742375_1554 src: /10.10.1.4:40854 dest: /10.10.1.2:9866
2019-03-01 10:19:25,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40852, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1053574486_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742373_1552, duration(ns): 523043141
2019-03-01 10:19:25,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742373_1552, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:19:25,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742377_1556 src: /10.10.1.6:44968 dest: /10.10.1.2:9866
2019-03-01 10:19:25,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40854, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_830062780_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742375_1554, duration(ns): 418820141
2019-03-01 10:19:25,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742375_1554, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:19:25,776 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57320, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1772450466_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742374_1553, duration(ns): 729638720
2019-03-01 10:19:25,776 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742374_1553, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:19:26,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:44968, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1053574486_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742377_1556, duration(ns): 474062538
2019-03-01 10:19:26,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742377_1556, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.3:9866] terminating
2019-03-01 10:19:26,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742379_1558 src: /10.10.1.6:44978 dest: /10.10.1.2:9866
2019-03-01 10:19:30,000 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1854ms (threshold=300ms), volume=file:/root/data/, blockId=1073742379
2019-03-01 10:19:32,143 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:44978, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1053574486_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742379_1558, duration(ns): 3998136130
2019-03-01 10:19:32,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742379_1558, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.3:9866] terminating
2019-03-01 10:19:32,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742381_1560 src: /10.10.1.6:45036 dest: /10.10.1.2:9866
2019-03-01 10:19:32,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:45036, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1053574486_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742381_1560, duration(ns): 485931845
2019-03-01 10:19:32,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742381_1560, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.4:9866] terminating
2019-03-01 10:19:32,726 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742382_1561 src: /10.10.1.3:47398 dest: /10.10.1.2:9866
2019-03-01 10:19:32,966 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742383_1562 src: /10.10.1.3:47400 dest: /10.10.1.2:9866
2019-03-01 10:19:33,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47398, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1053574486_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742382_1561, duration(ns): 460738685
2019-03-01 10:19:33,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742382_1561, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:19:33,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47400, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_969299202_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742383_1562, duration(ns): 428197548
2019-03-01 10:19:33,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742383_1562, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:19:33,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742384_1563 src: /10.10.1.4:40860 dest: /10.10.1.2:9866
2019-03-01 10:19:33,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40860, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_969299202_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742384_1563, duration(ns): 431632112
2019-03-01 10:19:33,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742384_1563, type=LAST_IN_PIPELINE terminating
2019-03-01 10:19:47,017 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742385_1564 src: /10.10.1.6:45170 dest: /10.10.1.2:9866
2019-03-01 10:19:51,226 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1325ms (threshold=300ms), volume=file:/root/data/, blockId=1073742385
2019-03-01 10:19:51,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:45170, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2009001100_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742385_1564, duration(ns): 1801422760
2019-03-01 10:19:51,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742385_1564, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.5:9866] terminating
2019-03-01 10:20:03,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742388_1567 src: /10.10.1.4:40878 dest: /10.10.1.2:9866
2019-03-01 10:20:06,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742390_1569 src: /10.10.1.6:45346 dest: /10.10.1.2:9866
2019-03-01 10:20:06,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742389_1568 src: /10.10.1.5:57334 dest: /10.10.1.2:9866
2019-03-01 10:20:07,579 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:857ms (threshold=300ms), volume=file:/root/data/, blockId=1073742388
2019-03-01 10:20:08,984 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 1285ms (threshold=300ms), downstream DNs=[10.10.1.3:9866], blockId=1073742388
2019-03-01 10:20:09,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40878, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1667938742_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742388_1567, duration(ns): 2711657654
2019-03-01 10:20:09,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742388_1567, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:20:11,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742392_1571 src: /10.10.1.6:45380 dest: /10.10.1.2:9866
2019-03-01 10:20:12,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742391_1570 src: /10.10.1.5:57336 dest: /10.10.1.2:9866
2019-03-01 10:20:13,606 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1303ms (threshold=300ms), volume=file:/root/data/, blockId=1073742390
2019-03-01 10:20:13,606 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1303ms (threshold=300ms), volume=file:/root/data/, blockId=1073742389
2019-03-01 10:20:14,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57334, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_717932894_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742389_1568, duration(ns): 5205643013
2019-03-01 10:20:14,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742389_1568, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:20:14,476 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57336, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1586943642_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742391_1570, duration(ns): 868324254
2019-03-01 10:20:14,476 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742391_1570, type=LAST_IN_PIPELINE terminating
2019-03-01 10:20:15,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:45380, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-438060798_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742392_1571, duration(ns): 866682866
2019-03-01 10:20:15,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742392_1571, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.3:9866] terminating
2019-03-01 10:20:17,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:45346, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2009001100_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742390_1569, duration(ns): 5258913352
2019-03-01 10:20:17,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742390_1569, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.5:9866] terminating
2019-03-01 10:20:17,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742395_1574 src: /10.10.1.3:47414 dest: /10.10.1.2:9866
2019-03-01 10:20:17,594 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742398_1577 src: /10.10.1.4:40892 dest: /10.10.1.2:9866
2019-03-01 10:20:17,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742396_1575 src: /10.10.1.5:57340 dest: /10.10.1.2:9866
2019-03-01 10:20:18,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57340, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2009001100_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742396_1575, duration(ns): 550277317
2019-03-01 10:20:18,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742396_1575, type=LAST_IN_PIPELINE terminating
2019-03-01 10:20:18,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742400_1579 src: /10.10.1.6:45436 dest: /10.10.1.2:9866
2019-03-01 10:20:18,214 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742399_1578 src: /10.10.1.3:47420 dest: /10.10.1.2:9866
2019-03-01 10:20:18,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47414, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1667938742_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742395_1574, duration(ns): 650974433
2019-03-01 10:20:18,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742395_1574, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.5:9866] terminating
2019-03-01 10:20:18,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40892, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_717932894_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742398_1577, duration(ns): 666424977
2019-03-01 10:20:18,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742398_1577, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:20:18,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742401_1580 src: /10.10.1.5:57342 dest: /10.10.1.2:9866
2019-03-01 10:20:18,308 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742403_1582 src: /10.10.1.6:45444 dest: /10.10.1.2:9866
2019-03-01 10:20:18,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742402_1581 src: /10.10.1.4:40896 dest: /10.10.1.2:9866
2019-03-01 10:20:18,736 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:45436, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-438060798_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742400_1579, duration(ns): 519955261
2019-03-01 10:20:18,736 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742400_1579, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.5:9866] terminating
2019-03-01 10:20:18,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742404_1583 src: /10.10.1.5:57344 dest: /10.10.1.2:9866
2019-03-01 10:20:18,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47420, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2009001100_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742399_1578, duration(ns): 583293730
2019-03-01 10:20:18,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742399_1578, type=LAST_IN_PIPELINE terminating
2019-03-01 10:20:18,866 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742405_1584 src: /10.10.1.6:45456 dest: /10.10.1.2:9866
2019-03-01 10:20:18,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:45444, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_717932894_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742403_1582, duration(ns): 676200933
2019-03-01 10:20:18,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742403_1582, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.4:9866, 10.10.1.3:9866] terminating
2019-03-01 10:20:19,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40896, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1586943642_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742402_1581, duration(ns): 690130810
2019-03-01 10:20:19,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742402_1581, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:20:19,037 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742406_1585 src: /10.10.1.6:45464 dest: /10.10.1.2:9866
2019-03-01 10:20:19,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:45456, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2009001100_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742405_1584, duration(ns): 426106942
2019-03-01 10:20:19,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742405_1584, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.4:9866] terminating
2019-03-01 10:20:19,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742407_1586 src: /10.10.1.3:47424 dest: /10.10.1.2:9866
2019-03-01 10:20:19,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742408_1587 src: /10.10.1.3:47426 dest: /10.10.1.2:9866
2019-03-01 10:20:19,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57344, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-438060798_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742404_1583, duration(ns): 1065408809
2019-03-01 10:20:19,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742404_1583, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:20:20,037 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47424, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1586943642_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742407_1586, duration(ns): 661133101
2019-03-01 10:20:20,037 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742407_1586, type=LAST_IN_PIPELINE terminating
2019-03-01 10:20:20,049 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47426, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2009001100_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742408_1587, duration(ns): 672908674
2019-03-01 10:20:20,049 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742408_1587, type=LAST_IN_PIPELINE terminating
2019-03-01 10:20:21,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:45464, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_717932894_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742406_1585, duration(ns): 2479462915
2019-03-01 10:20:21,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742406_1585, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.3:9866] terminating
2019-03-01 10:20:22,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57342, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1667938742_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742401_1580, duration(ns): 4113866097
2019-03-01 10:20:22,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742401_1580, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:20:22,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742411_1590 src: /10.10.1.4:40900 dest: /10.10.1.2:9866
2019-03-01 10:20:22,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742412_1591 src: /10.10.1.4:40904 dest: /10.10.1.2:9866
2019-03-01 10:20:22,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40900, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_717932894_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742411_1590, duration(ns): 548886667
2019-03-01 10:20:22,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742411_1590, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:20:22,992 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742414_1593 src: /10.10.1.5:57358 dest: /10.10.1.2:9866
2019-03-01 10:20:23,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40904, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1667938742_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742412_1591, duration(ns): 690190893
2019-03-01 10:20:23,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742412_1591, type=LAST_IN_PIPELINE terminating
2019-03-01 10:20:23,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742415_1594 src: /10.10.1.4:40908 dest: /10.10.1.2:9866
2019-03-01 10:20:23,698 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40908, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1667938742_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742415_1594, duration(ns): 519142641
2019-03-01 10:20:23,698 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742415_1594, type=LAST_IN_PIPELINE terminating
2019-03-01 10:20:28,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57358, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_717932894_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742414_1593, duration(ns): 5473078559
2019-03-01 10:20:28,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742414_1593, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-03-01 10:20:28,662 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742419_1598 src: /10.10.1.6:45554 dest: /10.10.1.2:9866
2019-03-01 10:20:28,663 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742418_1597 src: /10.10.1.5:57360 dest: /10.10.1.2:9866
2019-03-01 10:20:28,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742417_1596 src: /10.10.1.4:40910 dest: /10.10.1.2:9866
2019-03-01 10:20:28,666 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742416_1595 src: /10.10.1.5:57364 dest: /10.10.1.2:9866
2019-03-01 10:20:29,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:45554, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_717932894_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742419_1598, duration(ns): 392860024
2019-03-01 10:20:29,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742419_1598, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.4:9866] terminating
2019-03-01 10:20:29,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57360, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-438060798_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742418_1597, duration(ns): 430673122
2019-03-01 10:20:29,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742418_1597, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.4:9866] terminating
2019-03-01 10:20:29,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742420_1599 src: /10.10.1.5:57366 dest: /10.10.1.2:9866
2019-03-01 10:20:29,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742421_1600 src: /10.10.1.6:45562 dest: /10.10.1.2:9866
2019-03-01 10:20:29,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:45562, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-438060798_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742421_1600, duration(ns): 445495208
2019-03-01 10:20:29,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742421_1600, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.5:9866, 10.10.1.4:9866] terminating
2019-03-01 10:20:29,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40910, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1586943642_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742417_1596, duration(ns): 1084235128
2019-03-01 10:20:29,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742417_1596, type=LAST_IN_PIPELINE terminating
2019-03-01 10:20:29,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57364, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1667938742_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742416_1595, duration(ns): 1218719762
2019-03-01 10:20:29,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742416_1595, type=LAST_IN_PIPELINE terminating
2019-03-01 10:20:30,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.5:57366, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_717932894_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742420_1599, duration(ns): 1033039898
2019-03-01 10:20:30,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742420_1599, type=LAST_IN_PIPELINE terminating
2019-03-01 10:20:34,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742422_1601 src: /10.10.1.4:40912 dest: /10.10.1.2:9866
2019-03-01 10:20:34,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-258834523-130.127.133.101-1551459924751:blk_1073742423_1602 src: /10.10.1.3:47440 dest: /10.10.1.2:9866
2019-03-01 10:20:34,742 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.4:40912, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1586943642_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742422_1601, duration(ns): 401369225
2019-03-01 10:20:34,742 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742422_1601, type=LAST_IN_PIPELINE terminating
2019-03-01 10:20:34,928 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:47440, dest: /10.10.1.2:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1667938742_1, offset: 0, srvID: f33fa295-0841-426e-9ba0-92fb033d0f2d, blockid: BP-258834523-130.127.133.101-1551459924751:blk_1073742423_1602, duration(ns): 555102284
2019-03-01 10:20:34,928 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-258834523-130.127.133.101-1551459924751:blk_1073742423_1602, type=LAST_IN_PIPELINE terminating
2019-03-01 10:20:36,491 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742304_1483 replica FinalizedReplica, blk_1073742304_1483, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742304 for deletion
2019-03-01 10:20:36,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742338_1517 replica FinalizedReplica, blk_1073742338_1517, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742338 for deletion
2019-03-01 10:20:36,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742341_1520 replica FinalizedReplica, blk_1073742341_1520, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742341 for deletion
2019-03-01 10:20:36,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742346_1525 replica FinalizedReplica, blk_1073742346_1525, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742346 for deletion
2019-03-01 10:20:36,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742349_1528 replica FinalizedReplica, blk_1073742349_1528, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742349 for deletion
2019-03-01 10:20:36,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742383_1562 replica FinalizedReplica, blk_1073742383_1562, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742383 for deletion
2019-03-01 10:20:36,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742288_1467 replica FinalizedReplica, blk_1073742288_1467, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742288 for deletion
2019-03-01 10:20:36,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742352_1531 replica FinalizedReplica, blk_1073742352_1531, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742352 for deletion
2019-03-01 10:20:36,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742384_1563 replica FinalizedReplica, blk_1073742384_1563, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742384 for deletion
2019-03-01 10:20:36,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742356_1535 replica FinalizedReplica, blk_1073742356_1535, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742356 for deletion
2019-03-01 10:20:36,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742357_1536 replica FinalizedReplica, blk_1073742357_1536, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742357 for deletion
2019-03-01 10:20:36,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742298_1477 replica FinalizedReplica, blk_1073742298_1477, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742298 for deletion
2019-03-01 10:20:36,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742331_1510 replica FinalizedReplica, blk_1073742331_1510, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742331 for deletion
2019-03-01 10:20:36,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742299_1478 replica FinalizedReplica, blk_1073742299_1478, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742299 for deletion
2019-03-01 10:20:36,494 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742364_1543 replica FinalizedReplica, blk_1073742364_1543, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742364 for deletion
2019-03-01 10:20:36,494 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742302_1481 replica FinalizedReplica, blk_1073742302_1481, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742302 for deletion
2019-03-01 10:20:36,494 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742335_1514 replica FinalizedReplica, blk_1073742335_1514, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742335 for deletion
2019-03-01 10:20:36,517 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742304_1483 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742304
2019-03-01 10:20:36,539 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742338_1517 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742338
2019-03-01 10:20:36,562 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742341_1520 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742341
2019-03-01 10:20:36,584 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742346_1525 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742346
2019-03-01 10:20:36,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742349_1528 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742349
2019-03-01 10:20:36,627 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742383_1562 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742383
2019-03-01 10:20:36,650 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742288_1467 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742288
2019-03-01 10:20:36,671 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742352_1531 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742352
2019-03-01 10:20:36,692 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742384_1563 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742384
2019-03-01 10:20:36,714 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742356_1535 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742356
2019-03-01 10:20:36,736 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742357_1536 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742357
2019-03-01 10:20:36,762 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742298_1477 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742298
2019-03-01 10:20:36,785 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742331_1510 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742331
2019-03-01 10:20:36,810 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742299_1478 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742299
2019-03-01 10:20:36,832 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742364_1543 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742364
2019-03-01 10:20:36,859 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742302_1481 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742302
2019-03-01 10:20:36,883 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742335_1514 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742335
2019-03-01 10:20:42,491 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742320_1499 replica FinalizedReplica, blk_1073742320_1499, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742320 for deletion
2019-03-01 10:20:42,491 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742308_1487 replica FinalizedReplica, blk_1073742308_1487, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742308 for deletion
2019-03-01 10:20:42,491 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742309_1488 replica FinalizedReplica, blk_1073742309_1488, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742309 for deletion
2019-03-01 10:20:42,491 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742312_1491 replica FinalizedReplica, blk_1073742312_1491, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742312 for deletion
2019-03-01 10:20:42,491 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742318_1497 replica FinalizedReplica, blk_1073742318_1497, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742318 for deletion
2019-03-01 10:20:42,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742319_1498 replica FinalizedReplica, blk_1073742319_1498, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742319 for deletion
2019-03-01 10:20:42,513 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742320_1499 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742320
2019-03-01 10:20:42,536 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742308_1487 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742308
2019-03-01 10:20:42,559 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742309_1488 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742309
2019-03-01 10:20:42,583 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742312_1491 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742312
2019-03-01 10:20:42,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742318_1497 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742318
2019-03-01 10:20:42,627 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742319_1498 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742319
2019-03-01 10:20:45,491 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742370_1549 replica FinalizedReplica, blk_1073742370_1549, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742370 for deletion
2019-03-01 10:20:45,491 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742371_1550 replica FinalizedReplica, blk_1073742371_1550, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742371 for deletion
2019-03-01 10:20:45,491 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742340_1519 replica FinalizedReplica, blk_1073742340_1519, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742340 for deletion
2019-03-01 10:20:45,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742375_1554 replica FinalizedReplica, blk_1073742375_1554, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742375 for deletion
2019-03-01 10:20:45,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742344_1523 replica FinalizedReplica, blk_1073742344_1523, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742344 for deletion
2019-03-01 10:20:45,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742347_1526 replica FinalizedReplica, blk_1073742347_1526, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742347 for deletion
2019-03-01 10:20:45,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742321_1500 replica FinalizedReplica, blk_1073742321_1500, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742321 for deletion
2019-03-01 10:20:45,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742353_1532 replica FinalizedReplica, blk_1073742353_1532, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742353 for deletion
2019-03-01 10:20:45,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742322_1501 replica FinalizedReplica, blk_1073742322_1501, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742322 for deletion
2019-03-01 10:20:45,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742354_1533 replica FinalizedReplica, blk_1073742354_1533, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742354 for deletion
2019-03-01 10:20:45,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742323_1502 replica FinalizedReplica, blk_1073742323_1502, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742323 for deletion
2019-03-01 10:20:45,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742324_1503 replica FinalizedReplica, blk_1073742324_1503, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742324 for deletion
2019-03-01 10:20:45,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742326_1505 replica FinalizedReplica, blk_1073742326_1505, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742326 for deletion
2019-03-01 10:20:45,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742361_1540 replica FinalizedReplica, blk_1073742361_1540, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742361 for deletion
2019-03-01 10:20:45,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742329_1508 replica FinalizedReplica, blk_1073742329_1508, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742329 for deletion
2019-03-01 10:20:45,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742330_1509 replica FinalizedReplica, blk_1073742330_1509, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742330 for deletion
2019-03-01 10:20:45,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742334_1513 replica FinalizedReplica, blk_1073742334_1513, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742334 for deletion
2019-03-01 10:20:45,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742366_1545 replica FinalizedReplica, blk_1073742366_1545, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742366 for deletion
2019-03-01 10:20:45,512 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742370_1549 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742370
2019-03-01 10:20:45,532 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742371_1550 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742371
2019-03-01 10:20:45,555 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742340_1519 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742340
2019-03-01 10:20:45,576 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742375_1554 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742375
2019-03-01 10:20:45,598 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742344_1523 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742344
2019-03-01 10:20:45,619 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742347_1526 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742347
2019-03-01 10:20:45,641 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742321_1500 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742321
2019-03-01 10:20:45,661 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742353_1532 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742353
2019-03-01 10:20:45,684 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742322_1501 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742322
2019-03-01 10:20:45,705 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742354_1533 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742354
2019-03-01 10:20:45,727 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742323_1502 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742323
2019-03-01 10:20:45,749 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742324_1503 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742324
2019-03-01 10:20:45,772 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742326_1505 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742326
2019-03-01 10:20:45,793 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742361_1540 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742361
2019-03-01 10:20:45,815 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742329_1508 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742329
2019-03-01 10:20:45,837 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742330_1509 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742330
2019-03-01 10:20:45,860 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742334_1513 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742334
2019-03-01 10:20:45,880 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742366_1545 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742366
2019-03-01 10:20:57,491 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742400_1579 replica FinalizedReplica, blk_1073742400_1579, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742400 for deletion
2019-03-01 10:20:57,491 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742403_1582 replica FinalizedReplica, blk_1073742403_1582, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742403 for deletion
2019-03-01 10:20:57,491 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742404_1583 replica FinalizedReplica, blk_1073742404_1583, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742404 for deletion
2019-03-01 10:20:57,491 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742405_1584 replica FinalizedReplica, blk_1073742405_1584, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742405 for deletion
2019-03-01 10:20:57,491 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742406_1585 replica FinalizedReplica, blk_1073742406_1585, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742406 for deletion
2019-03-01 10:20:57,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742342_1521 replica FinalizedReplica, blk_1073742342_1521, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742342 for deletion
2019-03-01 10:20:57,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742408_1587 replica FinalizedReplica, blk_1073742408_1587, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742408 for deletion
2019-03-01 10:20:57,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742411_1590 replica FinalizedReplica, blk_1073742411_1590, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742411 for deletion
2019-03-01 10:20:57,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742414_1593 replica FinalizedReplica, blk_1073742414_1593, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742414 for deletion
2019-03-01 10:20:57,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742418_1597 replica FinalizedReplica, blk_1073742418_1597, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742418 for deletion
2019-03-01 10:20:57,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742419_1598 replica FinalizedReplica, blk_1073742419_1598, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742419 for deletion
2019-03-01 10:20:57,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742420_1599 replica FinalizedReplica, blk_1073742420_1599, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742420 for deletion
2019-03-01 10:20:57,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742421_1600 replica FinalizedReplica, blk_1073742421_1600, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742421 for deletion
2019-03-01 10:20:57,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742359_1538 replica FinalizedReplica, blk_1073742359_1538, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742359 for deletion
2019-03-01 10:20:57,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742362_1541 replica FinalizedReplica, blk_1073742362_1541, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742362 for deletion
2019-03-01 10:20:57,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742372_1551 replica FinalizedReplica, blk_1073742372_1551, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742372 for deletion
2019-03-01 10:20:57,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742373_1552 replica FinalizedReplica, blk_1073742373_1552, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742373 for deletion
2019-03-01 10:20:57,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742374_1553 replica FinalizedReplica, blk_1073742374_1553, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742374 for deletion
2019-03-01 10:20:57,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742377_1556 replica FinalizedReplica, blk_1073742377_1556, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742377 for deletion
2019-03-01 10:20:57,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742379_1558 replica FinalizedReplica, blk_1073742379_1558, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742379 for deletion
2019-03-01 10:20:57,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742381_1560 replica FinalizedReplica, blk_1073742381_1560, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742381 for deletion
2019-03-01 10:20:57,494 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742382_1561 replica FinalizedReplica, blk_1073742382_1561, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742382 for deletion
2019-03-01 10:20:57,494 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742385_1564 replica FinalizedReplica, blk_1073742385_1564, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742385 for deletion
2019-03-01 10:20:57,494 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742389_1568 replica FinalizedReplica, blk_1073742389_1568, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742389 for deletion
2019-03-01 10:20:57,494 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742390_1569 replica FinalizedReplica, blk_1073742390_1569, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742390 for deletion
2019-03-01 10:20:57,494 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742392_1571 replica FinalizedReplica, blk_1073742392_1571, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742392 for deletion
2019-03-01 10:20:57,494 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742396_1575 replica FinalizedReplica, blk_1073742396_1575, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742396 for deletion
2019-03-01 10:20:57,494 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742398_1577 replica FinalizedReplica, blk_1073742398_1577, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742398 for deletion
2019-03-01 10:20:57,495 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742399_1578 replica FinalizedReplica, blk_1073742399_1578, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742399 for deletion
2019-03-01 10:20:59,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742400_1579 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742400
2019-03-01 10:21:00,105 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742403_1582 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742403
2019-03-01 10:21:05,574 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742404_1583 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742404
2019-03-01 10:21:06,491 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742368_1547 replica FinalizedReplica, blk_1073742368_1547, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742368 for deletion
2019-03-01 10:21:06,491 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742401_1580 replica FinalizedReplica, blk_1073742401_1580, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742401 for deletion
2019-03-01 10:21:06,491 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742402_1581 replica FinalizedReplica, blk_1073742402_1581, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742402 for deletion
2019-03-01 10:21:06,491 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742311_1490 replica FinalizedReplica, blk_1073742311_1490, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742311 for deletion
2019-03-01 10:21:06,491 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742407_1586 replica FinalizedReplica, blk_1073742407_1586, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742407 for deletion
2019-03-01 10:21:06,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742343_1522 replica FinalizedReplica, blk_1073742343_1522, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742343 for deletion
2019-03-01 10:21:06,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742313_1492 replica FinalizedReplica, blk_1073742313_1492, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742313 for deletion
2019-03-01 10:21:06,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742315_1494 replica FinalizedReplica, blk_1073742315_1494, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742315 for deletion
2019-03-01 10:21:06,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742316_1495 replica FinalizedReplica, blk_1073742316_1495, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742316 for deletion
2019-03-01 10:21:06,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742412_1591 replica FinalizedReplica, blk_1073742412_1591, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742412 for deletion
2019-03-01 10:21:06,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742415_1594 replica FinalizedReplica, blk_1073742415_1594, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742415 for deletion
2019-03-01 10:21:06,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742416_1595 replica FinalizedReplica, blk_1073742416_1595, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742416 for deletion
2019-03-01 10:21:06,492 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742417_1596 replica FinalizedReplica, blk_1073742417_1596, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742417 for deletion
2019-03-01 10:21:06,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742388_1567 replica FinalizedReplica, blk_1073742388_1567, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742388 for deletion
2019-03-01 10:21:06,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742358_1537 replica FinalizedReplica, blk_1073742358_1537, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742358 for deletion
2019-03-01 10:21:06,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742422_1601 replica FinalizedReplica, blk_1073742422_1601, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742422 for deletion
2019-03-01 10:21:06,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742391_1570 replica FinalizedReplica, blk_1073742391_1570, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742391 for deletion
2019-03-01 10:21:06,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742423_1602 replica FinalizedReplica, blk_1073742423_1602, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742423 for deletion
2019-03-01 10:21:06,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742395_1574 replica FinalizedReplica, blk_1073742395_1574, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742395 for deletion
2019-03-01 10:21:06,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742363_1542 replica FinalizedReplica, blk_1073742363_1542, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742363 for deletion
2019-03-01 10:21:06,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742333_1512 replica FinalizedReplica, blk_1073742333_1512, FINALIZED
  getNumBytes()     = 134217728
  getBytesOnDisk()  = 134217728
  getVisibleLength()= 134217728
  getVolume()       = /root/data
  getBlockURI()     = file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir1/blk_1073742333 for deletion
2019-03-01 10:21:08,031 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742405_1584 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742405
2019-03-01 10:21:11,412 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742406_1585 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742406
2019-03-01 10:21:11,436 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742342_1521 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742342
2019-03-01 10:21:14,034 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-258834523-130.127.133.101-1551459924751 blk_1073742408_1587 URI file:/root/data/current/BP-258834523-130.127.133.101-1551459924751/current/finalized/subdir0/subdir2/blk_1073742408
