2019-02-05 00:33:45,107 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = clnode092.clemson.cloudlab.us/130.127.133.101
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-02-05T05:13Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-02-05 00:33:45,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-05 00:33:45,698 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/root/data
2019-02-05 00:33:45,907 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-05 00:33:45,983 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-02-05 00:33:45,983 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-02-05 00:33:46,245 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-02-05 00:33:46,249 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-02-05 00:33:46,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is clnode092.clemson.cloudlab.us
2019-02-05 00:33:46,255 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-02-05 00:33:46,259 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-02-05 00:33:46,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:9866
2019-02-05 00:33:46,285 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwidth is 10485760 bytes/s
2019-02-05 00:33:46,285 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-02-05 00:33:46,402 INFO org.eclipse.jetty.util.log: Logging initialized @1813ms
2019-02-05 00:33:46,513 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-05 00:33:46,517 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-02-05 00:33:46,522 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-05 00:33:46,524 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-02-05 00:33:46,524 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-05 00:33:46,524 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-05 00:33:46,547 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 46004
2019-02-05 00:33:46,549 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2019-02-05 00:33:46,583 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@48e92c5c{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-02-05 00:33:46,584 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@22356acd{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-02-05 00:33:46,654 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@366ac49b{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/datanode/,AVAILABLE}{/datanode}
2019-02-05 00:33:46,661 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@34cdeda2{HTTP/1.1,[http/1.1]}{localhost:46004}
2019-02-05 00:33:46,661 INFO org.eclipse.jetty.server.Server: Started @2073ms
2019-02-05 00:33:46,870 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864
2019-02-05 00:33:46,877 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-02-05 00:33:46,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2019-02-05 00:33:46,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-02-05 00:33:46,974 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-02-05 00:33:46,989 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9867
2019-02-05 00:33:47,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:9867
2019-02-05 00:33:47,049 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: mycluster
2019-02-05 00:33:47,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: mycluster
2019-02-05 00:33:47,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to node-0-link-0/10.10.1.1:8020 starting to offer service
2019-02-05 00:33:47,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to node-1-link-0/10.10.1.4:8020 starting to offer service
2019-02-05 00:33:47,079 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-05 00:33:47,079 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9867: starting
2019-02-05 00:33:48,163 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:33:48,163 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:33:49,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:33:49,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:33:50,165 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:33:50,165 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:33:51,166 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:33:51,166 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:33:52,167 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:33:52,167 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:33:52,374 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-02-05 00:33:52,411 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /root/data/in_use.lock acquired by nodename 82536@clnode092.clemson.cloudlab.us
2019-02-05 00:33:52,412 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory with location [DISK]file:/root/data is not formatted for namespace 1636360670. Formatting...
2019-02-05 00:33:52,413 INFO org.apache.hadoop.hdfs.server.common.Storage: Generated new storageID DS-50c35dca-477b-4651-a485-463fc0231236 for directory /root/data 
2019-02-05 00:33:52,485 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1033126199-130.127.133.53-1549352001420
2019-02-05 00:33:52,485 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /root/data/current/BP-1033126199-130.127.133.53-1549352001420
2019-02-05 00:33:52,486 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory for location [DISK]file:/root/data and block pool id BP-1033126199-130.127.133.53-1549352001420 is not formatted. Formatting ...
2019-02-05 00:33:52,486 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1033126199-130.127.133.53-1549352001420 directory /root/data/current/BP-1033126199-130.127.133.53-1549352001420/current
2019-02-05 00:33:52,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1636360670;bpid=BP-1033126199-130.127.133.53-1549352001420;lv=-57;nsInfo=lv=-64;cid=CID-34248e42-4c58-4e5a-bd49-8efa31b0674e;nsid=1636360670;c=1549352001420;bpid=BP-1033126199-130.127.133.53-1549352001420;dnuuid=null
2019-02-05 00:33:52,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID b6dd557e-664c-46ca-8ab7-90489cef54b2
2019-02-05 00:33:52,634 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-50c35dca-477b-4651-a485-463fc0231236
2019-02-05 00:33:52,634 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - [DISK]file:/root/data, StorageType: DISK
2019-02-05 00:33:52,638 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-02-05 00:33:52,646 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for /root/data
2019-02-05 00:33:52,656 INFO org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker: Scheduled health check for volume /root/data
2019-02-05 00:33:52,658 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1033126199-130.127.133.53-1549352001420
2019-02-05 00:33:52,658 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1033126199-130.127.133.53-1549352001420 on volume /root/data...
2019-02-05 00:33:52,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1033126199-130.127.133.53-1549352001420 on /root/data: 16ms
2019-02-05 00:33:52,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1033126199-130.127.133.53-1549352001420: 16ms
2019-02-05 00:33:52,677 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1033126199-130.127.133.53-1549352001420 on volume /root/data...
2019-02-05 00:33:52,677 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /root/data/current/BP-1033126199-130.127.133.53-1549352001420/current/replicas doesn't exist 
2019-02-05 00:33:52,677 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1033126199-130.127.133.53-1549352001420 on volume /root/data: 0ms
2019-02-05 00:33:52,677 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2019-02-05 00:33:52,679 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-1033126199-130.127.133.53-1549352001420 on volume /root/data
2019-02-05 00:33:52,680 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/root/data, DS-50c35dca-477b-4651-a485-463fc0231236): finished scanning block pool BP-1033126199-130.127.133.53-1549352001420
2019-02-05 00:33:52,690 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 2/5/19 3:08 AM with interval of 21600000ms
2019-02-05 00:33:52,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1033126199-130.127.133.53-1549352001420 (Datanode Uuid b6dd557e-664c-46ca-8ab7-90489cef54b2) service to node-0-link-0/10.10.1.1:8020 beginning handshake with NN
2019-02-05 00:33:52,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1033126199-130.127.133.53-1549352001420 (Datanode Uuid b6dd557e-664c-46ca-8ab7-90489cef54b2) service to node-1-link-0/10.10.1.4:8020 beginning handshake with NN
2019-02-05 00:33:52,709 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/root/data, DS-50c35dca-477b-4651-a485-463fc0231236): no suitable block pools found to scan.  Waiting 1814399970 ms.
2019-02-05 00:33:52,741 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1033126199-130.127.133.53-1549352001420 (Datanode Uuid b6dd557e-664c-46ca-8ab7-90489cef54b2) service to node-0-link-0/10.10.1.1:8020 successfully registered with NN
2019-02-05 00:33:52,742 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode node-0-link-0/10.10.1.1:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-02-05 00:33:52,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1033126199-130.127.133.53-1549352001420 (Datanode Uuid b6dd557e-664c-46ca-8ab7-90489cef54b2) service to node-1-link-0/10.10.1.4:8020 successfully registered with NN
2019-02-05 00:33:52,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode node-1-link-0/10.10.1.4:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-02-05 00:33:52,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe9b1a62fb93cbba9,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 5 msec to generate and 61 msecs for RPC and NN processing. Got back no commands.
2019-02-05 00:33:52,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xea69e1ee0479a7a1,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 5 msec to generate and 61 msecs for RPC and NN processing. Got back no commands.
2019-02-05 00:33:55,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1033126199-130.127.133.53-1549352001420 (Datanode Uuid b6dd557e-664c-46ca-8ab7-90489cef54b2) service to node-1-link-0/10.10.1.4:8020 trying to claim ACTIVE state with txid=1
2019-02-05 00:33:55,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1033126199-130.127.133.53-1549352001420 (Datanode Uuid b6dd557e-664c-46ca-8ab7-90489cef54b2) service to node-1-link-0/10.10.1.4:8020
2019-02-05 00:33:58,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741828_1004 src: /10.10.1.6:36080 dest: /10.10.1.5:9866
2019-02-05 00:33:58,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741829_1005 src: /10.10.1.6:36078 dest: /10.10.1.5:9866
2019-02-05 00:33:58,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741825_1001 src: /10.10.1.6:36074 dest: /10.10.1.5:9866
2019-02-05 00:33:58,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741827_1003 src: /10.10.1.6:36072 dest: /10.10.1.5:9866
2019-02-05 00:33:58,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741826_1002 src: /10.10.1.6:36076 dest: /10.10.1.5:9866
2019-02-05 00:33:58,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:36072, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-411529749_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741827_1003, duration(ns): 643481058
2019-02-05 00:33:58,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:36078, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1951007782_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741829_1005, duration(ns): 646245624
2019-02-05 00:33:58,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.2:9866] terminating
2019-02-05 00:33:58,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741829_1005, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.2:9866, 10.10.1.3:9866] terminating
2019-02-05 00:33:58,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:36074, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_538516184_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741825_1001, duration(ns): 650986698
2019-02-05 00:33:58,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.2:9866] terminating
2019-02-05 00:33:58,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:36076, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1772495524_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741826_1002, duration(ns): 656202156
2019-02-05 00:33:58,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.2:9866, 10.10.1.3:9866] terminating
2019-02-05 00:33:58,841 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:36080, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_12203650_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741828_1004, duration(ns): 663187783
2019-02-05 00:33:58,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741828_1004, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.2:9866, 10.10.1.3:9866] terminating
2019-02-05 00:33:58,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741830_1006 src: /10.10.1.3:42566 dest: /10.10.1.5:9866
2019-02-05 00:33:58,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741831_1007 src: /10.10.1.3:42568 dest: /10.10.1.5:9866
2019-02-05 00:33:58,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741832_1008 src: /10.10.1.2:45250 dest: /10.10.1.5:9866
2019-02-05 00:33:58,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741834_1010 src: /10.10.1.3:42572 dest: /10.10.1.5:9866
2019-02-05 00:33:58,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741833_1009 src: /10.10.1.3:42574 dest: /10.10.1.5:9866
2019-02-05 00:33:59,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:42568, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1951007782_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741831_1007, duration(ns): 557241904
2019-02-05 00:33:59,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741831_1007, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.2:9866] terminating
2019-02-05 00:33:59,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:42566, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-411529749_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741830_1006, duration(ns): 571351481
2019-02-05 00:33:59,446 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741830_1006, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.2:9866] terminating
2019-02-05 00:33:59,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:42574, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1772495524_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741833_1009, duration(ns): 572201840
2019-02-05 00:33:59,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741833_1009, type=LAST_IN_PIPELINE terminating
2019-02-05 00:33:59,480 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.2:45250, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_538516184_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741832_1008, duration(ns): 607267370
2019-02-05 00:33:59,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741832_1008, type=LAST_IN_PIPELINE terminating
2019-02-05 00:33:59,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741836_1012 src: /10.10.1.2:45252 dest: /10.10.1.5:9866
2019-02-05 00:33:59,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741837_1013 src: /10.10.1.2:45256 dest: /10.10.1.5:9866
2019-02-05 00:33:59,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741835_1011 src: /10.10.1.3:42576 dest: /10.10.1.5:9866
2019-02-05 00:33:59,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:42572, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_12203650_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741834_1010, duration(ns): 633562530
2019-02-05 00:33:59,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741834_1010, type=LAST_IN_PIPELINE terminating
2019-02-05 00:33:59,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741838_1014 src: /10.10.1.2:45258 dest: /10.10.1.5:9866
2019-02-05 00:33:59,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741839_1015 src: /10.10.1.3:42580 dest: /10.10.1.5:9866
2019-02-05 00:33:59,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.2:45256, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1772495524_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741837_1013, duration(ns): 457264989
2019-02-05 00:33:59,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741837_1013, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-02-05 00:33:59,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:42576, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1951007782_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741835_1011, duration(ns): 498597332
2019-02-05 00:33:59,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741835_1011, type=LAST_IN_PIPELINE terminating
2019-02-05 00:34:00,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741840_1016 src: /10.10.1.6:36102 dest: /10.10.1.5:9866
2019-02-05 00:34:00,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741841_1017 src: /10.10.1.6:36104 dest: /10.10.1.5:9866
2019-02-05 00:34:00,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.2:45252, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-411529749_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741836_1012, duration(ns): 573741591
2019-02-05 00:34:00,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741836_1012, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-02-05 00:34:00,110 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.2:45258, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_538516184_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741838_1014, duration(ns): 593296483
2019-02-05 00:34:00,110 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741838_1014, type=LAST_IN_PIPELINE terminating
2019-02-05 00:34:00,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741842_1018 src: /10.10.1.2:45262 dest: /10.10.1.5:9866
2019-02-05 00:34:00,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741843_1019 src: /10.10.1.3:42584 dest: /10.10.1.5:9866
2019-02-05 00:34:00,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:42580, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_12203650_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741839_1015, duration(ns): 641176071
2019-02-05 00:34:00,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741839_1015, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.2:9866] terminating
2019-02-05 00:34:00,244 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741844_1020 src: /10.10.1.6:36110 dest: /10.10.1.5:9866
2019-02-05 00:34:00,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:36102, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1772495524_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741840_1016, duration(ns): 525242556
2019-02-05 00:34:00,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741840_1016, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.2:9866] terminating
2019-02-05 00:34:00,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741845_1021 src: /10.10.1.6:36112 dest: /10.10.1.5:9866
2019-02-05 00:34:00,597 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:36104, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1951007782_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741841_1017, duration(ns): 554474874
2019-02-05 00:34:00,597 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741841_1017, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.2:9866, 10.10.1.3:9866] terminating
2019-02-05 00:34:00,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741846_1022 src: /10.10.1.6:36114 dest: /10.10.1.5:9866
2019-02-05 00:34:00,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.2:45262, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-411529749_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741842_1018, duration(ns): 528962431
2019-02-05 00:34:00,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741842_1018, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-02-05 00:34:00,690 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741847_1023 src: /10.10.1.3:42590 dest: /10.10.1.5:9866
2019-02-05 00:34:00,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:42584, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_538516184_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741843_1019, duration(ns): 557023556
2019-02-05 00:34:00,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741843_1019, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.2:9866] terminating
2019-02-05 00:34:00,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741848_1024 src: /10.10.1.6:36118 dest: /10.10.1.5:9866
2019-02-05 00:34:00,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:36110, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_12203650_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741844_1020, duration(ns): 663663516
2019-02-05 00:34:00,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741844_1020, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.2:9866] terminating
2019-02-05 00:34:00,953 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741849_1025 src: /10.10.1.6:36120 dest: /10.10.1.5:9866
2019-02-05 00:34:01,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:36112, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1772495524_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741845_1021, duration(ns): 585128392
2019-02-05 00:34:01,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741845_1021, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.2:9866] terminating
2019-02-05 00:34:01,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:42590, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-411529749_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741847_1023, duration(ns): 519187469
2019-02-05 00:34:01,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741847_1023, type=LAST_IN_PIPELINE terminating
2019-02-05 00:34:01,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741850_1026 src: /10.10.1.3:42594 dest: /10.10.1.5:9866
2019-02-05 00:34:01,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:36114, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1951007782_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741846_1022, duration(ns): 587430946
2019-02-05 00:34:01,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741846_1022, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.2:9866, 10.10.1.3:9866] terminating
2019-02-05 00:34:01,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741851_1027 src: /10.10.1.6:36126 dest: /10.10.1.5:9866
2019-02-05 00:34:01,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741852_1028 src: /10.10.1.2:45270 dest: /10.10.1.5:9866
2019-02-05 00:34:01,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:36118, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_538516184_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741848_1024, duration(ns): 622709379
2019-02-05 00:34:01,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741848_1024, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.2:9866, 10.10.1.3:9866] terminating
2019-02-05 00:34:01,411 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741853_1029 src: /10.10.1.6:36128 dest: /10.10.1.5:9866
2019-02-05 00:34:01,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:36120, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_12203650_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741849_1025, duration(ns): 750843074
2019-02-05 00:34:01,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741849_1025, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.2:9866] terminating
2019-02-05 00:34:01,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741854_1030 src: /10.10.1.6:36130 dest: /10.10.1.5:9866
2019-02-05 00:34:01,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:36126, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-411529749_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741851_1027, duration(ns): 522224007
2019-02-05 00:34:01,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741851_1027, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.2:9866] terminating
2019-02-05 00:34:01,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:42594, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1772495524_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741850_1026, duration(ns): 583367030
2019-02-05 00:34:01,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741850_1026, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.2:9866] terminating
2019-02-05 00:34:01,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741855_1031 src: /10.10.1.6:36132 dest: /10.10.1.5:9866
2019-02-05 00:34:01,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741856_1032 src: /10.10.1.6:36134 dest: /10.10.1.5:9866
2019-02-05 00:34:01,913 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.2:45270, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1951007782_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741852_1028, duration(ns): 638422641
2019-02-05 00:34:01,913 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741852_1028, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-02-05 00:34:01,955 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741857_1033 src: /10.10.1.3:42604 dest: /10.10.1.5:9866
2019-02-05 00:34:01,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:36128, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_538516184_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741853_1029, duration(ns): 544815196
2019-02-05 00:34:01,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741853_1029, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.2:9866] terminating
2019-02-05 00:34:02,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741858_1034 src: /10.10.1.6:36138 dest: /10.10.1.5:9866
2019-02-05 00:34:02,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:36130, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_12203650_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741854_1030, duration(ns): 542391130
2019-02-05 00:34:02,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741854_1030, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.2:9866, 10.10.1.3:9866] terminating
2019-02-05 00:34:02,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741859_1035 src: /10.10.1.2:45274 dest: /10.10.1.5:9866
2019-02-05 00:34:02,368 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:36132, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-411529749_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741855_1031, duration(ns): 527422069
2019-02-05 00:34:02,368 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741855_1031, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.2:9866] terminating
2019-02-05 00:34:02,401 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:36134, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1772495524_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741856_1032, duration(ns): 560577247
2019-02-05 00:34:02,401 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741856_1032, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.2:9866] terminating
2019-02-05 00:34:02,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741860_1036 src: /10.10.1.6:36142 dest: /10.10.1.5:9866
2019-02-05 00:34:02,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741861_1037 src: /10.10.1.3:42610 dest: /10.10.1.5:9866
2019-02-05 00:34:02,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:42604, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1951007782_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741857_1033, duration(ns): 524017734
2019-02-05 00:34:02,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741857_1033, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.2:9866] terminating
2019-02-05 00:34:02,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741862_1038 src: /10.10.1.3:42612 dest: /10.10.1.5:9866
2019-02-05 00:34:02,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:36138, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_538516184_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741858_1034, duration(ns): 655469881
2019-02-05 00:34:02,666 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741858_1034, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.2:9866] terminating
2019-02-05 00:34:02,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741863_1039 src: /10.10.1.6:36148 dest: /10.10.1.5:9866
2019-02-05 00:34:02,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.2:45274, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_12203650_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741859_1035, duration(ns): 515608169
2019-02-05 00:34:02,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741859_1035, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.3:9866] terminating
2019-02-05 00:34:02,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1033126199-130.127.133.53-1549352001420:blk_1073741864_1040 src: /10.10.1.3:42614 dest: /10.10.1.5:9866
2019-02-05 00:34:02,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:36142, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-411529749_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741860_1036, duration(ns): 512459646
2019-02-05 00:34:02,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741860_1036, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.3:9866, 10.10.1.2:9866] terminating
2019-02-05 00:34:02,975 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:42610, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1772495524_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741861_1037, duration(ns): 542548896
2019-02-05 00:34:02,975 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741861_1037, type=LAST_IN_PIPELINE terminating
2019-02-05 00:34:03,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:42612, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1951007782_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741862_1038, duration(ns): 586447155
2019-02-05 00:34:03,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741862_1038, type=LAST_IN_PIPELINE terminating
2019-02-05 00:34:03,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.6:36148, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_538516184_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741863_1039, duration(ns): 502918323
2019-02-05 00:34:03,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741863_1039, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.10.1.2:9866, 10.10.1.3:9866] terminating
2019-02-05 00:34:03,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.10.1.3:42614, dest: /10.10.1.5:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_12203650_1, offset: 0, srvID: b6dd557e-664c-46ca-8ab7-90489cef54b2, blockid: BP-1033126199-130.127.133.53-1549352001420:blk_1073741864_1040, duration(ns): 452396100
2019-02-05 00:34:03,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033126199-130.127.133.53-1549352001420:blk_1073741864_1040, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.10.1.2:9866] terminating
2019-02-05 00:34:28,749 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "clnode092.clemson.cloudlab.us/130.127.133.101"; destination host is: "node-1-link-0":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:789)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1501)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy20.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:166)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:514)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:645)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:841)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1802)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1167)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1063)
2019-02-05 00:34:31,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1033126199-130.127.133.53-1549352001420 (Datanode Uuid b6dd557e-664c-46ca-8ab7-90489cef54b2) service to node-0-link-0/10.10.1.1:8020 trying to claim ACTIVE state with txid=137
2019-02-05 00:34:31,747 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1033126199-130.127.133.53-1549352001420 (Datanode Uuid b6dd557e-664c-46ca-8ab7-90489cef54b2) service to node-0-link-0/10.10.1.1:8020 taking over ACTIVE state from Block pool BP-1033126199-130.127.133.53-1549352001420 (Datanode Uuid b6dd557e-664c-46ca-8ab7-90489cef54b2) service to node-1-link-0/10.10.1.4:8020 at higher txid=137
2019-02-05 00:34:32,748 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:34:33,749 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:34:34,749 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:34:35,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:34:36,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:34:37,751 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:34:38,752 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:34:39,752 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:34:40,753 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:34:41,753 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:34:41,755 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From clnode092.clemson.cloudlab.us/130.127.133.101 to node-1-link-0:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1501)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy20.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:166)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:514)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:645)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:841)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:687)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:790)
	at org.apache.hadoop.ipc.Client$Connection.access$3600(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1558)
	at org.apache.hadoop.ipc.Client.call(Client.java:1389)
	... 9 more
2019-02-05 00:34:43,757 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:34:44,758 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:34:45,758 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:34:46,759 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:34:47,760 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:34:48,761 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:34:49,762 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-1-link-0/10.10.1.4:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:34:49,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from node-1-link-0/10.10.1.4:8020 with standby state
2019-02-05 00:34:49,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1033126199-130.127.133.53-1549352001420 (Datanode Uuid b6dd557e-664c-46ca-8ab7-90489cef54b2) service to node-1-link-0/10.10.1.4:8020 beginning handshake with NN
2019-02-05 00:34:49,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1033126199-130.127.133.53-1549352001420 (Datanode Uuid b6dd557e-664c-46ca-8ab7-90489cef54b2) service to node-1-link-0/10.10.1.4:8020 successfully registered with NN
2019-02-05 00:34:49,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xea69e1ee0479a7a2,  containing 1 storage report(s), of which we sent 1. The reports had 40 total blocks and used 1 RPC(s). This took 1 msec to generate and 27 msecs for RPC and NN processing. Got back no commands.
2019-02-05 00:35:07,748 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "clnode092.clemson.cloudlab.us/130.127.133.101"; destination host is: "node-0-link-0":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:789)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1501)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy20.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:166)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:514)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:645)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:841)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1802)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1167)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1063)
2019-02-05 00:35:07,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1033126199-130.127.133.53-1549352001420 (Datanode Uuid b6dd557e-664c-46ca-8ab7-90489cef54b2) service to node-1-link-0/10.10.1.4:8020 trying to claim ACTIVE state with txid=138
2019-02-05 00:35:07,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1033126199-130.127.133.53-1549352001420 (Datanode Uuid b6dd557e-664c-46ca-8ab7-90489cef54b2) service to node-1-link-0/10.10.1.4:8020 taking over ACTIVE state from Block pool BP-1033126199-130.127.133.53-1549352001420 (Datanode Uuid b6dd557e-664c-46ca-8ab7-90489cef54b2) service to node-0-link-0/10.10.1.1:8020 at higher txid=138
2019-02-05 00:35:11,747 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:35:12,748 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:35:13,748 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:35:14,749 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:35:15,749 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:35:16,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:35:17,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:35:18,751 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:35:19,753 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:35:20,753 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:35:20,755 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From clnode092.clemson.cloudlab.us/130.127.133.101 to node-0-link-0:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1501)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy20.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:166)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:514)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:645)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:841)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:687)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:790)
	at org.apache.hadoop.ipc.Client$Connection.access$3600(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1558)
	at org.apache.hadoop.ipc.Client.call(Client.java:1389)
	... 9 more
2019-02-05 00:35:22,756 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:35:23,756 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:35:24,757 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:35:25,757 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:35:26,758 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-0-link-0/10.10.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-05 00:35:26,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from node-0-link-0/10.10.1.1:8020 with standby state
2019-02-05 00:35:26,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1033126199-130.127.133.53-1549352001420 (Datanode Uuid b6dd557e-664c-46ca-8ab7-90489cef54b2) service to node-0-link-0/10.10.1.1:8020 beginning handshake with NN
2019-02-05 00:35:26,919 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1033126199-130.127.133.53-1549352001420 (Datanode Uuid b6dd557e-664c-46ca-8ab7-90489cef54b2) service to node-0-link-0/10.10.1.1:8020 successfully registered with NN
2019-02-05 00:35:26,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe9b1a62fb93cbbaa,  containing 1 storage report(s), of which we sent 1. The reports had 40 total blocks and used 1 RPC(s). This took 0 msec to generate and 29 msecs for RPC and NN processing. Got back no commands.
