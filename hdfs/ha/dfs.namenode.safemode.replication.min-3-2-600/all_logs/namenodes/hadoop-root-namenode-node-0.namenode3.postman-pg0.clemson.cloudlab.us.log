2019-02-20 18:57:08,764 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode058.clemson.cloudlab.us/130.127.133.67
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-02-18T22:18Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-02-20 18:57:08,774 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-20 18:57:08,779 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-02-20 18:57:09,074 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-20 18:57:09,178 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-02-20 18:57:09,178 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-02-20 18:57:09,230 INFO org.apache.hadoop.hdfs.server.namenode.NameNodeUtils: fs.defaultFS is hdfs://mycluster
2019-02-20 18:57:09,231 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients should use mycluster to access this namenode/service.
2019-02-20 18:57:09,394 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-02-20 18:57:09,422 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://node-0-link-0:9870
2019-02-20 18:57:09,440 INFO org.eclipse.jetty.util.log: Logging initialized @1196ms
2019-02-20 18:57:09,548 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-20 18:57:09,562 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-02-20 18:57:09,573 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-20 18:57:09,576 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-02-20 18:57:09,576 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-20 18:57:09,576 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-20 18:57:09,603 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-02-20 18:57:09,603 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-02-20 18:57:09,612 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 9870
2019-02-20 18:57:09,613 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2019-02-20 18:57:09,648 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c137fd5{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-02-20 18:57:09,649 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d9d0818{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-02-20 18:57:09,724 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@376a0d86{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-02-20 18:57:09,745 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@536dbea0{HTTP/1.1,[http/1.1]}{node-0-link-0:9870}
2019-02-20 18:57:09,745 INFO org.eclipse.jetty.server.Server: Started @1503ms
2019-02-20 18:57:10,117 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-20 18:57:10,167 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2019-02-20 18:57:10,182 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2019-02-20 18:57:10,183 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2019-02-20 18:57:10,186 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2019-02-20 18:57:10,192 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-02-20 18:57:10,192 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-02-20 18:57:10,192 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2019-02-20 18:57:10,193 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID: mycluster
2019-02-20 18:57:10,193 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: true
2019-02-20 18:57:10,235 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-02-20 18:57:10,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-02-20 18:57:10,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-02-20 18:57:10,252 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-02-20 18:57:10,252 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Feb 20 18:57:10
2019-02-20 18:57:10,254 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-02-20 18:57:10,254 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-20 18:57:10,256 INFO org.apache.hadoop.util.GSet: 2.0% max memory 26.7 GB = 546.1 MB
2019-02-20 18:57:10,256 INFO org.apache.hadoop.util.GSet: capacity      = 2^26 = 67108864 entries
2019-02-20 18:57:10,388 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable = false
2019-02-20 18:57:10,397 INFO org.apache.hadoop.conf.Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-02-20 18:57:10,397 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-02-20 18:57:10,397 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2019-02-20 18:57:10,397 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2019-02-20 18:57:10,397 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2019-02-20 18:57:10,397 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-02-20 18:57:10,397 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-02-20 18:57:10,398 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-02-20 18:57:10,398 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2019-02-20 18:57:10,398 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-02-20 18:57:10,398 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-02-20 18:57:10,481 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-02-20 18:57:10,481 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-20 18:57:10,481 INFO org.apache.hadoop.util.GSet: 1.0% max memory 26.7 GB = 273.0 MB
2019-02-20 18:57:10,481 INFO org.apache.hadoop.util.GSet: capacity      = 2^25 = 33554432 entries
2019-02-20 18:57:10,554 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-02-20 18:57:10,554 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: POSIX ACL inheritance enabled? true
2019-02-20 18:57:10,554 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-02-20 18:57:10,554 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2019-02-20 18:57:10,561 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-02-20 18:57:10,564 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: SkipList is disabled
2019-02-20 18:57:10,569 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-02-20 18:57:10,569 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-20 18:57:10,569 INFO org.apache.hadoop.util.GSet: 0.25% max memory 26.7 GB = 68.3 MB
2019-02-20 18:57:10,570 INFO org.apache.hadoop.util.GSet: capacity      = 2^23 = 8388608 entries
2019-02-20 18:57:10,597 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-02-20 18:57:10,597 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-02-20 18:57:10,597 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-02-20 18:57:10,601 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-02-20 18:57:10,601 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-02-20 18:57:10,603 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-02-20 18:57:10,603 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-20 18:57:10,604 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-02-20 18:57:10,604 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-02-20 18:57:10,662 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 60668@clnode058.clemson.cloudlab.us
2019-02-20 18:57:12,057 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1159ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=316ms
GC pool 'PS Scavenge' had collection(s): count=1 time=951ms
2019-02-20 18:57:12,213 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2019-02-20 18:57:12,213 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-02-20 18:57:12,276 INFO org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager: Enable the erasure coding policy RS-6-3-1024k
2019-02-20 18:57:12,278 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-02-20 18:57:12,309 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-02-20 18:57:12,309 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-02-20 18:57:12,315 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-02-20 18:57:12,315 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-02-20 18:57:12,315 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 1703 msecs
2019-02-20 18:57:12,496 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to node-0-link-0:8020
2019-02-20 18:57:12,500 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-02-20 18:57:12,513 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2019-02-20 18:57:12,700 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-02-20 18:57:12,709 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2019-02-20 18:57:12,719 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2019-02-20 18:57:12,719 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2019-02-20 18:57:12,719 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2019-02-20 18:57:12,752 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-20 18:57:12,752 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2019-02-20 18:57:12,755 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: node-0-link-0/10.10.1.1:8020
2019-02-20 18:57:12,758 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for standby state
2019-02-20 18:57:12,762 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Will roll logs on active node every 120 seconds.
2019-02-20 18:57:12,768 INFO org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-1-link-0:9870]
Serving checkpoints at http://node-0-link-0:9870
2019-02-20 18:57:17,741 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, fileSize: 389. Sent total: 389 bytes. Size of last segment intended to send: -1 bytes.
2019-02-20 18:57:17,905 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2019-02-20 18:57:17,907 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at clnode058.clemson.cloudlab.us/130.127.133.67
************************************************************/
2019-02-20 18:57:28,587 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode058.clemson.cloudlab.us/130.127.133.67
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-02-18T22:18Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-02-20 18:57:28,597 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-20 18:57:28,602 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-02-20 18:57:28,889 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-20 18:57:28,998 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-02-20 18:57:28,998 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-02-20 18:57:29,049 INFO org.apache.hadoop.hdfs.server.namenode.NameNodeUtils: fs.defaultFS is hdfs://mycluster
2019-02-20 18:57:29,049 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients should use mycluster to access this namenode/service.
2019-02-20 18:57:29,209 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-02-20 18:57:29,237 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://node-0-link-0:9870
2019-02-20 18:57:29,255 INFO org.eclipse.jetty.util.log: Logging initialized @1181ms
2019-02-20 18:57:29,363 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-20 18:57:29,377 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-02-20 18:57:29,388 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-20 18:57:29,391 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-02-20 18:57:29,391 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-20 18:57:29,391 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-20 18:57:29,418 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-02-20 18:57:29,418 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-02-20 18:57:29,427 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 9870
2019-02-20 18:57:29,428 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2019-02-20 18:57:29,464 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c137fd5{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-02-20 18:57:29,464 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d9d0818{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-02-20 18:57:29,540 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@376a0d86{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-02-20 18:57:29,561 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@536dbea0{HTTP/1.1,[http/1.1]}{node-0-link-0:9870}
2019-02-20 18:57:29,561 INFO org.eclipse.jetty.server.Server: Started @1489ms
2019-02-20 18:57:29,888 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-20 18:57:29,946 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2019-02-20 18:57:29,961 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2019-02-20 18:57:29,962 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2019-02-20 18:57:29,965 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2019-02-20 18:57:29,971 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-02-20 18:57:29,972 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-02-20 18:57:29,972 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2019-02-20 18:57:29,972 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID: mycluster
2019-02-20 18:57:29,972 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: true
2019-02-20 18:57:30,015 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-02-20 18:57:30,027 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-02-20 18:57:30,028 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-02-20 18:57:30,032 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-02-20 18:57:30,033 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Feb 20 18:57:30
2019-02-20 18:57:30,035 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-02-20 18:57:30,035 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-20 18:57:30,036 INFO org.apache.hadoop.util.GSet: 2.0% max memory 26.7 GB = 546.1 MB
2019-02-20 18:57:30,036 INFO org.apache.hadoop.util.GSet: capacity      = 2^26 = 67108864 entries
2019-02-20 18:57:30,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable = false
2019-02-20 18:57:30,190 INFO org.apache.hadoop.conf.Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-02-20 18:57:30,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-02-20 18:57:30,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2019-02-20 18:57:30,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2019-02-20 18:57:30,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2019-02-20 18:57:30,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-02-20 18:57:30,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-02-20 18:57:30,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-02-20 18:57:30,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2019-02-20 18:57:30,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-02-20 18:57:30,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-02-20 18:57:30,275 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-02-20 18:57:30,275 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-20 18:57:30,275 INFO org.apache.hadoop.util.GSet: 1.0% max memory 26.7 GB = 273.0 MB
2019-02-20 18:57:30,275 INFO org.apache.hadoop.util.GSet: capacity      = 2^25 = 33554432 entries
2019-02-20 18:57:30,349 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-02-20 18:57:30,349 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: POSIX ACL inheritance enabled? true
2019-02-20 18:57:30,349 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-02-20 18:57:30,349 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2019-02-20 18:57:30,356 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-02-20 18:57:30,359 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: SkipList is disabled
2019-02-20 18:57:30,365 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-02-20 18:57:30,365 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-20 18:57:30,365 INFO org.apache.hadoop.util.GSet: 0.25% max memory 26.7 GB = 68.3 MB
2019-02-20 18:57:30,365 INFO org.apache.hadoop.util.GSet: capacity      = 2^23 = 8388608 entries
2019-02-20 18:57:30,393 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-02-20 18:57:30,393 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-02-20 18:57:30,393 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-02-20 18:57:30,397 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-02-20 18:57:30,397 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-02-20 18:57:30,400 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-02-20 18:57:30,400 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-20 18:57:30,400 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-02-20 18:57:30,400 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-02-20 18:57:30,458 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 61353@clnode058.clemson.cloudlab.us
2019-02-20 18:57:31,965 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1251ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=309ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1075ms
2019-02-20 18:57:33,090 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-4-link-0/10.10.1.2:8485. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-20 18:57:33,090 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-20 18:57:33,090 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-20 18:57:34,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-4-link-0/10.10.1.2:8485. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-20 18:57:34,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-20 18:57:34,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-20 18:57:35,092 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-20 18:57:35,092 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-20 18:57:35,092 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-4-link-0/10.10.1.2:8485. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-20 18:57:36,093 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-20 18:57:36,093 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-4-link-0/10.10.1.2:8485. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-20 18:57:36,093 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-20 18:57:36,563 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Waited 6002 ms (timeout=20000 ms) for a response for selectInputStreams. No responses yet.
2019-02-20 18:57:37,093 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-20 18:57:37,093 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-20 18:57:37,328 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2019-02-20 18:57:37,328 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-02-20 18:57:37,393 INFO org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager: Enable the erasure coding policy RS-6-3-1024k
2019-02-20 18:57:37,395 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-02-20 18:57:37,427 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-02-20 18:57:37,427 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-02-20 18:57:37,432 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-02-20 18:57:37,432 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-02-20 18:57:37,433 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 7025 msecs
2019-02-20 18:57:37,614 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to node-0-link-0:8020
2019-02-20 18:57:37,619 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-02-20 18:57:37,632 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2019-02-20 18:57:37,825 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-02-20 18:57:37,833 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2019-02-20 18:57:37,845 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2019-02-20 18:57:37,845 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2019-02-20 18:57:37,845 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2019-02-20 18:57:37,879 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2019-02-20 18:57:37,879 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-20 18:57:37,883 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: node-0-link-0/10.10.1.1:8020
2019-02-20 18:57:37,887 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for standby state
2019-02-20 18:57:37,892 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Will roll logs on active node every 120 seconds.
2019-02-20 18:57:37,898 INFO org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-1-link-0:9870]
Serving checkpoints at http://node-0-link-0:9870
2019-02-20 18:57:38,374 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=98e9e948-bdd7-441c-8fd1-0b747f77761a, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937) storage 98e9e948-bdd7-441c-8fd1-0b747f77761a
2019-02-20 18:57:38,376 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.2:9866
2019-02-20 18:57:38,377 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 98e9e948-bdd7-441c-8fd1-0b747f77761a (10.10.1.2:9866).
2019-02-20 18:57:38,461 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-955de0ba-c328-4360-9e47-0109a5ba597a for DN 10.10.1.2:9866
2019-02-20 18:57:38,508 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=45826c1d-460c-4636-8bd0-cdeb5313fd8f, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937) storage 45826c1d-460c-4636-8bd0-cdeb5313fd8f
2019-02-20 18:57:38,508 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.3:9866
2019-02-20 18:57:38,508 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 45826c1d-460c-4636-8bd0-cdeb5313fd8f (10.10.1.3:9866).
2019-02-20 18:57:38,509 INFO BlockStateChange: BLOCK* processReport 0x93f28186db621819: Processing first storage report for DS-955de0ba-c328-4360-9e47-0109a5ba597a from datanode 98e9e948-bdd7-441c-8fd1-0b747f77761a
2019-02-20 18:57:38,511 INFO BlockStateChange: BLOCK* processReport 0x93f28186db621819: from storage DS-955de0ba-c328-4360-9e47-0109a5ba597a node DatanodeRegistration(10.10.1.2:9866, datanodeUuid=98e9e948-bdd7-441c-8fd1-0b747f77761a, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937), blocks: 0, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0
2019-02-20 18:57:38,556 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-c5ecc5a9-eaae-43a6-a11c-ee8c28bc06e9 for DN 10.10.1.3:9866
2019-02-20 18:57:38,559 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.5:9866, datanodeUuid=fa00b432-7f72-4db1-a094-e44224a216a1, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937) storage fa00b432-7f72-4db1-a094-e44224a216a1
2019-02-20 18:57:38,559 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.5:9866
2019-02-20 18:57:38,559 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN fa00b432-7f72-4db1-a094-e44224a216a1 (10.10.1.5:9866).
2019-02-20 18:57:38,581 INFO BlockStateChange: BLOCK* processReport 0x51c00afc6c4f6363: Processing first storage report for DS-c5ecc5a9-eaae-43a6-a11c-ee8c28bc06e9 from datanode 45826c1d-460c-4636-8bd0-cdeb5313fd8f
2019-02-20 18:57:38,581 INFO BlockStateChange: BLOCK* processReport 0x51c00afc6c4f6363: from storage DS-c5ecc5a9-eaae-43a6-a11c-ee8c28bc06e9 node DatanodeRegistration(10.10.1.3:9866, datanodeUuid=45826c1d-460c-4636-8bd0-cdeb5313fd8f, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-02-20 18:57:38,609 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-7bf94cf2-19db-42f4-8ce2-dfd4b2d5a174 for DN 10.10.1.5:9866
2019-02-20 18:57:38,634 INFO BlockStateChange: BLOCK* processReport 0xc030fca1ebc1c27: Processing first storage report for DS-7bf94cf2-19db-42f4-8ce2-dfd4b2d5a174 from datanode fa00b432-7f72-4db1-a094-e44224a216a1
2019-02-20 18:57:38,634 INFO BlockStateChange: BLOCK* processReport 0xc030fca1ebc1c27: from storage DS-7bf94cf2-19db-42f4-8ce2-dfd4b2d5a174 node DatanodeRegistration(10.10.1.5:9866, datanodeUuid=fa00b432-7f72-4db1-a094-e44224a216a1, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-02-20 18:58:46,407 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for standby state
2019-02-20 18:58:46,408 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Edit log tailer interrupted
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:479)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
2019-02-20 18:58:46,511 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2019-02-20 18:58:46,520 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Starting recovery process for unclosed journal segments...
2019-02-20 18:58:46,596 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Successfully started new epoch 2
2019-02-20 18:58:46,596 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Beginning recovery of unclosed segment starting at txid 1
2019-02-20 18:58:46,630 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Recovery prepare phase complete. Responses:
10.10.1.3:8485: segmentState { startTxId: 1 endTxId: 535 isInProgress: true } lastWriterEpoch: 1 lastCommittedTxId: 533
10.10.1.5:8485: segmentState { startTxId: 1 endTxId: 535 isInProgress: true } lastWriterEpoch: 1 lastCommittedTxId: 533
10.10.1.2:8485: segmentState { startTxId: 1 endTxId: 535 isInProgress: true } lastWriterEpoch: 1 lastCommittedTxId: 533
2019-02-20 18:58:46,633 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Using longest log: 10.10.1.3:8485=segmentState {
  startTxId: 1
  endTxId: 535
  isInProgress: true
}
lastWriterEpoch: 1
lastCommittedTxId: 533

2019-02-20 18:58:46,709 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-root/dfs/name/current
2019-02-20 18:58:46,709 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Catching up to latest edits from old active before taking over writer role in edits logs
2019-02-20 18:58:46,725 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@2630665d expecting start txid #1
2019-02-20 18:58:46,725 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 18:58:46,728 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 18:58:46,729 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 18:58:46,960 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 1048576 edits # 535 loaded in 0 seconds
2019-02-20 18:58:46,961 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Marking all datanodes as stale
2019-02-20 18:58:46,961 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Reprocessing replication and invalidation queues
2019-02-20 18:58:46,961 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: initializing replication queues
2019-02-20 18:58:46,962 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Will take over writing edit logs at txnid 536
2019-02-20 18:58:46,965 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 536
2019-02-20 18:58:47,211 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 0 SyncTimes(ms): 0 52 
2019-02-20 18:58:47,237 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Initializing quota with 4 thread(s)
2019-02-20 18:58:47,241 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Quota initialization completed in 4 milliseconds
name space=21
storage space=64424509440
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2019-02-20 18:58:47,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-02-20 18:58:47,309 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 160
2019-02-20 18:58:47,309 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2019-02-20 18:58:47,309 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2019-02-20 18:58:47,309 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2019-02-20 18:58:47,309 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 3
2019-02-20 18:58:47,309 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 347 msec
2019-02-20 18:58:48,043 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile3._COPYING_ is closed by DFSClient_NONMAPREDUCE_-979840342_1
2019-02-20 18:58:48,043 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile2._COPYING_ is closed by DFSClient_NONMAPREDUCE_1732028541_1
2019-02-20 18:58:48,043 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile17._COPYING_ is closed by DFSClient_NONMAPREDUCE_-790959988_1
2019-02-20 18:59:20,706 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2019-02-20 18:59:20,708 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at clnode058.clemson.cloudlab.us/130.127.133.67
************************************************************/
2019-02-20 18:59:37,750 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode058.clemson.cloudlab.us/130.127.133.67
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-02-18T22:18Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-02-20 18:59:37,760 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-20 18:59:37,765 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-02-20 18:59:38,051 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-20 18:59:38,155 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-02-20 18:59:38,155 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-02-20 18:59:38,207 INFO org.apache.hadoop.hdfs.server.namenode.NameNodeUtils: fs.defaultFS is hdfs://mycluster
2019-02-20 18:59:38,207 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients should use mycluster to access this namenode/service.
2019-02-20 18:59:38,368 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-02-20 18:59:38,397 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://node-0-link-0:9870
2019-02-20 18:59:38,415 INFO org.eclipse.jetty.util.log: Logging initialized @1177ms
2019-02-20 18:59:38,521 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-20 18:59:38,535 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-02-20 18:59:38,546 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-20 18:59:38,549 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-02-20 18:59:38,549 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-20 18:59:38,549 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-20 18:59:38,575 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-02-20 18:59:38,576 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-02-20 18:59:38,585 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 9870
2019-02-20 18:59:38,586 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2019-02-20 18:59:38,621 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c137fd5{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-02-20 18:59:38,622 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d9d0818{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-02-20 18:59:38,697 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@376a0d86{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-02-20 18:59:38,717 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@536dbea0{HTTP/1.1,[http/1.1]}{node-0-link-0:9870}
2019-02-20 18:59:38,717 INFO org.eclipse.jetty.server.Server: Started @1481ms
2019-02-20 18:59:39,066 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-20 18:59:39,116 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2019-02-20 18:59:39,131 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2019-02-20 18:59:39,132 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2019-02-20 18:59:39,135 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2019-02-20 18:59:39,141 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-02-20 18:59:39,141 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-02-20 18:59:39,141 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2019-02-20 18:59:39,142 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID: mycluster
2019-02-20 18:59:39,142 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: true
2019-02-20 18:59:39,184 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-02-20 18:59:39,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-02-20 18:59:39,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-02-20 18:59:39,201 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-02-20 18:59:39,202 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Feb 20 18:59:39
2019-02-20 18:59:39,204 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-02-20 18:59:39,204 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-20 18:59:39,205 INFO org.apache.hadoop.util.GSet: 2.0% max memory 26.7 GB = 546.1 MB
2019-02-20 18:59:39,205 INFO org.apache.hadoop.util.GSet: capacity      = 2^26 = 67108864 entries
2019-02-20 18:59:39,340 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable = false
2019-02-20 18:59:39,350 INFO org.apache.hadoop.conf.Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-02-20 18:59:39,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-02-20 18:59:39,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2019-02-20 18:59:39,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2019-02-20 18:59:39,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2019-02-20 18:59:39,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-02-20 18:59:39,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-02-20 18:59:39,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-02-20 18:59:39,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2019-02-20 18:59:39,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-02-20 18:59:39,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-02-20 18:59:39,434 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-02-20 18:59:39,434 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-20 18:59:39,435 INFO org.apache.hadoop.util.GSet: 1.0% max memory 26.7 GB = 273.0 MB
2019-02-20 18:59:39,435 INFO org.apache.hadoop.util.GSet: capacity      = 2^25 = 33554432 entries
2019-02-20 18:59:39,508 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-02-20 18:59:39,508 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: POSIX ACL inheritance enabled? true
2019-02-20 18:59:39,508 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-02-20 18:59:39,509 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2019-02-20 18:59:39,515 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-02-20 18:59:39,518 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: SkipList is disabled
2019-02-20 18:59:39,523 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-02-20 18:59:39,523 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-20 18:59:39,524 INFO org.apache.hadoop.util.GSet: 0.25% max memory 26.7 GB = 68.3 MB
2019-02-20 18:59:39,524 INFO org.apache.hadoop.util.GSet: capacity      = 2^23 = 8388608 entries
2019-02-20 18:59:39,551 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-02-20 18:59:39,551 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-02-20 18:59:39,551 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-02-20 18:59:39,556 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-02-20 18:59:39,556 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-02-20 18:59:39,558 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-02-20 18:59:39,558 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-20 18:59:39,558 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-02-20 18:59:39,558 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-02-20 18:59:39,606 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 62734@clnode058.clemson.cloudlab.us
2019-02-20 18:59:41,096 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1224ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=313ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1050ms
2019-02-20 18:59:41,280 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-02-20 18:59:41,343 INFO org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager: Enable the erasure coding policy RS-6-3-1024k
2019-02-20 18:59:41,346 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-02-20 18:59:41,378 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-02-20 18:59:41,378 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-02-20 18:59:41,383 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@abf688e expecting start txid #1
2019-02-20 18:59:41,383 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 18:59:41,387 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 18:59:41,387 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 18:59:41,505 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 1048576 edits # 535 loaded in 0 seconds
2019-02-20 18:59:41,505 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@478ee483 expecting start txid #536
2019-02-20 18:59:41,505 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 18:59:41,505 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 18:59:41,505 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 18:59:41,514 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 1048576 edits # 7 loaded in 0 seconds
2019-02-20 18:59:41,514 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-02-20 18:59:41,514 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-02-20 18:59:41,514 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 1948 msecs
2019-02-20 18:59:41,700 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to node-0-link-0:8020
2019-02-20 18:59:41,705 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-02-20 18:59:41,717 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2019-02-20 18:59:41,906 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-02-20 18:59:41,915 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2019-02-20 18:59:41,926 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2019-02-20 18:59:41,956 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-20 18:59:41,957 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2019-02-20 18:59:41,959 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: node-0-link-0/10.10.1.1:8020
2019-02-20 18:59:41,962 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for standby state
2019-02-20 18:59:41,967 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Will roll logs on active node every 120 seconds.
2019-02-20 18:59:41,972 INFO org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-1-link-0:9870]
Serving checkpoints at http://node-0-link-0:9870
2019-02-20 18:59:42,561 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=98e9e948-bdd7-441c-8fd1-0b747f77761a, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937) storage 98e9e948-bdd7-441c-8fd1-0b747f77761a
2019-02-20 18:59:42,562 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.2:9866
2019-02-20 18:59:42,562 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 98e9e948-bdd7-441c-8fd1-0b747f77761a (10.10.1.2:9866).
2019-02-20 18:59:42,564 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=45826c1d-460c-4636-8bd0-cdeb5313fd8f, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937) storage 45826c1d-460c-4636-8bd0-cdeb5313fd8f
2019-02-20 18:59:42,564 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.3:9866
2019-02-20 18:59:42,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 45826c1d-460c-4636-8bd0-cdeb5313fd8f (10.10.1.3:9866).
2019-02-20 18:59:42,575 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-955de0ba-c328-4360-9e47-0109a5ba597a for DN 10.10.1.2:9866
2019-02-20 18:59:42,578 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-c5ecc5a9-eaae-43a6-a11c-ee8c28bc06e9 for DN 10.10.1.3:9866
2019-02-20 18:59:42,590 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.5:9866, datanodeUuid=fa00b432-7f72-4db1-a094-e44224a216a1, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937) storage fa00b432-7f72-4db1-a094-e44224a216a1
2019-02-20 18:59:42,590 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.5:9866
2019-02-20 18:59:42,591 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN fa00b432-7f72-4db1-a094-e44224a216a1 (10.10.1.5:9866).
2019-02-20 18:59:42,591 INFO BlockStateChange: BLOCK* processReport 0x93f28186db62181a: Processing first storage report for DS-955de0ba-c328-4360-9e47-0109a5ba597a from datanode 98e9e948-bdd7-441c-8fd1-0b747f77761a
2019-02-20 18:59:42,601 INFO BlockStateChange: BLOCK* processReport 0x93f28186db62181a: from storage DS-955de0ba-c328-4360-9e47-0109a5ba597a node DatanodeRegistration(10.10.1.2:9866, datanodeUuid=98e9e948-bdd7-441c-8fd1-0b747f77761a, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937), blocks: 160, hasStaleStorage: false, processing time: 10 msecs, invalidatedBlocks: 0
2019-02-20 18:59:42,602 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-7bf94cf2-19db-42f4-8ce2-dfd4b2d5a174 for DN 10.10.1.5:9866
2019-02-20 18:59:42,602 INFO BlockStateChange: BLOCK* processReport 0x51c00afc6c4f6364: Processing first storage report for DS-c5ecc5a9-eaae-43a6-a11c-ee8c28bc06e9 from datanode 45826c1d-460c-4636-8bd0-cdeb5313fd8f
2019-02-20 18:59:42,606 INFO BlockStateChange: BLOCK* processReport 0x51c00afc6c4f6364: from storage DS-c5ecc5a9-eaae-43a6-a11c-ee8c28bc06e9 node DatanodeRegistration(10.10.1.3:9866, datanodeUuid=45826c1d-460c-4636-8bd0-cdeb5313fd8f, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937), blocks: 160, hasStaleStorage: false, processing time: 4 msecs, invalidatedBlocks: 0
2019-02-20 18:59:42,606 INFO BlockStateChange: BLOCK* processReport 0xc030fca1ebc1c28: Processing first storage report for DS-7bf94cf2-19db-42f4-8ce2-dfd4b2d5a174 from datanode fa00b432-7f72-4db1-a094-e44224a216a1
2019-02-20 18:59:42,611 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 159 has reached the threshold 0.9990 of total blocks 160. The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2019-02-20 18:59:42,611 INFO BlockStateChange: BLOCK* processReport 0xc030fca1ebc1c28: from storage DS-7bf94cf2-19db-42f4-8ce2-dfd4b2d5a174 node DatanodeRegistration(10.10.1.5:9866, datanodeUuid=fa00b432-7f72-4db1-a094-e44224a216a1, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937), blocks: 160, hasStaleStorage: false, processing time: 5 msecs, invalidatedBlocks: 0
2019-02-20 19:00:02,613 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 160 has reached the threshold 0.9990 of total blocks 160. The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2019-02-20 19:00:12,614 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2019-02-20 19:00:12,614 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 30 secs
2019-02-20 19:00:12,615 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 3 datanodes
2019-02-20 19:00:12,615 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2019-02-20 19:01:41,982 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:01:42,039 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -126 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -126 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -126 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -126 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-20 19:02:42,047 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:02:42,051 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -186 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -186 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -186 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -186 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-20 19:03:42,057 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:03:42,061 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -246 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -246 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -246 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -246 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-20 19:04:42,067 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:04:42,071 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -306 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -306 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -306 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -306 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-20 19:05:42,078 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:05:42,083 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -366 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -366 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -366 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -366 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-20 19:06:42,089 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:06:42,092 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -426 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -426 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -426 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -426 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-20 19:07:42,099 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:07:42,103 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -486 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -486 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -486 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -486 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-20 19:08:42,109 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:08:42,112 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -546 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -546 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -546 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -546 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-20 19:09:42,118 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:09:42,122 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -606 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -606 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -606 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 157 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in -606 seconds. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-20 19:09:49,868 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for standby state
2019-02-20 19:09:49,868 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Edit log tailer interrupted
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:479)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
2019-02-20 19:09:49,870 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2019-02-20 19:09:49,880 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Starting recovery process for unclosed journal segments...
2019-02-20 19:09:49,921 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Successfully started new epoch 4
2019-02-20 19:09:49,921 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Beginning recovery of unclosed segment starting at txid 543
2019-02-20 19:09:49,945 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Recovery prepare phase complete. Responses:
10.10.1.2:8485: segmentState { startTxId: 543 endTxId: 543 isInProgress: true } lastWriterEpoch: 3 lastCommittedTxId: 540
10.10.1.3:8485: segmentState { startTxId: 543 endTxId: 543 isInProgress: true } lastWriterEpoch: 3 lastCommittedTxId: 540
10.10.1.5:8485: segmentState { startTxId: 543 endTxId: 543 isInProgress: true } lastWriterEpoch: 3 lastCommittedTxId: 540
2019-02-20 19:09:49,947 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Using longest log: 10.10.1.2:8485=segmentState {
  startTxId: 543
  endTxId: 543
  isInProgress: true
}
lastWriterEpoch: 3
lastCommittedTxId: 540

2019-02-20 19:09:49,978 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-root/dfs/name/current
2019-02-20 19:09:50,006 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000000536 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000000536-0000000000000000542
2019-02-20 19:09:50,024 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Catching up to latest edits from old active before taking over writer role in edits logs
2019-02-20 19:09:50,027 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@5e83c5e3 expecting start txid #543
2019-02-20 19:09:50,028 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:09:50,029 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 543
2019-02-20 19:09:50,029 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 543
2019-02-20 19:09:50,035 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-20 19:09:50,036 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Marking all datanodes as stale
2019-02-20 19:09:50,037 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Reprocessing replication and invalidation queues
2019-02-20 19:09:50,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: initializing replication queues
2019-02-20 19:09:50,037 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Will take over writing edit logs at txnid 544
2019-02-20 19:09:50,040 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 544
2019-02-20 19:09:50,300 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 0 SyncTimes(ms): 0 66 
2019-02-20 19:09:50,321 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Initializing quota with 4 thread(s)
2019-02-20 19:09:50,326 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Quota initialization completed in 3 milliseconds
name space=21
storage space=64424509440
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2019-02-20 19:09:50,331 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-02-20 19:09:50,398 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 160
2019-02-20 19:09:50,398 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2019-02-20 19:09:50,398 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2019-02-20 19:09:50,398 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2019-02-20 19:09:50,398 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2019-02-20 19:09:50,398 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 361 msec
2019-02-20 19:10:04,665 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741985_1161, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile20._COPYING_
2019-02-20 19:10:05,911 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741986_1162, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile20._COPYING_
2019-02-20 19:10:06,429 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741987_1163, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile20._COPYING_
2019-02-20 19:10:06,940 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741988_1164, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile20._COPYING_
2019-02-20 19:10:07,465 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741989_1165, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile20._COPYING_
2019-02-20 19:10:08,152 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741990_1166, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile20._COPYING_
2019-02-20 19:10:08,733 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741991_1167, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile20._COPYING_
2019-02-20 19:10:09,166 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741992_1168, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile20._COPYING_
2019-02-20 19:10:09,635 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile20._COPYING_ is closed by DFSClient_NONMAPREDUCE_2067943744_1
2019-02-20 19:10:10,129 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741993_1169, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile6._COPYING_
2019-02-20 19:10:10,656 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741994_1170, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile6._COPYING_
2019-02-20 19:10:11,071 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741995_1171, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile6._COPYING_
2019-02-20 19:10:11,520 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741996_1172, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile6._COPYING_
2019-02-20 19:10:11,930 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741997_1173, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile6._COPYING_
2019-02-20 19:10:12,325 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741998_1174, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile6._COPYING_
2019-02-20 19:10:12,747 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741999_1175, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile6._COPYING_
2019-02-20 19:10:13,181 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742000_1176, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile6._COPYING_
2019-02-20 19:10:13,665 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile6._COPYING_ is closed by DFSClient_NONMAPREDUCE_-547124836_1
2019-02-20 19:10:16,352 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742001_1177, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile14._COPYING_
2019-02-20 19:10:18,125 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742002_1178, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile14._COPYING_
2019-02-20 19:10:18,602 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742003_1179, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile14._COPYING_
2019-02-20 19:10:19,040 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742004_1180, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile14._COPYING_
2019-02-20 19:10:19,558 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742005_1181, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile14._COPYING_
2019-02-20 19:10:19,956 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742006_1182, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile14._COPYING_
2019-02-20 19:10:20,440 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742007_1183, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile14._COPYING_
2019-02-20 19:10:22,819 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742008_1184, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile14._COPYING_
2019-02-20 19:10:23,424 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile14._COPYING_ is closed by DFSClient_NONMAPREDUCE_-594922444_1
2019-02-20 19:10:24,756 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2019-02-20 19:10:24,759 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at clnode058.clemson.cloudlab.us/130.127.133.67
************************************************************/
2019-02-20 19:10:41,796 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode058.clemson.cloudlab.us/130.127.133.67
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-02-18T22:18Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-02-20 19:10:41,805 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-20 19:10:41,810 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-02-20 19:10:42,095 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-20 19:10:42,201 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-02-20 19:10:42,201 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-02-20 19:10:42,253 INFO org.apache.hadoop.hdfs.server.namenode.NameNodeUtils: fs.defaultFS is hdfs://mycluster
2019-02-20 19:10:42,253 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients should use mycluster to access this namenode/service.
2019-02-20 19:10:42,417 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-02-20 19:10:42,447 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://node-0-link-0:9870
2019-02-20 19:10:42,465 INFO org.eclipse.jetty.util.log: Logging initialized @1177ms
2019-02-20 19:10:42,571 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-20 19:10:42,584 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-02-20 19:10:42,595 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-20 19:10:42,598 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-02-20 19:10:42,598 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-20 19:10:42,598 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-20 19:10:42,625 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-02-20 19:10:42,625 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-02-20 19:10:42,635 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 9870
2019-02-20 19:10:42,636 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2019-02-20 19:10:42,672 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c137fd5{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-02-20 19:10:42,672 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d9d0818{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-02-20 19:10:42,747 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@376a0d86{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-02-20 19:10:42,770 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@536dbea0{HTTP/1.1,[http/1.1]}{node-0-link-0:9870}
2019-02-20 19:10:42,770 INFO org.eclipse.jetty.server.Server: Started @1484ms
2019-02-20 19:10:43,103 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-20 19:10:43,161 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2019-02-20 19:10:43,175 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2019-02-20 19:10:43,177 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2019-02-20 19:10:43,179 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2019-02-20 19:10:43,186 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-02-20 19:10:43,186 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-02-20 19:10:43,186 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2019-02-20 19:10:43,186 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID: mycluster
2019-02-20 19:10:43,186 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: true
2019-02-20 19:10:43,229 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-02-20 19:10:43,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-02-20 19:10:43,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-02-20 19:10:43,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-02-20 19:10:43,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Feb 20 19:10:43
2019-02-20 19:10:43,248 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-02-20 19:10:43,249 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-20 19:10:43,250 INFO org.apache.hadoop.util.GSet: 2.0% max memory 26.7 GB = 546.1 MB
2019-02-20 19:10:43,250 INFO org.apache.hadoop.util.GSet: capacity      = 2^26 = 67108864 entries
2019-02-20 19:10:43,383 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable = false
2019-02-20 19:10:43,392 INFO org.apache.hadoop.conf.Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-02-20 19:10:43,392 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-02-20 19:10:43,392 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2019-02-20 19:10:43,392 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2019-02-20 19:10:43,392 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2019-02-20 19:10:43,393 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-02-20 19:10:43,393 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-02-20 19:10:43,393 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-02-20 19:10:43,393 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2019-02-20 19:10:43,393 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-02-20 19:10:43,393 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-02-20 19:10:43,478 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-02-20 19:10:43,478 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-20 19:10:43,478 INFO org.apache.hadoop.util.GSet: 1.0% max memory 26.7 GB = 273.0 MB
2019-02-20 19:10:43,478 INFO org.apache.hadoop.util.GSet: capacity      = 2^25 = 33554432 entries
2019-02-20 19:10:43,552 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-02-20 19:10:43,552 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: POSIX ACL inheritance enabled? true
2019-02-20 19:10:43,552 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-02-20 19:10:43,552 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2019-02-20 19:10:43,559 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-02-20 19:10:43,562 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: SkipList is disabled
2019-02-20 19:10:43,567 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-02-20 19:10:43,567 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-20 19:10:43,567 INFO org.apache.hadoop.util.GSet: 0.25% max memory 26.7 GB = 68.3 MB
2019-02-20 19:10:43,567 INFO org.apache.hadoop.util.GSet: capacity      = 2^23 = 8388608 entries
2019-02-20 19:10:43,595 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-02-20 19:10:43,595 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-02-20 19:10:43,595 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-02-20 19:10:43,599 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-02-20 19:10:43,599 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-02-20 19:10:43,602 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-02-20 19:10:43,602 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-20 19:10:43,602 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-02-20 19:10:43,602 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-02-20 19:10:43,661 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 63743@clnode058.clemson.cloudlab.us
2019-02-20 19:10:45,070 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1148ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=281ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1002ms
2019-02-20 19:10:45,239 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-02-20 19:10:45,301 INFO org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager: Enable the erasure coding policy RS-6-3-1024k
2019-02-20 19:10:45,303 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-02-20 19:10:45,334 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-02-20 19:10:45,334 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-02-20 19:10:45,339 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@abf688e expecting start txid #1
2019-02-20 19:10:45,339 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:10:45,343 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:10:45,343 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:10:45,463 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 1048576 edits # 535 loaded in 0 seconds
2019-02-20 19:10:45,463 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@478ee483 expecting start txid #536
2019-02-20 19:10:45,463 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:10:45,463 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:10:45,463 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:10:45,468 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 1048576 edits # 7 loaded in 0 seconds
2019-02-20 19:10:45,468 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1a7288a3 expecting start txid #543
2019-02-20 19:10:45,468 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:10:45,468 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:10:45,468 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:10:45,474 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-20 19:10:45,474 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@2974f221 expecting start txid #544
2019-02-20 19:10:45,474 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:10:45,474 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:10:45,474 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:10:45,494 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 1048576 edits # 85 loaded in 0 seconds
2019-02-20 19:10:45,495 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-02-20 19:10:45,495 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-02-20 19:10:45,495 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 1885 msecs
2019-02-20 19:10:45,678 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to node-0-link-0:8020
2019-02-20 19:10:45,683 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-02-20 19:10:45,695 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2019-02-20 19:10:45,882 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-02-20 19:10:45,891 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2019-02-20 19:10:45,902 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2019-02-20 19:10:45,939 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-20 19:10:45,939 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2019-02-20 19:10:45,942 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: node-0-link-0/10.10.1.1:8020
2019-02-20 19:10:45,945 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for standby state
2019-02-20 19:10:45,951 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Will roll logs on active node every 120 seconds.
2019-02-20 19:10:45,958 INFO org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-1-link-0:9870]
Serving checkpoints at http://node-0-link-0:9870
2019-02-20 19:10:46,729 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.5:9866, datanodeUuid=fa00b432-7f72-4db1-a094-e44224a216a1, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937) storage fa00b432-7f72-4db1-a094-e44224a216a1
2019-02-20 19:10:46,730 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.5:9866
2019-02-20 19:10:46,731 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN fa00b432-7f72-4db1-a094-e44224a216a1 (10.10.1.5:9866).
2019-02-20 19:10:46,732 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=98e9e948-bdd7-441c-8fd1-0b747f77761a, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937) storage 98e9e948-bdd7-441c-8fd1-0b747f77761a
2019-02-20 19:10:46,732 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.2:9866
2019-02-20 19:10:46,733 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 98e9e948-bdd7-441c-8fd1-0b747f77761a (10.10.1.2:9866).
2019-02-20 19:10:46,733 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=45826c1d-460c-4636-8bd0-cdeb5313fd8f, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937) storage 45826c1d-460c-4636-8bd0-cdeb5313fd8f
2019-02-20 19:10:46,733 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.3:9866
2019-02-20 19:10:46,733 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 45826c1d-460c-4636-8bd0-cdeb5313fd8f (10.10.1.3:9866).
2019-02-20 19:10:46,744 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-955de0ba-c328-4360-9e47-0109a5ba597a for DN 10.10.1.2:9866
2019-02-20 19:10:46,747 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-c5ecc5a9-eaae-43a6-a11c-ee8c28bc06e9 for DN 10.10.1.3:9866
2019-02-20 19:10:46,748 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-7bf94cf2-19db-42f4-8ce2-dfd4b2d5a174 for DN 10.10.1.5:9866
2019-02-20 19:10:46,760 INFO BlockStateChange: BLOCK* processReport 0x51c00afc6c4f6365: Processing first storage report for DS-c5ecc5a9-eaae-43a6-a11c-ee8c28bc06e9 from datanode 45826c1d-460c-4636-8bd0-cdeb5313fd8f
2019-02-20 19:10:46,772 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 159 has reached the threshold 0.9990 of total blocks 160. The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2019-02-20 19:10:46,773 INFO BlockStateChange: BLOCK* processReport 0x51c00afc6c4f6365: from storage DS-c5ecc5a9-eaae-43a6-a11c-ee8c28bc06e9 node DatanodeRegistration(10.10.1.3:9866, datanodeUuid=45826c1d-460c-4636-8bd0-cdeb5313fd8f, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937), blocks: 160, hasStaleStorage: false, processing time: 13 msecs, invalidatedBlocks: 0
2019-02-20 19:10:46,773 INFO BlockStateChange: BLOCK* processReport 0x93f28186db62181b: Processing first storage report for DS-955de0ba-c328-4360-9e47-0109a5ba597a from datanode 98e9e948-bdd7-441c-8fd1-0b747f77761a
2019-02-20 19:10:46,777 INFO BlockStateChange: BLOCK* processReport 0x93f28186db62181b: from storage DS-955de0ba-c328-4360-9e47-0109a5ba597a node DatanodeRegistration(10.10.1.2:9866, datanodeUuid=98e9e948-bdd7-441c-8fd1-0b747f77761a, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937), blocks: 160, hasStaleStorage: false, processing time: 4 msecs, invalidatedBlocks: 0
2019-02-20 19:10:46,777 INFO BlockStateChange: BLOCK* processReport 0xc030fca1ebc1c29: Processing first storage report for DS-7bf94cf2-19db-42f4-8ce2-dfd4b2d5a174 from datanode fa00b432-7f72-4db1-a094-e44224a216a1
2019-02-20 19:10:46,780 INFO BlockStateChange: BLOCK* processReport 0xc030fca1ebc1c29: from storage DS-7bf94cf2-19db-42f4-8ce2-dfd4b2d5a174 node DatanodeRegistration(10.10.1.5:9866, datanodeUuid=fa00b432-7f72-4db1-a094-e44224a216a1, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937), blocks: 160, hasStaleStorage: false, processing time: 3 msecs, invalidatedBlocks: 0
2019-02-20 19:11:06,775 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 160 has reached the threshold 0.9990 of total blocks 160. The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2019-02-20 19:11:16,776 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2019-02-20 19:11:16,776 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 30 secs
2019-02-20 19:11:16,776 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 3 datanodes
2019-02-20 19:11:16,776 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2019-02-20 19:12:45,970 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:12:46,235 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1907834a expecting start txid #629
2019-02-20 19:12:46,235 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:12:46,236 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 629
2019-02-20 19:12:46,236 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 629
2019-02-20 19:12:46,259 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 8132 edits # 146 loaded in 0 seconds
2019-02-20 19:14:46,267 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:14:46,446 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3f3211f1 expecting start txid #775
2019-02-20 19:14:46,446 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:14:46,446 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 775
2019-02-20 19:14:46,446 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 775
2019-02-20 19:14:46,478 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 22265 edits # 398 loaded in 0 seconds
2019-02-20 19:16:46,486 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:16:46,680 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4e03313c expecting start txid #1173
2019-02-20 19:16:46,680 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:16:46,680 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1173
2019-02-20 19:16:46,680 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1173
2019-02-20 19:16:46,690 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 5939 edits # 107 loaded in 0 seconds
2019-02-20 19:18:46,697 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:18:46,882 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@23eca255 expecting start txid #1280
2019-02-20 19:18:46,882 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:18:46,882 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1280
2019-02-20 19:18:46,882 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1280
2019-02-20 19:18:46,904 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 15432 edits # 273 loaded in 0 seconds
2019-02-20 19:20:46,910 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:20:47,084 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@5af977bf expecting start txid #1553
2019-02-20 19:20:47,084 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:20:47,084 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1553
2019-02-20 19:20:47,084 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1553
2019-02-20 19:20:47,101 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 15128 edits # 272 loaded in 0 seconds
2019-02-20 19:20:53,650 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for standby state
2019-02-20 19:20:53,651 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Edit log tailer interrupted
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:479)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
2019-02-20 19:20:53,654 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2019-02-20 19:20:53,662 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Starting recovery process for unclosed journal segments...
2019-02-20 19:20:53,700 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Successfully started new epoch 6
2019-02-20 19:20:53,700 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Beginning recovery of unclosed segment starting at txid 1825
2019-02-20 19:20:53,720 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Recovery prepare phase complete. Responses:
10.10.1.2:8485: segmentState { startTxId: 1825 endTxId: 1861 isInProgress: true } lastWriterEpoch: 5 lastCommittedTxId: 1857
10.10.1.3:8485: segmentState { startTxId: 1825 endTxId: 1861 isInProgress: true } lastWriterEpoch: 5 lastCommittedTxId: 1857
10.10.1.5:8485: segmentState { startTxId: 1825 endTxId: 1861 isInProgress: true } lastWriterEpoch: 5 lastCommittedTxId: 1857
2019-02-20 19:20:53,723 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Using longest log: 10.10.1.2:8485=segmentState {
  startTxId: 1825
  endTxId: 1861
  isInProgress: true
}
lastWriterEpoch: 5
lastCommittedTxId: 1857

2019-02-20 19:20:53,763 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-root/dfs/name/current
2019-02-20 19:20:53,789 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000000544 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000000544-0000000000000000628
2019-02-20 19:20:53,807 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Catching up to latest edits from old active before taking over writer role in edits logs
2019-02-20 19:20:53,811 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3eda8df2 expecting start txid #1825
2019-02-20 19:20:53,811 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:20:53,812 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1825
2019-02-20 19:20:53,812 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1825
2019-02-20 19:20:53,819 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 1048576 edits # 37 loaded in 0 seconds
2019-02-20 19:20:53,819 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Marking all datanodes as stale
2019-02-20 19:20:53,820 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Reprocessing replication and invalidation queues
2019-02-20 19:20:53,821 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: initializing replication queues
2019-02-20 19:20:53,821 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Will take over writing edit logs at txnid 1862
2019-02-20 19:20:53,823 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1862
2019-02-20 19:20:54,069 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 0 SyncTimes(ms): 0 57 
2019-02-20 19:20:54,094 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Initializing quota with 4 thread(s)
2019-02-20 19:20:54,098 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Quota initialization completed in 3 milliseconds
name space=21
storage space=63619203072
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2019-02-20 19:20:54,103 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-02-20 19:20:54,170 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 158
2019-02-20 19:20:54,170 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2019-02-20 19:20:54,170 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2019-02-20 19:20:54,170 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2019-02-20 19:20:54,170 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 2
2019-02-20 19:20:54,170 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 349 msec
2019-02-20 19:20:55,675 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742359_1535, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile16._COPYING_
2019-02-20 19:20:55,675 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742360_1536, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-02-20 19:21:02,583 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile1._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1244676317_1
2019-02-20 19:21:02,605 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile16._COPYING_ is closed by DFSClient_NONMAPREDUCE_-811621574_1
2019-02-20 19:21:08,375 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742361_1537, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-02-20 19:21:12,351 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742362_1538, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile13._COPYING_
2019-02-20 19:21:13,771 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742363_1539, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-02-20 19:21:13,774 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742364_1540, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile9._COPYING_
2019-02-20 19:21:14,514 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742365_1541, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-02-20 19:21:14,695 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742366_1542, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile13._COPYING_
2019-02-20 19:21:15,253 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742367_1543, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile9._COPYING_
2019-02-20 19:21:15,312 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742368_1544, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-02-20 19:21:15,495 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742369_1545, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile13._COPYING_
2019-02-20 19:21:15,794 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742370_1546, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-02-20 19:21:15,829 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742371_1547, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile9._COPYING_
2019-02-20 19:21:15,951 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742372_1548, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile13._COPYING_
2019-02-20 19:21:16,672 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742373_1549, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-02-20 19:21:18,371 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742374_1550, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile8._COPYING_
2019-02-20 19:21:19,501 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742375_1551, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile9._COPYING_
2019-02-20 19:21:19,501 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742376_1552, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile13._COPYING_
2019-02-20 19:21:19,510 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742377_1553, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-02-20 19:21:20,035 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742378_1554, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-02-20 19:21:20,114 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742379_1555, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-02-20 19:21:21,494 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742380_1556, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile9._COPYING_
2019-02-20 19:21:21,497 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742381_1557, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile8._COPYING_
2019-02-20 19:21:21,498 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742382_1558, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile13._COPYING_
2019-02-20 19:21:22,466 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742383_1559, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile9._COPYING_
2019-02-20 19:21:22,563 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742384_1560, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile13._COPYING_
2019-02-20 19:21:22,573 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742385_1561, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-02-20 19:21:22,683 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742386_1562, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile8._COPYING_
2019-02-20 19:21:22,890 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742387_1563, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-02-20 19:21:23,231 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742388_1564, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile9._COPYING_
2019-02-20 19:21:23,445 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742389_1565, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile8._COPYING_
2019-02-20 19:21:23,450 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile3._COPYING_ is closed by DFSClient_NONMAPREDUCE_-188644236_1
2019-02-20 19:21:23,476 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742390_1566, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-02-20 19:21:25,679 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742391_1567, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile13._COPYING_
2019-02-20 19:21:25,680 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742392_1568, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile9._COPYING_
2019-02-20 19:21:28,360 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742393_1569, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-02-20 19:21:28,384 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742394_1570, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile8._COPYING_
2019-02-20 19:21:28,845 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2019-02-20 19:21:28,847 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at clnode058.clemson.cloudlab.us/130.127.133.67
************************************************************/
2019-02-20 19:21:45,883 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode058.clemson.cloudlab.us/130.127.133.67
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-02-18T22:18Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-02-20 19:21:45,893 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-20 19:21:45,898 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-02-20 19:21:46,183 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-20 19:21:46,288 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-02-20 19:21:46,288 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-02-20 19:21:46,340 INFO org.apache.hadoop.hdfs.server.namenode.NameNodeUtils: fs.defaultFS is hdfs://mycluster
2019-02-20 19:21:46,340 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients should use mycluster to access this namenode/service.
2019-02-20 19:21:46,503 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-02-20 19:21:46,531 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://node-0-link-0:9870
2019-02-20 19:21:46,549 INFO org.eclipse.jetty.util.log: Logging initialized @1180ms
2019-02-20 19:21:46,656 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-20 19:21:46,670 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-02-20 19:21:46,680 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-20 19:21:46,683 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-02-20 19:21:46,683 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-20 19:21:46,683 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-20 19:21:46,710 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-02-20 19:21:46,710 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-02-20 19:21:46,719 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 9870
2019-02-20 19:21:46,720 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2019-02-20 19:21:46,756 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c137fd5{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-02-20 19:21:46,757 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d9d0818{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-02-20 19:21:46,832 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@376a0d86{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-02-20 19:21:46,854 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@536dbea0{HTTP/1.1,[http/1.1]}{node-0-link-0:9870}
2019-02-20 19:21:46,854 INFO org.eclipse.jetty.server.Server: Started @1487ms
2019-02-20 19:21:47,209 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-20 19:21:47,262 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2019-02-20 19:21:47,276 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2019-02-20 19:21:47,277 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2019-02-20 19:21:47,280 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2019-02-20 19:21:47,287 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-02-20 19:21:47,287 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-02-20 19:21:47,287 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2019-02-20 19:21:47,287 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID: mycluster
2019-02-20 19:21:47,287 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: true
2019-02-20 19:21:47,330 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-02-20 19:21:47,343 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-02-20 19:21:47,343 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-02-20 19:21:47,347 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-02-20 19:21:47,348 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Feb 20 19:21:47
2019-02-20 19:21:47,350 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-02-20 19:21:47,350 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-20 19:21:47,351 INFO org.apache.hadoop.util.GSet: 2.0% max memory 26.7 GB = 546.1 MB
2019-02-20 19:21:47,352 INFO org.apache.hadoop.util.GSet: capacity      = 2^26 = 67108864 entries
2019-02-20 19:21:47,489 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable = false
2019-02-20 19:21:47,499 INFO org.apache.hadoop.conf.Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-02-20 19:21:47,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-02-20 19:21:47,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2019-02-20 19:21:47,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2019-02-20 19:21:47,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2019-02-20 19:21:47,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-02-20 19:21:47,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-02-20 19:21:47,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-02-20 19:21:47,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2019-02-20 19:21:47,500 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-02-20 19:21:47,500 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-02-20 19:21:47,583 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-02-20 19:21:47,583 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-20 19:21:47,583 INFO org.apache.hadoop.util.GSet: 1.0% max memory 26.7 GB = 273.0 MB
2019-02-20 19:21:47,583 INFO org.apache.hadoop.util.GSet: capacity      = 2^25 = 33554432 entries
2019-02-20 19:21:47,657 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-02-20 19:21:47,657 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: POSIX ACL inheritance enabled? true
2019-02-20 19:21:47,657 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-02-20 19:21:47,657 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2019-02-20 19:21:47,664 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-02-20 19:21:47,667 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: SkipList is disabled
2019-02-20 19:21:47,672 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-02-20 19:21:47,672 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-20 19:21:47,672 INFO org.apache.hadoop.util.GSet: 0.25% max memory 26.7 GB = 68.3 MB
2019-02-20 19:21:47,672 INFO org.apache.hadoop.util.GSet: capacity      = 2^23 = 8388608 entries
2019-02-20 19:21:47,700 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-02-20 19:21:47,700 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-02-20 19:21:47,700 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-02-20 19:21:47,704 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-02-20 19:21:47,704 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-02-20 19:21:47,707 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-02-20 19:21:47,707 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-20 19:21:47,707 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-02-20 19:21:47,707 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-02-20 19:21:47,755 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 64732@clnode058.clemson.cloudlab.us
2019-02-20 19:21:49,320 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1313ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=364ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1074ms
2019-02-20 19:21:49,489 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-02-20 19:21:49,551 INFO org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager: Enable the erasure coding policy RS-6-3-1024k
2019-02-20 19:21:49,553 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-02-20 19:21:49,584 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-02-20 19:21:49,584 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-02-20 19:21:49,589 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@abf688e expecting start txid #1
2019-02-20 19:21:49,589 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:21:49,593 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:21:49,593 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:21:49,710 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 1048576 edits # 535 loaded in 0 seconds
2019-02-20 19:21:49,710 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@478ee483 expecting start txid #536
2019-02-20 19:21:49,710 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:21:49,711 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:21:49,711 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:21:49,716 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 1048576 edits # 7 loaded in 0 seconds
2019-02-20 19:21:49,716 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1a7288a3 expecting start txid #543
2019-02-20 19:21:49,716 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:21:49,716 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:21:49,716 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:21:49,721 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-20 19:21:49,721 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@2974f221 expecting start txid #544
2019-02-20 19:21:49,721 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:21:49,722 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:21:49,722 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:21:49,739 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 1048576 edits # 85 loaded in 0 seconds
2019-02-20 19:21:49,739 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@58fe0499 expecting start txid #629
2019-02-20 19:21:49,739 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:21:49,739 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:21:49,739 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:21:49,752 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 8132 edits # 146 loaded in 0 seconds
2019-02-20 19:21:49,752 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@686449f9 expecting start txid #775
2019-02-20 19:21:49,753 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:21:49,753 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:21:49,753 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:21:49,774 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 22265 edits # 398 loaded in 0 seconds
2019-02-20 19:21:49,774 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@665df3c6 expecting start txid #1173
2019-02-20 19:21:49,774 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:21:49,775 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:21:49,775 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:21:49,781 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 5939 edits # 107 loaded in 0 seconds
2019-02-20 19:21:49,781 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@68b6f0d6 expecting start txid #1280
2019-02-20 19:21:49,781 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:21:49,781 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:21:49,781 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:21:49,796 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 15432 edits # 273 loaded in 0 seconds
2019-02-20 19:21:49,796 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4044fb95 expecting start txid #1553
2019-02-20 19:21:49,796 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:21:49,796 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:21:49,796 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:21:49,808 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 15128 edits # 272 loaded in 0 seconds
2019-02-20 19:21:49,808 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@aa549e5 expecting start txid #1825
2019-02-20 19:21:49,808 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:21:49,808 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:21:49,808 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:21:49,812 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 1048576 edits # 37 loaded in 0 seconds
2019-02-20 19:21:49,812 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@36f48b4 expecting start txid #1862
2019-02-20 19:21:49,812 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1862&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1862&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1862&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:21:49,812 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1862&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1862&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1862&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:21:49,812 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1862&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:21:49,820 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1862&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1862&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1862&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 1048576 edits # 125 loaded in 0 seconds
2019-02-20 19:21:49,820 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-02-20 19:21:49,820 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-02-20 19:21:49,820 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 2105 msecs
2019-02-20 19:21:50,002 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to node-0-link-0:8020
2019-02-20 19:21:50,007 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-02-20 19:21:50,019 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2019-02-20 19:21:50,205 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-02-20 19:21:50,214 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 4
2019-02-20 19:21:50,225 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 149 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2019-02-20 19:21:50,255 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-20 19:21:50,255 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2019-02-20 19:21:50,258 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: node-0-link-0/10.10.1.1:8020
2019-02-20 19:21:50,261 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for standby state
2019-02-20 19:21:50,265 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Will roll logs on active node every 120 seconds.
2019-02-20 19:21:50,271 INFO org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-1-link-0:9870]
Serving checkpoints at http://node-0-link-0:9870
2019-02-20 19:21:50,737 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=45826c1d-460c-4636-8bd0-cdeb5313fd8f, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937) storage 45826c1d-460c-4636-8bd0-cdeb5313fd8f
2019-02-20 19:21:50,738 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.3:9866
2019-02-20 19:21:50,739 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 45826c1d-460c-4636-8bd0-cdeb5313fd8f (10.10.1.3:9866).
2019-02-20 19:21:50,740 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.5:9866, datanodeUuid=fa00b432-7f72-4db1-a094-e44224a216a1, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937) storage fa00b432-7f72-4db1-a094-e44224a216a1
2019-02-20 19:21:50,740 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.5:9866
2019-02-20 19:21:50,740 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN fa00b432-7f72-4db1-a094-e44224a216a1 (10.10.1.5:9866).
2019-02-20 19:21:50,751 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-7bf94cf2-19db-42f4-8ce2-dfd4b2d5a174 for DN 10.10.1.5:9866
2019-02-20 19:21:50,754 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-c5ecc5a9-eaae-43a6-a11c-ee8c28bc06e9 for DN 10.10.1.3:9866
2019-02-20 19:21:50,766 INFO BlockStateChange: BLOCK* processReport 0x51c00afc6c4f6366: Processing first storage report for DS-c5ecc5a9-eaae-43a6-a11c-ee8c28bc06e9 from datanode 45826c1d-460c-4636-8bd0-cdeb5313fd8f
2019-02-20 19:21:50,776 INFO BlockStateChange: BLOCK* processReport 0x51c00afc6c4f6366: from storage DS-c5ecc5a9-eaae-43a6-a11c-ee8c28bc06e9 node DatanodeRegistration(10.10.1.3:9866, datanodeUuid=45826c1d-460c-4636-8bd0-cdeb5313fd8f, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937), blocks: 154, hasStaleStorage: false, processing time: 9 msecs, invalidatedBlocks: 0
2019-02-20 19:21:50,776 INFO BlockStateChange: BLOCK* processReport 0xc030fca1ebc1c2a: Processing first storage report for DS-7bf94cf2-19db-42f4-8ce2-dfd4b2d5a174 from datanode fa00b432-7f72-4db1-a094-e44224a216a1
2019-02-20 19:21:50,779 INFO BlockStateChange: BLOCK* processReport 0xc030fca1ebc1c2a: from storage DS-7bf94cf2-19db-42f4-8ce2-dfd4b2d5a174 node DatanodeRegistration(10.10.1.5:9866, datanodeUuid=fa00b432-7f72-4db1-a094-e44224a216a1, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937), blocks: 154, hasStaleStorage: false, processing time: 3 msecs, invalidatedBlocks: 0
2019-02-20 19:21:50,783 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=98e9e948-bdd7-441c-8fd1-0b747f77761a, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937) storage 98e9e948-bdd7-441c-8fd1-0b747f77761a
2019-02-20 19:21:50,783 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.2:9866
2019-02-20 19:21:50,784 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 98e9e948-bdd7-441c-8fd1-0b747f77761a (10.10.1.2:9866).
2019-02-20 19:21:50,785 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-955de0ba-c328-4360-9e47-0109a5ba597a for DN 10.10.1.2:9866
2019-02-20 19:21:50,787 INFO BlockStateChange: BLOCK* processReport 0x93f28186db62181c: Processing first storage report for DS-955de0ba-c328-4360-9e47-0109a5ba597a from datanode 98e9e948-bdd7-441c-8fd1-0b747f77761a
2019-02-20 19:21:50,791 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 149 has reached the threshold 0.9990 of total blocks 150. The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2019-02-20 19:21:50,791 INFO BlockStateChange: BLOCK* processReport 0x93f28186db62181c: from storage DS-955de0ba-c328-4360-9e47-0109a5ba597a node DatanodeRegistration(10.10.1.2:9866, datanodeUuid=98e9e948-bdd7-441c-8fd1-0b747f77761a, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937), blocks: 154, hasStaleStorage: false, processing time: 4 msecs, invalidatedBlocks: 0
2019-02-20 19:22:10,793 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 150 has reached the threshold 0.9990 of total blocks 150. The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2019-02-20 19:22:20,794 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2019-02-20 19:22:20,794 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 30 secs
2019-02-20 19:22:20,795 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 3 datanodes
2019-02-20 19:22:20,795 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2019-02-20 19:23:50,282 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:23:50,335 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-20 19:24:50,342 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:24:50,346 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-20 19:25:50,352 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:25:50,356 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-20 19:26:50,362 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:26:50,366 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-20 19:27:50,372 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:27:50,376 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-20 19:28:50,383 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:28:50,386 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-20 19:29:50,392 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:29:50,395 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-20 19:30:50,404 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:30:50,407 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-20 19:31:50,414 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:31:50,417 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 92 needs additional 57 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-20 19:31:58,181 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for standby state
2019-02-20 19:31:58,182 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Edit log tailer interrupted
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:479)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
2019-02-20 19:31:58,184 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2019-02-20 19:31:58,192 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Starting recovery process for unclosed journal segments...
2019-02-20 19:31:58,233 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Successfully started new epoch 8
2019-02-20 19:31:58,234 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Beginning recovery of unclosed segment starting at txid 1987
2019-02-20 19:31:58,254 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Recovery prepare phase complete. Responses:
10.10.1.2:8485: segmentState { startTxId: 1987 endTxId: 1987 isInProgress: true } lastWriterEpoch: 7 lastCommittedTxId: 1985
10.10.1.3:8485: segmentState { startTxId: 1987 endTxId: 1987 isInProgress: true } lastWriterEpoch: 7 lastCommittedTxId: 1985
10.10.1.5:8485: segmentState { startTxId: 1987 endTxId: 1987 isInProgress: true } lastWriterEpoch: 7 lastCommittedTxId: 1985
2019-02-20 19:31:58,256 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Using longest log: 10.10.1.2:8485=segmentState {
  startTxId: 1987
  endTxId: 1987
  isInProgress: true
}
lastWriterEpoch: 7
lastCommittedTxId: 1985

2019-02-20 19:31:58,297 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-root/dfs/name/current
2019-02-20 19:31:58,323 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000001862 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000001862-0000000000000001986
2019-02-20 19:31:58,341 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Catching up to latest edits from old active before taking over writer role in edits logs
2019-02-20 19:31:58,345 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@729f0453 expecting start txid #1987
2019-02-20 19:31:58,345 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1987&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1987&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:31:58,346 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1987&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1987&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1987
2019-02-20 19:31:58,346 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1987&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1987
2019-02-20 19:31:58,353 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1987&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1987&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-20 19:31:58,353 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Marking all datanodes as stale
2019-02-20 19:31:58,354 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Reprocessing replication and invalidation queues
2019-02-20 19:31:58,354 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: initializing replication queues
2019-02-20 19:31:58,354 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Will take over writing edit logs at txnid 1988
2019-02-20 19:31:58,357 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1988
2019-02-20 19:31:58,624 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 0 SyncTimes(ms): 0 69 
2019-02-20 19:31:58,646 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Initializing quota with 4 thread(s)
2019-02-20 19:31:58,650 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Quota initialization completed in 4 milliseconds
name space=21
storage space=62008590336
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2019-02-20 19:31:58,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-02-20 19:31:58,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 154
2019-02-20 19:31:58,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2019-02-20 19:31:58,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2019-02-20 19:31:58,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2019-02-20 19:31:58,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 4
2019-02-20 19:31:58,719 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 365 msec
2019-02-20 19:32:12,634 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742395_1571, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile7._COPYING_
2019-02-20 19:32:13,318 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742396_1572, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile7._COPYING_
2019-02-20 19:32:14,302 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742397_1573, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile7._COPYING_
2019-02-20 19:32:14,710 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742398_1574, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile7._COPYING_
2019-02-20 19:32:15,116 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742399_1575, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile7._COPYING_
2019-02-20 19:32:15,525 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742400_1576, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile7._COPYING_
2019-02-20 19:32:15,917 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742401_1577, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile7._COPYING_
2019-02-20 19:32:16,295 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742402_1578, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile7._COPYING_
2019-02-20 19:32:16,751 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile7._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1185145648_1
2019-02-20 19:32:17,727 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742403_1579, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile6._COPYING_
2019-02-20 19:32:17,947 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742404_1580, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-02-20 19:32:18,357 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742405_1581, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile6._COPYING_
2019-02-20 19:32:18,558 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742406_1582, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-02-20 19:32:18,833 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742407_1583, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile6._COPYING_
2019-02-20 19:32:19,024 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742408_1584, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-02-20 19:32:19,277 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742409_1585, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile6._COPYING_
2019-02-20 19:32:19,470 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742410_1586, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-02-20 19:32:19,702 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742411_1587, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile6._COPYING_
2019-02-20 19:32:19,918 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742412_1588, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-02-20 19:32:20,148 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742413_1589, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile6._COPYING_
2019-02-20 19:32:20,351 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742414_1590, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-02-20 19:32:20,652 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742415_1591, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile6._COPYING_
2019-02-20 19:32:20,777 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742416_1592, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-02-20 19:32:21,071 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742417_1593, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile6._COPYING_
2019-02-20 19:32:21,281 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742418_1594, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-02-20 19:32:21,544 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile6._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1171063094_1
2019-02-20 19:32:21,721 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile3._COPYING_ is closed by DFSClient_NONMAPREDUCE_-730270326_1
2019-02-20 19:32:32,885 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2019-02-20 19:32:32,887 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at clnode058.clemson.cloudlab.us/130.127.133.67
************************************************************/
2019-02-20 19:32:49,924 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode058.clemson.cloudlab.us/130.127.133.67
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-02-18T22:18Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-02-20 19:32:49,934 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-20 19:32:49,939 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-02-20 19:32:50,226 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-20 19:32:50,331 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-02-20 19:32:50,331 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-02-20 19:32:50,383 INFO org.apache.hadoop.hdfs.server.namenode.NameNodeUtils: fs.defaultFS is hdfs://mycluster
2019-02-20 19:32:50,383 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients should use mycluster to access this namenode/service.
2019-02-20 19:32:50,548 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-02-20 19:32:50,575 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://node-0-link-0:9870
2019-02-20 19:32:50,593 INFO org.eclipse.jetty.util.log: Logging initialized @1187ms
2019-02-20 19:32:50,702 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-20 19:32:50,716 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-02-20 19:32:50,727 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-20 19:32:50,730 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-02-20 19:32:50,730 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-20 19:32:50,730 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-20 19:32:50,757 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-02-20 19:32:50,757 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-02-20 19:32:50,766 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 9870
2019-02-20 19:32:50,768 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2019-02-20 19:32:50,803 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c137fd5{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-02-20 19:32:50,803 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d9d0818{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-02-20 19:32:50,879 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@376a0d86{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-02-20 19:32:50,901 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@536dbea0{HTTP/1.1,[http/1.1]}{node-0-link-0:9870}
2019-02-20 19:32:50,901 INFO org.eclipse.jetty.server.Server: Started @1497ms
2019-02-20 19:32:51,230 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-20 19:32:51,287 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2019-02-20 19:32:51,302 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2019-02-20 19:32:51,303 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2019-02-20 19:32:51,306 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2019-02-20 19:32:51,313 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-02-20 19:32:51,313 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-02-20 19:32:51,313 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2019-02-20 19:32:51,313 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID: mycluster
2019-02-20 19:32:51,313 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: true
2019-02-20 19:32:51,356 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-02-20 19:32:51,373 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-02-20 19:32:51,373 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-02-20 19:32:51,377 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-02-20 19:32:51,378 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Feb 20 19:32:51
2019-02-20 19:32:51,380 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-02-20 19:32:51,380 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-20 19:32:51,382 INFO org.apache.hadoop.util.GSet: 2.0% max memory 26.7 GB = 546.1 MB
2019-02-20 19:32:51,382 INFO org.apache.hadoop.util.GSet: capacity      = 2^26 = 67108864 entries
2019-02-20 19:32:51,529 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable = false
2019-02-20 19:32:51,538 INFO org.apache.hadoop.conf.Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-02-20 19:32:51,539 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-02-20 19:32:51,539 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2019-02-20 19:32:51,539 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2019-02-20 19:32:51,539 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2019-02-20 19:32:51,539 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-02-20 19:32:51,539 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-02-20 19:32:51,539 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-02-20 19:32:51,539 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2019-02-20 19:32:51,539 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-02-20 19:32:51,539 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-02-20 19:32:51,625 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-02-20 19:32:51,625 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-20 19:32:51,625 INFO org.apache.hadoop.util.GSet: 1.0% max memory 26.7 GB = 273.0 MB
2019-02-20 19:32:51,625 INFO org.apache.hadoop.util.GSet: capacity      = 2^25 = 33554432 entries
2019-02-20 19:32:51,698 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-02-20 19:32:51,698 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: POSIX ACL inheritance enabled? true
2019-02-20 19:32:51,698 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-02-20 19:32:51,699 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2019-02-20 19:32:51,705 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-02-20 19:32:51,708 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: SkipList is disabled
2019-02-20 19:32:51,714 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-02-20 19:32:51,714 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-20 19:32:51,714 INFO org.apache.hadoop.util.GSet: 0.25% max memory 26.7 GB = 68.3 MB
2019-02-20 19:32:51,714 INFO org.apache.hadoop.util.GSet: capacity      = 2^23 = 8388608 entries
2019-02-20 19:32:51,741 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-02-20 19:32:51,741 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-02-20 19:32:51,742 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-02-20 19:32:51,746 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-02-20 19:32:51,746 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-02-20 19:32:51,748 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-02-20 19:32:51,748 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-20 19:32:51,748 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-02-20 19:32:51,748 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-02-20 19:32:51,801 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 65734@clnode058.clemson.cloudlab.us
2019-02-20 19:32:53,176 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1125ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=350ms
GC pool 'PS Scavenge' had collection(s): count=1 time=895ms
2019-02-20 19:32:53,344 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-02-20 19:32:53,407 INFO org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager: Enable the erasure coding policy RS-6-3-1024k
2019-02-20 19:32:53,410 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-02-20 19:32:53,441 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-02-20 19:32:53,441 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-02-20 19:32:53,446 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@abf688e expecting start txid #1
2019-02-20 19:32:53,446 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:32:53,450 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,450 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,567 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 1048576 edits # 535 loaded in 0 seconds
2019-02-20 19:32:53,568 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@478ee483 expecting start txid #536
2019-02-20 19:32:53,568 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:32:53,568 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,568 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,573 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=536&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 1048576 edits # 7 loaded in 0 seconds
2019-02-20 19:32:53,573 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1a7288a3 expecting start txid #543
2019-02-20 19:32:53,573 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:32:53,573 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,574 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,579 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-20 19:32:53,579 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@2974f221 expecting start txid #544
2019-02-20 19:32:53,579 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:32:53,579 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,579 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,596 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=544&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 1048576 edits # 85 loaded in 0 seconds
2019-02-20 19:32:53,596 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@58fe0499 expecting start txid #629
2019-02-20 19:32:53,597 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:32:53,597 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,597 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,610 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=629&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 8132 edits # 146 loaded in 0 seconds
2019-02-20 19:32:53,610 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@686449f9 expecting start txid #775
2019-02-20 19:32:53,610 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:32:53,610 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,610 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,632 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=775&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 22265 edits # 398 loaded in 0 seconds
2019-02-20 19:32:53,633 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@665df3c6 expecting start txid #1173
2019-02-20 19:32:53,633 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:32:53,633 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,633 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,639 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1173&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 5939 edits # 107 loaded in 0 seconds
2019-02-20 19:32:53,639 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@68b6f0d6 expecting start txid #1280
2019-02-20 19:32:53,639 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:32:53,639 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,639 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,653 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1280&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 15432 edits # 273 loaded in 0 seconds
2019-02-20 19:32:53,653 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4044fb95 expecting start txid #1553
2019-02-20 19:32:53,653 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:32:53,654 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,654 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,666 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1553&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 15128 edits # 272 loaded in 0 seconds
2019-02-20 19:32:53,666 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@aa549e5 expecting start txid #1825
2019-02-20 19:32:53,666 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:32:53,666 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,666 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,670 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1825&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 1048576 edits # 37 loaded in 0 seconds
2019-02-20 19:32:53,670 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@36f48b4 expecting start txid #1862
2019-02-20 19:32:53,670 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1862&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1862&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1862&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:32:53,670 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1862&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1862&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1862&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,670 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1862&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,678 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1862&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1862&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1862&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 1048576 edits # 125 loaded in 0 seconds
2019-02-20 19:32:53,678 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@5c00384f expecting start txid #1987
2019-02-20 19:32:53,678 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1987&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1987&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1987&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:32:53,678 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1987&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1987&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1987&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,678 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1987&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,682 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1987&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1987&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1987&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-20 19:32:53,682 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3b7ff809 expecting start txid #1988
2019-02-20 19:32:53,682 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1988&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1988&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1988&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:32:53,682 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1988&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1988&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1988&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,682 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1988&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 1
2019-02-20 19:32:53,688 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1988&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1988&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1988&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 1048576 edits # 85 loaded in 0 seconds
2019-02-20 19:32:53,688 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-02-20 19:32:53,688 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-02-20 19:32:53,688 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 1931 msecs
2019-02-20 19:32:53,871 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to node-0-link-0:8020
2019-02-20 19:32:53,876 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-02-20 19:32:53,889 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2019-02-20 19:32:54,083 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-02-20 19:32:54,094 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 4
2019-02-20 19:32:54,106 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 149 blocks to reach the threshold 0.9990 of total blocks 150.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2019-02-20 19:32:54,137 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-20 19:32:54,137 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2019-02-20 19:32:54,140 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: node-0-link-0/10.10.1.1:8020
2019-02-20 19:32:54,143 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for standby state
2019-02-20 19:32:54,148 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Will roll logs on active node every 120 seconds.
2019-02-20 19:32:54,154 INFO org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-1-link-0:9870]
Serving checkpoints at http://node-0-link-0:9870
2019-02-20 19:32:54,917 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=98e9e948-bdd7-441c-8fd1-0b747f77761a, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937) storage 98e9e948-bdd7-441c-8fd1-0b747f77761a
2019-02-20 19:32:54,919 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.2:9866
2019-02-20 19:32:54,919 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 98e9e948-bdd7-441c-8fd1-0b747f77761a (10.10.1.2:9866).
2019-02-20 19:32:54,921 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=45826c1d-460c-4636-8bd0-cdeb5313fd8f, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937) storage 45826c1d-460c-4636-8bd0-cdeb5313fd8f
2019-02-20 19:32:54,921 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.3:9866
2019-02-20 19:32:54,921 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 45826c1d-460c-4636-8bd0-cdeb5313fd8f (10.10.1.3:9866).
2019-02-20 19:32:54,921 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.5:9866, datanodeUuid=fa00b432-7f72-4db1-a094-e44224a216a1, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937) storage fa00b432-7f72-4db1-a094-e44224a216a1
2019-02-20 19:32:54,921 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.5:9866
2019-02-20 19:32:54,922 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN fa00b432-7f72-4db1-a094-e44224a216a1 (10.10.1.5:9866).
2019-02-20 19:32:54,932 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-955de0ba-c328-4360-9e47-0109a5ba597a for DN 10.10.1.2:9866
2019-02-20 19:32:54,935 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-7bf94cf2-19db-42f4-8ce2-dfd4b2d5a174 for DN 10.10.1.5:9866
2019-02-20 19:32:54,936 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-c5ecc5a9-eaae-43a6-a11c-ee8c28bc06e9 for DN 10.10.1.3:9866
2019-02-20 19:32:54,948 INFO BlockStateChange: BLOCK* processReport 0x51c00afc6c4f6367: Processing first storage report for DS-c5ecc5a9-eaae-43a6-a11c-ee8c28bc06e9 from datanode 45826c1d-460c-4636-8bd0-cdeb5313fd8f
2019-02-20 19:32:54,962 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 149 has reached the threshold 0.9990 of total blocks 150. The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2019-02-20 19:32:54,962 INFO BlockStateChange: BLOCK* processReport 0x51c00afc6c4f6367: from storage DS-c5ecc5a9-eaae-43a6-a11c-ee8c28bc06e9 node DatanodeRegistration(10.10.1.3:9866, datanodeUuid=45826c1d-460c-4636-8bd0-cdeb5313fd8f, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937), blocks: 154, hasStaleStorage: false, processing time: 14 msecs, invalidatedBlocks: 0
2019-02-20 19:32:54,962 INFO BlockStateChange: BLOCK* processReport 0xc030fca1ebc1c2b: Processing first storage report for DS-7bf94cf2-19db-42f4-8ce2-dfd4b2d5a174 from datanode fa00b432-7f72-4db1-a094-e44224a216a1
2019-02-20 19:32:54,966 INFO BlockStateChange: BLOCK* processReport 0xc030fca1ebc1c2b: from storage DS-7bf94cf2-19db-42f4-8ce2-dfd4b2d5a174 node DatanodeRegistration(10.10.1.5:9866, datanodeUuid=fa00b432-7f72-4db1-a094-e44224a216a1, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937), blocks: 154, hasStaleStorage: false, processing time: 4 msecs, invalidatedBlocks: 0
2019-02-20 19:32:54,966 INFO BlockStateChange: BLOCK* processReport 0x93f28186db62181d: Processing first storage report for DS-955de0ba-c328-4360-9e47-0109a5ba597a from datanode 98e9e948-bdd7-441c-8fd1-0b747f77761a
2019-02-20 19:32:54,969 INFO BlockStateChange: BLOCK* processReport 0x93f28186db62181d: from storage DS-955de0ba-c328-4360-9e47-0109a5ba597a node DatanodeRegistration(10.10.1.2:9866, datanodeUuid=98e9e948-bdd7-441c-8fd1-0b747f77761a, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-3268194d-2de8-49bf-875c-db2fffd3f546;nsid=629877884;c=1550714226937), blocks: 154, hasStaleStorage: false, processing time: 3 msecs, invalidatedBlocks: 0
2019-02-20 19:33:14,964 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 150 has reached the threshold 0.9990 of total blocks 150. The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2019-02-20 19:33:24,965 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2019-02-20 19:33:24,965 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 30 secs
2019-02-20 19:33:24,965 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 3 datanodes
2019-02-20 19:33:24,965 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2019-02-20 19:34:54,168 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:34:54,402 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1deeb3ca expecting start txid #2073
2019-02-20 19:34:54,402 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2073&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2073&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:34:54,403 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2073&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2073&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 2073
2019-02-20 19:34:54,403 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2073&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 2073
2019-02-20 19:34:54,436 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2073&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2073&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 21521 edits # 383 loaded in 0 seconds
2019-02-20 19:36:54,444 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:36:54,621 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@7bde9d5c expecting start txid #2456
2019-02-20 19:36:54,621 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2456&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2456&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:36:54,621 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2456&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2456&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 2456
2019-02-20 19:36:54,621 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2456&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 2456
2019-02-20 19:36:54,633 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2456&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2456&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 9762 edits # 177 loaded in 0 seconds
2019-02-20 19:38:54,641 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:38:54,822 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@36458dd4 expecting start txid #2633
2019-02-20 19:38:54,822 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2633&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2633&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2633&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:38:54,823 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2633&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2633&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2633&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 2633
2019-02-20 19:38:54,823 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2633&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 2633
2019-02-20 19:38:54,842 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2633&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2633&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2633&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 18068 edits # 324 loaded in 0 seconds
2019-02-20 19:40:54,851 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:40:55,049 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1b14ac9c expecting start txid #2957
2019-02-20 19:40:55,049 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2957&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2957&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:40:55,049 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2957&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2957&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 2957
2019-02-20 19:40:55,049 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2957&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 2957
2019-02-20 19:40:55,062 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2957&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2957&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 12584 edits # 226 loaded in 0 seconds
2019-02-20 19:42:55,070 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-20 19:42:55,276 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@684b7c6b expecting start txid #3183
2019-02-20 19:42:55,276 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3183&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3183&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-20 19:42:55,276 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3183&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3183&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 3183
2019-02-20 19:42:55,276 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3183&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true' to transaction ID 3183
2019-02-20 19:42:55,291 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3183&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3183&storageInfo=-64%3A629877884%3A1550714226937%3ACID-3268194d-2de8-49bf-875c-db2fffd3f546&inProgressOk=true of size 13979 edits # 252 loaded in 0 seconds
