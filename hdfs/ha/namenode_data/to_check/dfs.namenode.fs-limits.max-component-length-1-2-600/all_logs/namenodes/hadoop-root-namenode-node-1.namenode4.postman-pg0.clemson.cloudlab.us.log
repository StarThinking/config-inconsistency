2019-02-21 13:45:06,162 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode042.clemson.cloudlab.us/130.127.133.51
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-02-19T02:28Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-02-21 13:45:06,172 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-21 13:45:06,177 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-02-21 13:45:06,466 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-21 13:45:06,573 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-02-21 13:45:06,573 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-02-21 13:45:06,624 INFO org.apache.hadoop.hdfs.server.namenode.NameNodeUtils: fs.defaultFS is hdfs://mycluster
2019-02-21 13:45:06,624 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients should use mycluster to access this namenode/service.
2019-02-21 13:45:06,783 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-02-21 13:45:06,811 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://node-1-link-0:9870
2019-02-21 13:45:06,829 INFO org.eclipse.jetty.util.log: Logging initialized @1199ms
2019-02-21 13:45:06,936 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-21 13:45:06,950 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-02-21 13:45:06,960 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-21 13:45:06,963 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-02-21 13:45:06,964 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-21 13:45:06,964 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-21 13:45:06,990 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-02-21 13:45:06,990 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-02-21 13:45:06,999 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 9870
2019-02-21 13:45:07,001 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2019-02-21 13:45:07,036 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c137fd5{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-02-21 13:45:07,037 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d9d0818{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-02-21 13:45:07,112 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@376a0d86{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-02-21 13:45:07,135 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@536dbea0{HTTP/1.1,[http/1.1]}{node-1-link-0:9870}
2019-02-21 13:45:07,135 INFO org.eclipse.jetty.server.Server: Started @1507ms
2019-02-21 13:45:07,473 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-21 13:45:07,524 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2019-02-21 13:45:07,539 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2019-02-21 13:45:07,540 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2019-02-21 13:45:07,543 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2019-02-21 13:45:07,549 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-02-21 13:45:07,549 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-02-21 13:45:07,550 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2019-02-21 13:45:07,550 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID: mycluster
2019-02-21 13:45:07,550 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: true
2019-02-21 13:45:07,593 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-02-21 13:45:07,605 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-02-21 13:45:07,605 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-02-21 13:45:07,610 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-02-21 13:45:07,610 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Feb 21 13:45:07
2019-02-21 13:45:07,612 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-02-21 13:45:07,612 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-21 13:45:07,614 INFO org.apache.hadoop.util.GSet: 2.0% max memory 26.7 GB = 546.1 MB
2019-02-21 13:45:07,614 INFO org.apache.hadoop.util.GSet: capacity      = 2^26 = 67108864 entries
2019-02-21 13:45:07,748 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable = false
2019-02-21 13:45:07,757 INFO org.apache.hadoop.conf.Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-02-21 13:45:07,757 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-02-21 13:45:07,757 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2019-02-21 13:45:07,757 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2019-02-21 13:45:07,758 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2019-02-21 13:45:07,758 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-02-21 13:45:07,758 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-02-21 13:45:07,758 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-02-21 13:45:07,758 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2019-02-21 13:45:07,758 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-02-21 13:45:07,758 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-02-21 13:45:07,843 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-02-21 13:45:07,843 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-21 13:45:07,843 INFO org.apache.hadoop.util.GSet: 1.0% max memory 26.7 GB = 273.0 MB
2019-02-21 13:45:07,843 INFO org.apache.hadoop.util.GSet: capacity      = 2^25 = 33554432 entries
2019-02-21 13:45:07,916 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-02-21 13:45:07,916 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: POSIX ACL inheritance enabled? true
2019-02-21 13:45:07,916 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-02-21 13:45:07,917 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2019-02-21 13:45:07,924 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-02-21 13:45:07,926 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: SkipList is disabled
2019-02-21 13:45:07,932 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-02-21 13:45:07,932 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-21 13:45:07,932 INFO org.apache.hadoop.util.GSet: 0.25% max memory 26.7 GB = 68.3 MB
2019-02-21 13:45:07,932 INFO org.apache.hadoop.util.GSet: capacity      = 2^23 = 8388608 entries
2019-02-21 13:45:07,959 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-02-21 13:45:07,960 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-02-21 13:45:07,960 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-02-21 13:45:07,964 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-02-21 13:45:07,964 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-02-21 13:45:07,966 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-02-21 13:45:07,966 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-21 13:45:07,967 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-02-21 13:45:07,967 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-02-21 13:45:07,993 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 100490@clnode042.clemson.cloudlab.us
2019-02-21 13:45:09,361 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1074ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=330ms
GC pool 'PS Scavenge' had collection(s): count=1 time=914ms
2019-02-21 13:45:10,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-4-link-0/10.10.1.2:8485. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-21 13:45:10,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-21 13:45:10,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-21 13:45:11,485 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-21 13:45:11,485 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-4-link-0/10.10.1.2:8485. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-21 13:45:11,485 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-21 13:45:12,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-21 13:45:12,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-4-link-0/10.10.1.2:8485. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-21 13:45:12,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-21 13:45:13,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-21 13:45:13,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-21 13:45:13,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-4-link-0/10.10.1.2:8485. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-21 13:45:14,097 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Waited 6001 ms (timeout=20000 ms) for a response for selectInputStreams. No responses yet.
2019-02-21 13:45:14,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-21 13:45:14,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-4-link-0/10.10.1.2:8485. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-21 13:45:14,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-21 13:45:15,083 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2019-02-21 13:45:15,083 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-02-21 13:45:15,144 INFO org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager: Enable the erasure coding policy RS-6-3-1024k
2019-02-21 13:45:15,146 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-02-21 13:45:15,177 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-02-21 13:45:15,177 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-02-21 13:45:15,182 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-02-21 13:45:15,182 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-02-21 13:45:15,182 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 7208 msecs
2019-02-21 13:45:15,361 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to node-1-link-0:8020
2019-02-21 13:45:15,366 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-02-21 13:45:15,379 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2019-02-21 13:45:15,567 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-02-21 13:45:15,575 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2019-02-21 13:45:15,585 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2019-02-21 13:45:15,586 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2019-02-21 13:45:15,586 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2019-02-21 13:45:15,620 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-21 13:45:15,620 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2019-02-21 13:45:15,630 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: node-1-link-0/10.10.1.4:8020
2019-02-21 13:45:15,634 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for standby state
2019-02-21 13:45:15,638 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Will roll logs on active node every 120 seconds.
2019-02-21 13:45:15,645 INFO org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-0-link-0:9870]
Serving checkpoints at http://node-1-link-0:9870
2019-02-21 13:45:16,230 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.5:9866, datanodeUuid=bbbd9e3d-a9c1-412c-8eea-c0eaa8156b8f, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-23e35154-5274-4de9-8d07-196c4382bd6c;nsid=1969679814;c=1550781885051) storage bbbd9e3d-a9c1-412c-8eea-c0eaa8156b8f
2019-02-21 13:45:16,232 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.5:9866
2019-02-21 13:45:16,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN bbbd9e3d-a9c1-412c-8eea-c0eaa8156b8f (10.10.1.5:9866).
2019-02-21 13:45:16,259 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=ace0929e-b1d4-4c28-bb36-3a8207830786, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-23e35154-5274-4de9-8d07-196c4382bd6c;nsid=1969679814;c=1550781885051) storage ace0929e-b1d4-4c28-bb36-3a8207830786
2019-02-21 13:45:16,260 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.3:9866
2019-02-21 13:45:16,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN ace0929e-b1d4-4c28-bb36-3a8207830786 (10.10.1.3:9866).
2019-02-21 13:45:16,280 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=f0848aeb-442f-4329-ab0e-4f5304ca825c, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-23e35154-5274-4de9-8d07-196c4382bd6c;nsid=1969679814;c=1550781885051) storage f0848aeb-442f-4329-ab0e-4f5304ca825c
2019-02-21 13:45:16,280 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.2:9866
2019-02-21 13:45:16,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN f0848aeb-442f-4329-ab0e-4f5304ca825c (10.10.1.2:9866).
2019-02-21 13:45:16,320 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-c5c2b850-4fc6-431b-b274-5d2834f0b52c for DN 10.10.1.3:9866
2019-02-21 13:45:16,324 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-d7ea203f-77fb-4e5b-ae28-c6a58b3e0edc for DN 10.10.1.5:9866
2019-02-21 13:45:16,329 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-b3edba74-952c-4bbb-8bd0-34ebe7c744f8 for DN 10.10.1.2:9866
2019-02-21 13:45:16,373 INFO BlockStateChange: BLOCK* processReport 0xdf611fe4e7a4d6b3: Processing first storage report for DS-d7ea203f-77fb-4e5b-ae28-c6a58b3e0edc from datanode bbbd9e3d-a9c1-412c-8eea-c0eaa8156b8f
2019-02-21 13:45:16,375 INFO BlockStateChange: BLOCK* processReport 0xdf611fe4e7a4d6b3: from storage DS-d7ea203f-77fb-4e5b-ae28-c6a58b3e0edc node DatanodeRegistration(10.10.1.5:9866, datanodeUuid=bbbd9e3d-a9c1-412c-8eea-c0eaa8156b8f, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-23e35154-5274-4de9-8d07-196c4382bd6c;nsid=1969679814;c=1550781885051), blocks: 0, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0
2019-02-21 13:45:16,375 INFO BlockStateChange: BLOCK* processReport 0x8958ecc37eacf768: Processing first storage report for DS-c5c2b850-4fc6-431b-b274-5d2834f0b52c from datanode ace0929e-b1d4-4c28-bb36-3a8207830786
2019-02-21 13:45:16,375 INFO BlockStateChange: BLOCK* processReport 0x8958ecc37eacf768: from storage DS-c5c2b850-4fc6-431b-b274-5d2834f0b52c node DatanodeRegistration(10.10.1.3:9866, datanodeUuid=ace0929e-b1d4-4c28-bb36-3a8207830786, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-23e35154-5274-4de9-8d07-196c4382bd6c;nsid=1969679814;c=1550781885051), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-02-21 13:45:16,375 INFO BlockStateChange: BLOCK* processReport 0x2ff0023e1a36ea41: Processing first storage report for DS-b3edba74-952c-4bbb-8bd0-34ebe7c744f8 from datanode f0848aeb-442f-4329-ab0e-4f5304ca825c
2019-02-21 13:45:16,375 INFO BlockStateChange: BLOCK* processReport 0x2ff0023e1a36ea41: from storage DS-b3edba74-952c-4bbb-8bd0-34ebe7c744f8 node DatanodeRegistration(10.10.1.2:9866, datanodeUuid=f0848aeb-442f-4329-ab0e-4f5304ca825c, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-23e35154-5274-4de9-8d07-196c4382bd6c;nsid=1969679814;c=1550781885051), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-02-21 13:45:17,007 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for standby state
2019-02-21 13:45:17,009 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Edit log tailer interrupted
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:479)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
2019-02-21 13:45:17,113 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2019-02-21 13:45:17,122 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Starting recovery process for unclosed journal segments...
2019-02-21 13:45:17,207 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Successfully started new epoch 1
2019-02-21 13:45:17,207 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-root/dfs/name/current
2019-02-21 13:45:17,207 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Catching up to latest edits from old active before taking over writer role in edits logs
2019-02-21 13:45:17,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Marking all datanodes as stale
2019-02-21 13:45:17,213 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Reprocessing replication and invalidation queues
2019-02-21 13:45:17,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: initializing replication queues
2019-02-21 13:45:17,213 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Will take over writing edit logs at txnid 1
2019-02-21 13:45:17,217 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2019-02-21 13:45:17,701 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Initializing quota with 4 thread(s)
2019-02-21 13:45:17,706 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Quota initialization completed in 5 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2019-02-21 13:45:17,712 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2019-02-21 13:45:17,712 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-02-21 13:45:17,712 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2019-02-21 13:45:17,712 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2019-02-21 13:45:17,712 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2019-02-21 13:45:17,712 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2019-02-21 13:45:17,712 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 499 msec
2019-02-21 13:45:22,958 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile17._COPYING_
2019-02-21 13:45:22,961 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile14._COPYING_
2019-02-21 13:45:22,984 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile13._COPYING_
2019-02-21 13:45:22,986 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile10._COPYING_
2019-02-21 13:45:23,059 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile19._COPYING_
2019-02-21 13:45:23,063 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-02-21 13:45:23,069 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile11._COPYING_
2019-02-21 13:45:23,101 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile8._COPYING_
2019-02-21 13:45:23,116 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile6._COPYING_
2019-02-21 13:45:23,124 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile18._COPYING_
2019-02-21 13:45:23,129 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-02-21 13:45:23,145 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile16._COPYING_
2019-02-21 13:45:23,152 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-02-21 13:45:23,160 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile12._COPYING_
2019-02-21 13:45:23,161 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-02-21 13:45:23,167 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile15._COPYING_
2019-02-21 13:45:23,172 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741841_1017, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile9._COPYING_
2019-02-21 13:45:23,173 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741842_1018, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-02-21 13:45:23,271 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741843_1019, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile7._COPYING_
2019-02-21 13:45:23,317 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741844_1020, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile20._COPYING_
2019-02-21 13:45:25,134 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741845_1021, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile12._COPYING_
2019-02-21 13:45:25,418 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741846_1022, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile7._COPYING_
2019-02-21 13:45:25,441 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741847_1023, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile16._COPYING_
2019-02-21 13:45:25,452 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741848_1024, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile10._COPYING_
2019-02-21 13:45:25,468 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741849_1025, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-02-21 13:45:25,562 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741850_1026, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile13._COPYING_
2019-02-21 13:45:25,604 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741851_1027, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile1._COPYING_
2019-02-21 13:45:25,677 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741852_1028, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile15._COPYING_
2019-02-21 13:45:25,686 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741853_1029, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile11._COPYING_
2019-02-21 13:45:25,705 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741854_1030, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-02-21 13:45:25,709 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741855_1031, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-02-21 13:45:25,712 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741856_1032, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile17._COPYING_
2019-02-21 13:45:25,737 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741857_1033, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile9._COPYING_
2019-02-21 13:45:25,740 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741858_1034, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile19._COPYING_
2019-02-21 13:45:25,759 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741859_1035, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-02-21 13:45:25,788 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741860_1036, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile20._COPYING_
2019-02-21 13:45:25,798 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741861_1037, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile14._COPYING_
2019-02-21 13:45:25,810 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741862_1038, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile8._COPYING_
2019-02-21 13:45:25,813 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741863_1039, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile6._COPYING_
2019-02-21 13:45:25,874 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741864_1040, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile18._COPYING_
2019-02-21 13:45:27,635 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741865_1041, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile7._COPYING_
2019-02-21 13:45:27,692 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741866_1042, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile13._COPYING_
2019-02-21 13:45:27,714 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741867_1043, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile12._COPYING_
2019-02-21 13:45:27,800 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741868_1044, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-02-21 13:45:27,832 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741869_1045, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-02-21 13:45:27,886 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741870_1046, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile10._COPYING_
2019-02-21 13:45:27,904 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741871_1047, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-02-21 13:45:27,905 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741872_1048, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile1._COPYING_
2019-02-21 13:45:27,923 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741873_1049, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile19._COPYING_
2019-02-21 13:45:27,926 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741874_1050, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile16._COPYING_
2019-02-21 13:45:27,927 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741875_1051, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile18._COPYING_
2019-02-21 13:45:28,016 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741876_1052, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile9._COPYING_
2019-02-21 13:45:28,040 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741877_1053, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile14._COPYING_
2019-02-21 13:45:28,061 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741878_1054, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile17._COPYING_
2019-02-21 13:45:28,068 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741879_1055, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile15._COPYING_
2019-02-21 13:45:28,081 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741880_1056, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile20._COPYING_
2019-02-21 13:45:28,116 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741881_1057, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-02-21 13:45:28,124 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741882_1058, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile11._COPYING_
2019-02-21 13:45:29,980 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741883_1059, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile7._COPYING_
2019-02-21 13:45:29,980 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741884_1060, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-02-21 13:45:29,981 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741885_1061, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-02-21 13:45:29,982 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741886_1062, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile13._COPYING_
2019-02-21 13:45:29,982 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741887_1063, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile8._COPYING_
2019-02-21 13:45:29,983 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741888_1064, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile6._COPYING_
2019-02-21 13:45:29,984 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741889_1065, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile12._COPYING_
2019-02-21 13:45:29,984 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741890_1066, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile17._COPYING_
2019-02-21 13:45:29,984 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741891_1067, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-02-21 13:45:29,985 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741892_1068, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile10._COPYING_
2019-02-21 13:45:30,038 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741893_1069, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile9._COPYING_
2019-02-21 13:45:30,068 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741894_1070, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile19._COPYING_
2019-02-21 13:45:30,073 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741895_1071, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-02-21 13:45:30,083 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741896_1072, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile14._COPYING_
2019-02-21 13:45:30,083 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741897_1073, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile16._COPYING_
2019-02-21 13:45:30,148 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741898_1074, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile11._COPYING_
2019-02-21 13:45:30,194 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741899_1075, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-02-21 13:45:30,293 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741900_1076, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile18._COPYING_
2019-02-21 13:45:30,317 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741901_1077, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile15._COPYING_
2019-02-21 13:45:30,536 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741902_1078, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile20._COPYING_
2019-02-21 13:45:30,719 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741903_1079, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile7._COPYING_
2019-02-21 13:45:32,087 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741904_1080, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-02-21 13:45:32,194 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741905_1081, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile14._COPYING_
2019-02-21 13:45:32,236 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741906_1082, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile19._COPYING_
2019-02-21 13:45:32,298 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741907_1083, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile12._COPYING_
2019-02-21 13:45:32,367 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741908_1084, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile11._COPYING_
2019-02-21 13:45:32,380 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741909_1085, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-02-21 13:45:32,404 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741910_1086, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile8._COPYING_
2019-02-21 13:45:32,430 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741911_1087, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile17._COPYING_
2019-02-21 13:45:32,432 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741912_1088, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile16._COPYING_
2019-02-21 13:45:32,438 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741913_1089, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile10._COPYING_
2019-02-21 13:45:32,472 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741914_1090, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile18._COPYING_
2019-02-21 13:45:32,472 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741915_1091, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-02-21 13:45:32,494 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741916_1092, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-02-21 13:45:32,547 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741917_1093, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile13._COPYING_
2019-02-21 13:45:32,580 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741918_1094, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile9._COPYING_
2019-02-21 13:45:32,646 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741919_1095, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile6._COPYING_
2019-02-21 13:45:32,667 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741920_1096, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile15._COPYING_
2019-02-21 13:45:32,687 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741921_1097, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-02-21 13:45:32,954 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741922_1098, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile7._COPYING_
2019-02-21 13:45:34,916 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741923_1099, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-02-21 13:45:34,917 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741924_1100, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile19._COPYING_
2019-02-21 13:45:34,917 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741925_1101, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile17._COPYING_
2019-02-21 13:45:34,918 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741926_1102, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile12._COPYING_
2019-02-21 13:45:34,918 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741927_1103, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile20._COPYING_
2019-02-21 13:45:34,919 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741928_1104, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile13._COPYING_
2019-02-21 13:45:34,919 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741929_1105, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile14._COPYING_
2019-02-21 13:45:34,920 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741930_1106, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile8._COPYING_
2019-02-21 13:45:34,920 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741931_1107, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-02-21 13:45:34,920 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741932_1108, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile6._COPYING_
2019-02-21 13:45:34,921 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741933_1109, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile9._COPYING_
2019-02-21 13:45:34,922 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741934_1110, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-02-21 13:45:34,922 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741935_1111, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile18._COPYING_
2019-02-21 13:45:34,922 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741936_1112, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-02-21 13:45:34,923 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741937_1113, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile10._COPYING_
2019-02-21 13:45:34,923 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741938_1114, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-02-21 13:45:34,923 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741939_1115, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile16._COPYING_
2019-02-21 13:45:34,924 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741940_1116, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile11._COPYING_
2019-02-21 13:45:34,924 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741941_1117, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile7._COPYING_
2019-02-21 13:45:34,924 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741942_1118, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile15._COPYING_
2019-02-21 13:45:36,261 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741943_1119, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile13._COPYING_
2019-02-21 13:45:36,362 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741944_1120, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile15._COPYING_
2019-02-21 13:45:36,374 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741945_1121, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile18._COPYING_
2019-02-21 13:45:36,513 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741946_1122, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile7._COPYING_
2019-02-21 13:45:36,582 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741947_1123, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-02-21 13:45:36,605 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741948_1124, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-02-21 13:45:36,646 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741949_1125, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile9._COPYING_
2019-02-21 13:45:36,724 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741950_1126, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-02-21 13:45:36,941 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741951_1127, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile17._COPYING_
2019-02-21 13:45:36,957 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741952_1128, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile14._COPYING_
2019-02-21 13:45:36,983 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741953_1129, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile12._COPYING_
2019-02-21 13:45:37,004 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741954_1130, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-02-21 13:45:37,010 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741955_1131, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile8._COPYING_
2019-02-21 13:45:37,124 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741956_1132, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile19._COPYING_
2019-02-21 13:45:38,092 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile7._COPYING_ is closed by DFSClient_NONMAPREDUCE_1003488820_1
2019-02-21 13:45:38,117 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741957_1133, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile20._COPYING_
2019-02-21 13:45:38,405 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741958_1134, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-02-21 13:45:38,475 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741959_1135, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile9._COPYING_
2019-02-21 13:45:38,518 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741960_1136, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-02-21 13:45:38,562 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741961_1137, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-02-21 13:45:38,643 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741962_1138, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile10._COPYING_
2019-02-21 13:45:38,698 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741963_1139, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile6._COPYING_
2019-02-21 13:45:38,727 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741964_1140, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile11._COPYING_
2019-02-21 13:45:38,926 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741965_1141, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile19._COPYING_
2019-02-21 13:45:38,928 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741966_1142, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile18._COPYING_
2019-02-21 13:45:39,000 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741967_1143, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile15._COPYING_
2019-02-21 13:45:39,051 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741968_1144, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile13._COPYING_
2019-02-21 13:45:39,165 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741969_1145, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile14._COPYING_
2019-02-21 13:45:39,257 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741970_1146, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile16._COPYING_
2019-02-21 13:45:39,491 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741971_1147, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile17._COPYING_
2019-02-21 13:45:39,583 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741972_1148, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-02-21 13:45:39,599 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741973_1149, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile12._COPYING_
2019-02-21 13:45:39,727 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741974_1150, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile8._COPYING_
2019-02-21 13:45:39,905 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741975_1151, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-02-21 13:45:40,198 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741976_1152, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-02-21 13:45:40,268 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741977_1153, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile20._COPYING_
2019-02-21 13:45:40,464 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile18._COPYING_ is closed by DFSClient_NONMAPREDUCE_-199736536_1
2019-02-21 13:45:40,498 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile9._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1441126212_1
2019-02-21 13:45:40,808 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741978_1154, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile10._COPYING_
2019-02-21 13:45:40,830 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile4._COPYING_ is closed by DFSClient_NONMAPREDUCE_1263977986_1
2019-02-21 13:45:40,846 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile13._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1828581221_1
2019-02-21 13:45:40,991 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741979_1155, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile6._COPYING_
2019-02-21 13:45:41,040 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741980_1156, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile11._COPYING_
2019-02-21 13:45:41,088 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile17._COPYING_ is closed by DFSClient_NONMAPREDUCE_1278273436_1
2019-02-21 13:45:41,189 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile15._COPYING_ is closed by DFSClient_NONMAPREDUCE_1836498065_1
2019-02-21 13:45:41,298 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile2._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1483117149_1
2019-02-21 13:45:41,440 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile14._COPYING_ is closed by DFSClient_NONMAPREDUCE_-188968836_1
2019-02-21 13:45:41,452 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile19._COPYING_ is closed by DFSClient_NONMAPREDUCE_-438813596_1
2019-02-21 13:45:41,461 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile3._COPYING_ is closed by DFSClient_NONMAPREDUCE_2036676315_1
2019-02-21 13:45:41,485 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741981_1157, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile16._COPYING_
2019-02-21 13:45:41,497 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741982_1158, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile8._COPYING_
2019-02-21 13:45:41,499 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile5._COPYING_ is closed by DFSClient_NONMAPREDUCE_1618198366_1
2019-02-21 13:45:41,649 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile10._COPYING_ is closed by DFSClient_NONMAPREDUCE_-796780371_1
2019-02-21 13:45:41,692 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile12._COPYING_ is closed by DFSClient_NONMAPREDUCE_664316960_1
2019-02-21 13:45:41,719 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile1._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1126517249_1
2019-02-21 13:45:41,763 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741983_1159, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile20._COPYING_
2019-02-21 13:45:41,867 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741984_1160, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile6._COPYING_
2019-02-21 13:45:42,117 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile8._COPYING_ is closed by DFSClient_NONMAPREDUCE_1414405454_1
2019-02-21 13:45:42,129 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile16._COPYING_ is closed by DFSClient_NONMAPREDUCE_354040698_1
2019-02-21 13:45:42,153 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile11._COPYING_ is closed by DFSClient_NONMAPREDUCE_-846338577_1
2019-02-21 13:45:42,359 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile20._COPYING_ is closed by DFSClient_NONMAPREDUCE_113927560_1
2019-02-21 13:45:42,456 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile6._COPYING_ is closed by DFSClient_NONMAPREDUCE_1716450796_1
2019-02-21 13:46:21,225 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2019-02-21 13:46:21,227 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at clnode042.clemson.cloudlab.us/130.127.133.51
************************************************************/
2019-02-21 13:46:38,258 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode042.clemson.cloudlab.us/130.127.133.51
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-02-19T02:28Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-02-21 13:46:38,268 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-21 13:46:38,273 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-02-21 13:46:38,558 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-21 13:46:38,662 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-02-21 13:46:38,662 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-02-21 13:46:38,714 INFO org.apache.hadoop.hdfs.server.namenode.NameNodeUtils: fs.defaultFS is hdfs://mycluster
2019-02-21 13:46:38,714 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients should use mycluster to access this namenode/service.
2019-02-21 13:46:38,882 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-02-21 13:46:38,910 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://node-1-link-0:9870
2019-02-21 13:46:38,927 INFO org.eclipse.jetty.util.log: Logging initialized @1182ms
2019-02-21 13:46:39,036 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-21 13:46:39,050 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-02-21 13:46:39,061 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-21 13:46:39,064 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-02-21 13:46:39,064 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-21 13:46:39,064 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-21 13:46:39,091 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-02-21 13:46:39,091 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-02-21 13:46:39,100 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 9870
2019-02-21 13:46:39,101 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2019-02-21 13:46:39,137 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c137fd5{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-02-21 13:46:39,140 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d9d0818{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-02-21 13:46:39,215 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@376a0d86{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-02-21 13:46:39,239 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@536dbea0{HTTP/1.1,[http/1.1]}{node-1-link-0:9870}
2019-02-21 13:46:39,240 INFO org.eclipse.jetty.server.Server: Started @1496ms
2019-02-21 13:46:39,582 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-21 13:46:39,633 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2019-02-21 13:46:39,647 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2019-02-21 13:46:39,649 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2019-02-21 13:46:39,652 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2019-02-21 13:46:39,658 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-02-21 13:46:39,658 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-02-21 13:46:39,658 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2019-02-21 13:46:39,659 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID: mycluster
2019-02-21 13:46:39,659 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: true
2019-02-21 13:46:39,702 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-02-21 13:46:39,714 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-02-21 13:46:39,714 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-02-21 13:46:39,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-02-21 13:46:39,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Feb 21 13:46:39
2019-02-21 13:46:39,721 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-02-21 13:46:39,721 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-21 13:46:39,723 INFO org.apache.hadoop.util.GSet: 2.0% max memory 26.7 GB = 546.1 MB
2019-02-21 13:46:39,723 INFO org.apache.hadoop.util.GSet: capacity      = 2^26 = 67108864 entries
2019-02-21 13:46:39,856 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable = false
2019-02-21 13:46:39,866 INFO org.apache.hadoop.conf.Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-02-21 13:46:39,866 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-02-21 13:46:39,866 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2019-02-21 13:46:39,866 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2019-02-21 13:46:39,866 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2019-02-21 13:46:39,866 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-02-21 13:46:39,866 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-02-21 13:46:39,866 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-02-21 13:46:39,866 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2019-02-21 13:46:39,866 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-02-21 13:46:39,867 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-02-21 13:46:39,951 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-02-21 13:46:39,951 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-21 13:46:39,951 INFO org.apache.hadoop.util.GSet: 1.0% max memory 26.7 GB = 273.0 MB
2019-02-21 13:46:39,951 INFO org.apache.hadoop.util.GSet: capacity      = 2^25 = 33554432 entries
2019-02-21 13:46:40,025 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-02-21 13:46:40,025 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: POSIX ACL inheritance enabled? true
2019-02-21 13:46:40,025 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-02-21 13:46:40,025 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2019-02-21 13:46:40,032 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-02-21 13:46:40,035 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: SkipList is disabled
2019-02-21 13:46:40,040 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-02-21 13:46:40,040 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-21 13:46:40,041 INFO org.apache.hadoop.util.GSet: 0.25% max memory 26.7 GB = 68.3 MB
2019-02-21 13:46:40,041 INFO org.apache.hadoop.util.GSet: capacity      = 2^23 = 8388608 entries
2019-02-21 13:46:40,068 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-02-21 13:46:40,068 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-02-21 13:46:40,068 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-02-21 13:46:40,072 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-02-21 13:46:40,073 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-02-21 13:46:40,075 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-02-21 13:46:40,075 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-21 13:46:40,075 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-02-21 13:46:40,075 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-02-21 13:46:40,130 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 100916@clnode042.clemson.cloudlab.us
2019-02-21 13:46:41,586 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1199ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=308ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1025ms
2019-02-21 13:46:41,761 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-02-21 13:46:41,825 INFO org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager: Enable the erasure coding policy RS-6-3-1024k
2019-02-21 13:46:41,827 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-02-21 13:46:41,858 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-02-21 13:46:41,858 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-02-21 13:46:41,865 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@abf688e expecting start txid #1
2019-02-21 13:46:41,865 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 13:46:41,869 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 13:46:41,869 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 13:46:42,014 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile17._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,026 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile14._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,027 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile10._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,028 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile13._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,032 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile19._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,033 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile1._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,034 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile11._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,035 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile8._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,036 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile6._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,036 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile18._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,037 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile15._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,038 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile16._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,039 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile2._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,039 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile5._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,040 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile9._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,041 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile3._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,042 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile4._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,042 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile12._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,045 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile7._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,045 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile20._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,071 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile7 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,076 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile18 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,077 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile9 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,077 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile4 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,078 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile13 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,079 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile17 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,080 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile15 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,080 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile2 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,081 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile14 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,082 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile19 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,082 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile3 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,083 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile5 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,084 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile10 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,084 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile12 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,085 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile1 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,086 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile8 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,087 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile16 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,087 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile11 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,088 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile20 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,093 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile6 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 13:46:42,094 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 1048576 edits # 541 loaded in 0 seconds
2019-02-21 13:46:42,094 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-02-21 13:46:42,094 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-02-21 13:46:42,094 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 2011 msecs
2019-02-21 13:46:42,282 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to node-1-link-0:8020
2019-02-21 13:46:42,287 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-02-21 13:46:42,300 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2019-02-21 13:46:42,488 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-02-21 13:46:42,496 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2019-02-21 13:46:42,508 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2019-02-21 13:46:42,539 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-21 13:46:42,539 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2019-02-21 13:46:42,544 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: node-1-link-0/10.10.1.4:8020
2019-02-21 13:46:42,547 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for standby state
2019-02-21 13:46:42,553 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Will roll logs on active node every 120 seconds.
2019-02-21 13:46:42,559 INFO org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-0-link-0:9870]
Serving checkpoints at http://node-1-link-0:9870
2019-02-21 13:46:42,743 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=f0848aeb-442f-4329-ab0e-4f5304ca825c, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-23e35154-5274-4de9-8d07-196c4382bd6c;nsid=1969679814;c=1550781885051) storage f0848aeb-442f-4329-ab0e-4f5304ca825c
2019-02-21 13:46:42,744 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.2:9866
2019-02-21 13:46:42,745 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN f0848aeb-442f-4329-ab0e-4f5304ca825c (10.10.1.2:9866).
2019-02-21 13:46:42,757 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-b3edba74-952c-4bbb-8bd0-34ebe7c744f8 for DN 10.10.1.2:9866
2019-02-21 13:46:42,773 INFO BlockStateChange: BLOCK* processReport 0x2ff0023e1a36ea42: Processing first storage report for DS-b3edba74-952c-4bbb-8bd0-34ebe7c744f8 from datanode f0848aeb-442f-4329-ab0e-4f5304ca825c
2019-02-21 13:46:42,791 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 159 has reached the threshold 0.9990 of total blocks 160. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 30 seconds.
2019-02-21 13:46:42,791 INFO BlockStateChange: BLOCK* processReport 0x2ff0023e1a36ea42: from storage DS-b3edba74-952c-4bbb-8bd0-34ebe7c744f8 node DatanodeRegistration(10.10.1.2:9866, datanodeUuid=f0848aeb-442f-4329-ab0e-4f5304ca825c, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-23e35154-5274-4de9-8d07-196c4382bd6c;nsid=1969679814;c=1550781885051), blocks: 160, hasStaleStorage: false, processing time: 17 msecs, invalidatedBlocks: 0
2019-02-21 13:46:43,268 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.5:9866, datanodeUuid=bbbd9e3d-a9c1-412c-8eea-c0eaa8156b8f, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-23e35154-5274-4de9-8d07-196c4382bd6c;nsid=1969679814;c=1550781885051) storage bbbd9e3d-a9c1-412c-8eea-c0eaa8156b8f
2019-02-21 13:46:43,268 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.5:9866
2019-02-21 13:46:43,268 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN bbbd9e3d-a9c1-412c-8eea-c0eaa8156b8f (10.10.1.5:9866).
2019-02-21 13:46:43,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-d7ea203f-77fb-4e5b-ae28-c6a58b3e0edc for DN 10.10.1.5:9866
2019-02-21 13:46:43,273 INFO BlockStateChange: BLOCK* processReport 0xdf611fe4e7a4d6b4: Processing first storage report for DS-d7ea203f-77fb-4e5b-ae28-c6a58b3e0edc from datanode bbbd9e3d-a9c1-412c-8eea-c0eaa8156b8f
2019-02-21 13:46:43,276 INFO BlockStateChange: BLOCK* processReport 0xdf611fe4e7a4d6b4: from storage DS-d7ea203f-77fb-4e5b-ae28-c6a58b3e0edc node DatanodeRegistration(10.10.1.5:9866, datanodeUuid=bbbd9e3d-a9c1-412c-8eea-c0eaa8156b8f, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-23e35154-5274-4de9-8d07-196c4382bd6c;nsid=1969679814;c=1550781885051), blocks: 160, hasStaleStorage: false, processing time: 4 msecs, invalidatedBlocks: 0
2019-02-21 13:46:43,287 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=ace0929e-b1d4-4c28-bb36-3a8207830786, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-23e35154-5274-4de9-8d07-196c4382bd6c;nsid=1969679814;c=1550781885051) storage ace0929e-b1d4-4c28-bb36-3a8207830786
2019-02-21 13:46:43,287 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.3:9866
2019-02-21 13:46:43,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN ace0929e-b1d4-4c28-bb36-3a8207830786 (10.10.1.3:9866).
2019-02-21 13:46:43,289 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-c5c2b850-4fc6-431b-b274-5d2834f0b52c for DN 10.10.1.3:9866
2019-02-21 13:46:43,292 INFO BlockStateChange: BLOCK* processReport 0x8958ecc37eacf769: Processing first storage report for DS-c5c2b850-4fc6-431b-b274-5d2834f0b52c from datanode ace0929e-b1d4-4c28-bb36-3a8207830786
2019-02-21 13:46:43,295 INFO BlockStateChange: BLOCK* processReport 0x8958ecc37eacf769: from storage DS-c5c2b850-4fc6-431b-b274-5d2834f0b52c node DatanodeRegistration(10.10.1.3:9866, datanodeUuid=ace0929e-b1d4-4c28-bb36-3a8207830786, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-23e35154-5274-4de9-8d07-196c4382bd6c;nsid=1969679814;c=1550781885051), blocks: 160, hasStaleStorage: false, processing time: 3 msecs, invalidatedBlocks: 0
2019-02-21 13:47:00,937 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for standby state
2019-02-21 13:47:00,937 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Edit log tailer interrupted
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:479)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
2019-02-21 13:47:00,939 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2019-02-21 13:47:00,949 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Starting recovery process for unclosed journal segments...
2019-02-21 13:47:00,996 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Successfully started new epoch 3
2019-02-21 13:47:00,996 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Beginning recovery of unclosed segment starting at txid 542
2019-02-21 13:47:01,017 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Recovery prepare phase complete. Responses:
10.10.1.5:8485: segmentState { startTxId: 542 endTxId: 542 isInProgress: true } lastWriterEpoch: 2 lastCommittedTxId: 540
10.10.1.3:8485: segmentState { startTxId: 542 endTxId: 542 isInProgress: true } lastWriterEpoch: 2 lastCommittedTxId: 540
10.10.1.2:8485: segmentState { startTxId: 542 endTxId: 542 isInProgress: true } lastWriterEpoch: 2 lastCommittedTxId: 540
2019-02-21 13:47:01,019 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Using longest log: 10.10.1.5:8485=segmentState {
  startTxId: 542
  endTxId: 542
  isInProgress: true
}
lastWriterEpoch: 2
lastCommittedTxId: 540

2019-02-21 13:47:01,059 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-root/dfs/name/current
2019-02-21 13:47:01,088 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000000001-0000000000000000541
2019-02-21 13:47:01,105 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Catching up to latest edits from old active before taking over writer role in edits logs
2019-02-21 13:47:01,108 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@6fd0076c expecting start txid #542
2019-02-21 13:47:01,109 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 13:47:01,109 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 542
2019-02-21 13:47:01,109 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 542
2019-02-21 13:47:01,208 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-21 13:47:01,208 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Marking all datanodes as stale
2019-02-21 13:47:01,209 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Will take over writing edit logs at txnid 543
2019-02-21 13:47:01,211 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 543
2019-02-21 13:47:01,483 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Initializing quota with 4 thread(s)
2019-02-21 13:47:01,487 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Quota initialization completed in 3 milliseconds
name space=21
storage space=64424509440
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2019-02-21 13:47:01,492 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-02-21 13:47:02,794 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 160 has reached the threshold 0.9990 of total blocks 160. The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2019-02-21 13:47:12,795 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: initializing replication queues
2019-02-21 13:47:12,795 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2019-02-21 13:47:12,795 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 30 secs
2019-02-21 13:47:12,795 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 3 datanodes
2019-02-21 13:47:12,796 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2019-02-21 13:47:12,859 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 160
2019-02-21 13:47:12,859 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2019-02-21 13:47:12,859 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2019-02-21 13:47:12,859 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2019-02-21 13:47:12,859 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2019-02-21 13:47:12,859 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 64 msec
2019-02-21 13:49:19,623 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 10.10.1.1
2019-02-21 13:49:19,623 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-02-21 13:49:19,623 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 543, 543
2019-02-21 13:49:19,624 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 542 Number of syncs: 1 SyncTimes(ms): 14 54 
2019-02-21 13:49:19,646 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 542 Number of syncs: 2 SyncTimes(ms): 25 65 
2019-02-21 13:49:19,651 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000000543 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000000543-0000000000000000544
2019-02-21 13:49:19,651 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 545
2019-02-21 13:49:37,232 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:53036: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile1._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:49:38,530 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:53050: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile10._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:49:39,001 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:53054: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile6._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:49:39,054 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:53058: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile14._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:49:39,601 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:53066: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile19._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:49:41,567 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:53082: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile9._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:49:41,731 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:53094: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile17._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:49:46,261 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:53150: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile5._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:49:46,487 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:53154: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile3._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:49:46,650 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:53158: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile13._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:49:47,892 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:53170: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile12._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:49:49,612 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:53186: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile8._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:49:51,655 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:53204: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile18._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:49:53,049 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:53218: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile2._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:49:55,848 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:53242: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile16._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:49:57,597 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:53262: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile15._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:49:57,942 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:53270: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile20._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:49:58,219 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:53274: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile4._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:49:58,836 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:53286: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile11._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:50:06,179 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:53350: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile7._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:51:19,837 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 10.10.1.1
2019-02-21 13:51:19,837 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-02-21 13:51:19,837 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 545, 565
2019-02-21 13:51:19,837 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 22 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 21 SyncTimes(ms): 196 201 
2019-02-21 13:51:19,853 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 22 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 22 SyncTimes(ms): 206 206 
2019-02-21 13:51:19,856 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000000545 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000000545-0000000000000000566
2019-02-21 13:51:19,856 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 567
2019-02-21 13:51:59,039 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:54082: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile1._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:52:00,946 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:54086: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile10._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:52:01,759 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:54098: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile14._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:52:02,015 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:54102: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile6._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:52:02,640 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:54106: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile19._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:52:04,232 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:54124: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile17._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:52:04,257 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:54126: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile9._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:52:08,471 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:54186: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile5._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:52:09,075 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:54190: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile3._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:52:09,279 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:54198: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile13._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:52:10,024 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:54210: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile12._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:52:11,034 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:54218: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile8._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:52:14,244 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:54248: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile18._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:52:15,843 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:54258: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile2._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:52:17,803 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:54278: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile16._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:52:19,286 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:54292: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile15._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:52:20,142 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:54306: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile20._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:52:20,804 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:54318: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile4._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:52:21,901 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:54328: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile11._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:52:28,554 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:54392: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile7._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:53:20,034 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 10.10.1.1
2019-02-21 13:53:20,034 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-02-21 13:53:20,034 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 567, 567
2019-02-21 13:53:20,035 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 1 SyncTimes(ms): 8 62 
2019-02-21 13:53:20,048 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 17 66 
2019-02-21 13:53:20,051 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000000567 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000000567-0000000000000000568
2019-02-21 13:53:20,051 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 569
2019-02-21 13:54:20,979 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:55122: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile1._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:54:22,726 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:55126: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile10._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:54:24,014 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:55138: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile14._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:54:24,771 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:55142: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile6._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:54:25,632 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:55150: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile19._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:54:26,874 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:55170: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile17._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:54:27,373 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:55176: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile9._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:54:30,638 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:55220: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile5._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:54:31,694 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:55230: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile3._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:54:31,739 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:55234: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile13._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:54:32,519 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:55250: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile12._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:54:32,855 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:55254: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile8._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:54:35,807 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:55278: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile18._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:54:38,529 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:55310: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile2._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:54:39,936 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:55318: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile16._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:54:41,222 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:55334: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile15._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:54:41,922 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:55342: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile20._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:54:43,155 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:55354: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile4._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:54:44,601 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:55374: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile11._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:54:51,106 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:55438: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile7._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:55:20,227 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 10.10.1.1
2019-02-21 13:55:20,227 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-02-21 13:55:20,228 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 569, 569
2019-02-21 13:55:20,228 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 1 SyncTimes(ms): 10 66 
2019-02-21 13:55:20,247 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 19 75 
2019-02-21 13:55:20,249 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000000569 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000000569-0000000000000000570
2019-02-21 13:55:20,250 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 571
2019-02-21 13:56:43,180 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:56162: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile1._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:56:44,178 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:56166: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile10._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:56:45,848 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:56178: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile14._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:56:46,560 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:56182: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile6._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:56:48,114 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:56190: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile19._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:56:49,483 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:56214: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile17._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:56:50,025 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:56226: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile9._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:56:52,836 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:56262: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile5._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:56:53,797 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:56270: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile13._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:56:54,330 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:56274: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile3._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:56:54,623 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:56282: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile12._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:56:55,202 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:56290: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile8._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:56:57,586 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:56314: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile18._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:57:00,928 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:56350: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile2._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:57:02,633 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:56362: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile16._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:57:03,356 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:56370: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile15._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:57:03,749 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:56382: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile20._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:57:05,340 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:56394: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile4._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:57:07,001 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:56414: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile11._COPYING_ in directory / is exceeded: limit=1 length=18
2019-02-21 13:57:13,381 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 10.10.1.6:56474: org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile7._COPYING_ in directory / is exceeded: limit=1 length=17
2019-02-21 13:57:20,436 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 10.10.1.1
2019-02-21 13:57:20,436 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-02-21 13:57:20,436 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 571, 571
2019-02-21 13:57:20,436 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 1 SyncTimes(ms): 6 66 
2019-02-21 13:57:20,453 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 16 73 
2019-02-21 13:57:20,456 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000000571 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000000571-0000000000000000572
2019-02-21 13:57:20,456 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 573
2019-02-21 13:57:25,243 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2019-02-21 13:57:25,246 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at clnode042.clemson.cloudlab.us/130.127.133.51
************************************************************/
2019-02-21 13:57:42,288 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode042.clemson.cloudlab.us/130.127.133.51
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-02-19T02:28Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-02-21 13:57:42,300 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-21 13:57:42,305 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-02-21 13:57:42,590 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-21 13:57:42,694 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-02-21 13:57:42,694 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-02-21 13:57:42,746 INFO org.apache.hadoop.hdfs.server.namenode.NameNodeUtils: fs.defaultFS is hdfs://mycluster
2019-02-21 13:57:42,746 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients should use mycluster to access this namenode/service.
2019-02-21 13:57:42,909 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-02-21 13:57:42,937 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://node-1-link-0:9870
2019-02-21 13:57:42,955 INFO org.eclipse.jetty.util.log: Logging initialized @1178ms
2019-02-21 13:57:43,062 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-21 13:57:43,076 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-02-21 13:57:43,087 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-21 13:57:43,090 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-02-21 13:57:43,090 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-21 13:57:43,090 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-21 13:57:43,117 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-02-21 13:57:43,117 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-02-21 13:57:43,127 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 9870
2019-02-21 13:57:43,128 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2019-02-21 13:57:43,163 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c137fd5{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-02-21 13:57:43,164 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d9d0818{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-02-21 13:57:43,239 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@376a0d86{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-02-21 13:57:43,259 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@536dbea0{HTTP/1.1,[http/1.1]}{node-1-link-0:9870}
2019-02-21 13:57:43,259 INFO org.eclipse.jetty.server.Server: Started @1483ms
2019-02-21 13:57:43,600 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-21 13:57:43,651 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2019-02-21 13:57:43,665 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2019-02-21 13:57:43,667 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2019-02-21 13:57:43,669 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2019-02-21 13:57:43,676 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-02-21 13:57:43,676 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-02-21 13:57:43,676 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2019-02-21 13:57:43,676 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID: mycluster
2019-02-21 13:57:43,677 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: true
2019-02-21 13:57:43,719 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-02-21 13:57:43,732 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-02-21 13:57:43,732 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-02-21 13:57:43,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-02-21 13:57:43,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Feb 21 13:57:43
2019-02-21 13:57:43,739 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-02-21 13:57:43,739 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-21 13:57:43,740 INFO org.apache.hadoop.util.GSet: 2.0% max memory 26.7 GB = 546.1 MB
2019-02-21 13:57:43,741 INFO org.apache.hadoop.util.GSet: capacity      = 2^26 = 67108864 entries
2019-02-21 13:57:43,882 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable = false
2019-02-21 13:57:43,891 INFO org.apache.hadoop.conf.Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-02-21 13:57:43,892 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-02-21 13:57:43,892 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2019-02-21 13:57:43,892 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2019-02-21 13:57:43,892 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2019-02-21 13:57:43,892 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-02-21 13:57:43,892 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-02-21 13:57:43,892 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-02-21 13:57:43,892 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2019-02-21 13:57:43,892 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-02-21 13:57:43,892 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-02-21 13:57:43,976 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-02-21 13:57:43,976 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-21 13:57:43,976 INFO org.apache.hadoop.util.GSet: 1.0% max memory 26.7 GB = 273.0 MB
2019-02-21 13:57:43,976 INFO org.apache.hadoop.util.GSet: capacity      = 2^25 = 33554432 entries
2019-02-21 13:57:44,049 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-02-21 13:57:44,049 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: POSIX ACL inheritance enabled? true
2019-02-21 13:57:44,049 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-02-21 13:57:44,050 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2019-02-21 13:57:44,056 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-02-21 13:57:44,059 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: SkipList is disabled
2019-02-21 13:57:44,065 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-02-21 13:57:44,065 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-21 13:57:44,065 INFO org.apache.hadoop.util.GSet: 0.25% max memory 26.7 GB = 68.3 MB
2019-02-21 13:57:44,065 INFO org.apache.hadoop.util.GSet: capacity      = 2^23 = 8388608 entries
2019-02-21 13:57:44,092 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-02-21 13:57:44,093 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-02-21 13:57:44,093 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-02-21 13:57:44,097 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-02-21 13:57:44,097 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-02-21 13:57:44,099 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-02-21 13:57:44,099 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-21 13:57:44,100 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-02-21 13:57:44,100 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-02-21 13:57:44,168 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 101308@clnode042.clemson.cloudlab.us
2019-02-21 13:57:45,592 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1179ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=348ms
GC pool 'PS Scavenge' had collection(s): count=1 time=948ms
2019-02-21 13:57:45,759 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-02-21 13:57:45,820 INFO org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager: Enable the erasure coding policy RS-6-3-1024k
2019-02-21 13:57:45,822 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-02-21 13:57:45,852 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-02-21 13:57:45,852 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-02-21 13:57:45,857 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@abf688e expecting start txid #1
2019-02-21 13:57:45,858 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 13:57:45,861 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 13:57:45,861 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 13:57:45,982 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 1048576 edits # 541 loaded in 0 seconds
2019-02-21 13:57:45,982 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@478ee483 expecting start txid #542
2019-02-21 13:57:45,982 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 13:57:45,983 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 13:57:45,983 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 13:57:45,987 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-21 13:57:45,987 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1a7288a3 expecting start txid #543
2019-02-21 13:57:45,988 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 13:57:45,988 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 13:57:45,988 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 13:57:45,990 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 42 edits # 2 loaded in 0 seconds
2019-02-21 13:57:45,990 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@2974f221 expecting start txid #545
2019-02-21 13:57:45,990 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 13:57:45,991 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 13:57:45,991 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 13:57:46,006 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 1193 edits # 22 loaded in 0 seconds
2019-02-21 13:57:46,006 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@58fe0499 expecting start txid #567
2019-02-21 13:57:46,006 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=567&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=567&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=567&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 13:57:46,006 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=567&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=567&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=567&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 13:57:46,006 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=567&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 13:57:46,008 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=567&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=567&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=567&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 42 edits # 2 loaded in 0 seconds
2019-02-21 13:57:46,008 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@686449f9 expecting start txid #569
2019-02-21 13:57:46,008 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=569&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=569&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=569&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 13:57:46,008 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=569&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=569&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=569&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 13:57:46,008 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=569&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 13:57:46,010 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=569&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=569&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=569&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 42 edits # 2 loaded in 0 seconds
2019-02-21 13:57:46,010 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@665df3c6 expecting start txid #571
2019-02-21 13:57:46,010 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=571&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=571&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=571&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 13:57:46,010 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=571&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=571&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=571&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 13:57:46,010 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=571&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 13:57:46,012 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=571&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=571&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=571&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 42 edits # 2 loaded in 0 seconds
2019-02-21 13:57:46,012 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@68b6f0d6 expecting start txid #573
2019-02-21 13:57:46,012 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=573&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=573&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=573&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 13:57:46,012 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=573&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=573&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=573&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 13:57:46,013 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=573&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 13:57:46,016 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=573&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=573&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=573&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-21 13:57:46,016 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-02-21 13:57:46,017 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-02-21 13:57:46,017 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 1909 msecs
2019-02-21 13:57:46,197 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to node-1-link-0:8020
2019-02-21 13:57:46,202 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-02-21 13:57:46,214 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2019-02-21 13:57:46,400 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-02-21 13:57:46,408 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2019-02-21 13:57:46,418 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2019-02-21 13:57:46,418 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2019-02-21 13:57:46,419 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2019-02-21 13:57:46,453 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-21 13:57:46,453 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2019-02-21 13:57:46,457 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: node-1-link-0/10.10.1.4:8020
2019-02-21 13:57:46,460 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for standby state
2019-02-21 13:57:46,466 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Will roll logs on active node every 120 seconds.
2019-02-21 13:57:46,473 INFO org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-0-link-0:9870]
Serving checkpoints at http://node-1-link-0:9870
2019-02-21 13:57:46,672 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.5:9866, datanodeUuid=bbbd9e3d-a9c1-412c-8eea-c0eaa8156b8f, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-23e35154-5274-4de9-8d07-196c4382bd6c;nsid=1969679814;c=1550781885051) storage bbbd9e3d-a9c1-412c-8eea-c0eaa8156b8f
2019-02-21 13:57:46,673 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.5:9866
2019-02-21 13:57:46,674 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN bbbd9e3d-a9c1-412c-8eea-c0eaa8156b8f (10.10.1.5:9866).
2019-02-21 13:57:46,676 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=ace0929e-b1d4-4c28-bb36-3a8207830786, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-23e35154-5274-4de9-8d07-196c4382bd6c;nsid=1969679814;c=1550781885051) storage ace0929e-b1d4-4c28-bb36-3a8207830786
2019-02-21 13:57:46,676 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.3:9866
2019-02-21 13:57:46,676 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN ace0929e-b1d4-4c28-bb36-3a8207830786 (10.10.1.3:9866).
2019-02-21 13:57:46,686 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-c5c2b850-4fc6-431b-b274-5d2834f0b52c for DN 10.10.1.3:9866
2019-02-21 13:57:46,689 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-d7ea203f-77fb-4e5b-ae28-c6a58b3e0edc for DN 10.10.1.5:9866
2019-02-21 13:57:46,701 INFO BlockStateChange: BLOCK* processReport 0x8958ecc37eacf76a: Processing first storage report for DS-c5c2b850-4fc6-431b-b274-5d2834f0b52c from datanode ace0929e-b1d4-4c28-bb36-3a8207830786
2019-02-21 13:57:46,702 INFO BlockStateChange: BLOCK* processReport 0x8958ecc37eacf76a: from storage DS-c5c2b850-4fc6-431b-b274-5d2834f0b52c node DatanodeRegistration(10.10.1.3:9866, datanodeUuid=ace0929e-b1d4-4c28-bb36-3a8207830786, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-23e35154-5274-4de9-8d07-196c4382bd6c;nsid=1969679814;c=1550781885051), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2019-02-21 13:57:46,702 INFO BlockStateChange: BLOCK* processReport 0xdf611fe4e7a4d6b5: Processing first storage report for DS-d7ea203f-77fb-4e5b-ae28-c6a58b3e0edc from datanode bbbd9e3d-a9c1-412c-8eea-c0eaa8156b8f
2019-02-21 13:57:46,702 INFO BlockStateChange: BLOCK* processReport 0xdf611fe4e7a4d6b5: from storage DS-d7ea203f-77fb-4e5b-ae28-c6a58b3e0edc node DatanodeRegistration(10.10.1.5:9866, datanodeUuid=bbbd9e3d-a9c1-412c-8eea-c0eaa8156b8f, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-23e35154-5274-4de9-8d07-196c4382bd6c;nsid=1969679814;c=1550781885051), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-02-21 13:57:46,797 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=f0848aeb-442f-4329-ab0e-4f5304ca825c, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-23e35154-5274-4de9-8d07-196c4382bd6c;nsid=1969679814;c=1550781885051) storage f0848aeb-442f-4329-ab0e-4f5304ca825c
2019-02-21 13:57:46,797 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.2:9866
2019-02-21 13:57:46,797 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN f0848aeb-442f-4329-ab0e-4f5304ca825c (10.10.1.2:9866).
2019-02-21 13:57:46,799 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-b3edba74-952c-4bbb-8bd0-34ebe7c744f8 for DN 10.10.1.2:9866
2019-02-21 13:57:46,800 INFO BlockStateChange: BLOCK* processReport 0x2ff0023e1a36ea43: Processing first storage report for DS-b3edba74-952c-4bbb-8bd0-34ebe7c744f8 from datanode f0848aeb-442f-4329-ab0e-4f5304ca825c
2019-02-21 13:57:46,800 INFO BlockStateChange: BLOCK* processReport 0x2ff0023e1a36ea43: from storage DS-b3edba74-952c-4bbb-8bd0-34ebe7c744f8 node DatanodeRegistration(10.10.1.2:9866, datanodeUuid=f0848aeb-442f-4329-ab0e-4f5304ca825c, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-23e35154-5274-4de9-8d07-196c4382bd6c;nsid=1969679814;c=1550781885051), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-02-21 13:58:04,406 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for standby state
2019-02-21 13:58:04,407 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Edit log tailer interrupted
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:479)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
2019-02-21 13:58:04,410 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2019-02-21 13:58:04,419 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Starting recovery process for unclosed journal segments...
2019-02-21 13:58:04,463 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Successfully started new epoch 5
2019-02-21 13:58:04,463 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Beginning recovery of unclosed segment starting at txid 574
2019-02-21 13:58:04,483 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Recovery prepare phase complete. Responses:
10.10.1.3:8485: segmentState { startTxId: 574 endTxId: 574 isInProgress: true } lastWriterEpoch: 4 lastCommittedTxId: 572
10.10.1.2:8485: segmentState { startTxId: 574 endTxId: 574 isInProgress: true } lastWriterEpoch: 4 lastCommittedTxId: 572
10.10.1.5:8485: segmentState { startTxId: 574 endTxId: 574 isInProgress: true } lastWriterEpoch: 4 lastCommittedTxId: 572
2019-02-21 13:58:04,485 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Using longest log: 10.10.1.3:8485=segmentState {
  startTxId: 574
  endTxId: 574
  isInProgress: true
}
lastWriterEpoch: 4
lastCommittedTxId: 572

2019-02-21 13:58:04,526 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-root/dfs/name/current
2019-02-21 13:58:04,554 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000000573 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000000573-0000000000000000573
2019-02-21 13:58:04,570 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Catching up to latest edits from old active before taking over writer role in edits logs
2019-02-21 13:58:04,575 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3782187 expecting start txid #574
2019-02-21 13:58:04,575 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=574&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=574&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 13:58:04,576 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=574&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=574&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 574
2019-02-21 13:58:04,576 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=574&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 574
2019-02-21 13:58:04,583 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=574&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=574&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-21 13:58:04,583 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Marking all datanodes as stale
2019-02-21 13:58:04,584 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Reprocessing replication and invalidation queues
2019-02-21 13:58:04,584 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: initializing replication queues
2019-02-21 13:58:04,585 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Will take over writing edit logs at txnid 575
2019-02-21 13:58:04,587 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 575
2019-02-21 13:58:04,869 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Initializing quota with 4 thread(s)
2019-02-21 13:58:04,873 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Quota initialization completed in 4 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2019-02-21 13:58:04,878 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2019-02-21 13:58:04,878 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-02-21 13:58:04,878 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2019-02-21 13:58:04,878 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2019-02-21 13:58:04,878 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2019-02-21 13:58:04,878 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2019-02-21 13:58:04,878 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 294 msec
2019-02-21 13:59:07,542 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 6 Number of transactions batched in Syncs: 574 Number of syncs: 1 SyncTimes(ms): 13 80 
2019-02-21 13:59:07,612 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741985_1161, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile14._COPYING_
2019-02-21 13:59:08,161 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741986_1162, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile14._COPYING_
2019-02-21 13:59:08,666 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741987_1163, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile14._COPYING_
2019-02-21 13:59:08,971 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741988_1164, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile6._COPYING_
2019-02-21 13:59:09,064 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741989_1165, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile14._COPYING_
2019-02-21 13:59:09,502 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741990_1166, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile14._COPYING_
2019-02-21 13:59:09,563 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741991_1167, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile6._COPYING_
2019-02-21 13:59:09,847 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741992_1168, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-02-21 13:59:09,977 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741993_1169, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile14._COPYING_
2019-02-21 13:59:10,021 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741994_1170, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile6._COPYING_
2019-02-21 13:59:10,442 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741995_1171, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-02-21 13:59:10,482 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741996_1172, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile14._COPYING_
2019-02-21 13:59:10,566 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741997_1173, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile6._COPYING_
2019-02-21 13:59:10,910 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741998_1174, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-02-21 13:59:10,950 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741999_1175, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile14._COPYING_
2019-02-21 13:59:10,972 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742000_1176, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile19._COPYING_
2019-02-21 13:59:11,068 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742001_1177, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile6._COPYING_
2019-02-21 13:59:11,433 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742002_1178, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-02-21 13:59:11,458 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile14._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2055867961_1
2019-02-21 13:59:11,529 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742003_1179, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile6._COPYING_
2019-02-21 13:59:11,671 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742004_1180, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile19._COPYING_
2019-02-21 13:59:11,949 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742005_1181, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-02-21 13:59:12,030 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742006_1182, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile6._COPYING_
2019-02-21 13:59:12,152 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742007_1183, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile19._COPYING_
2019-02-21 13:59:12,429 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742008_1184, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-02-21 13:59:12,574 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742009_1185, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile6._COPYING_
2019-02-21 13:59:12,662 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742010_1186, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile17._COPYING_
2019-02-21 13:59:12,676 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742011_1187, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile19._COPYING_
2019-02-21 13:59:12,863 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742012_1188, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile9._COPYING_
2019-02-21 13:59:12,904 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742013_1189, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile10._COPYING_
2019-02-21 13:59:12,915 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742014_1190, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile1._COPYING_
2019-02-21 13:59:13,104 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile6._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1836339098_1
2019-02-21 13:59:13,234 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742015_1191, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile19._COPYING_
2019-02-21 13:59:13,455 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742016_1192, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile17._COPYING_
2019-02-21 13:59:13,457 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742017_1193, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-02-21 13:59:13,598 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742018_1194, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile9._COPYING_
2019-02-21 13:59:13,600 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742019_1195, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile10._COPYING_
2019-02-21 13:59:13,919 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742020_1196, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile19._COPYING_
2019-02-21 13:59:14,071 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742021_1197, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile17._COPYING_
2019-02-21 13:59:14,083 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile1._COPYING_ is closed by DFSClient_NONMAPREDUCE_417602226_1
2019-02-21 13:59:14,133 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742022_1198, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile10._COPYING_
2019-02-21 13:59:14,178 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742023_1199, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile9._COPYING_
2019-02-21 13:59:14,454 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742024_1200, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile19._COPYING_
2019-02-21 13:59:14,590 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742025_1201, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile17._COPYING_
2019-02-21 13:59:14,637 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742026_1202, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile10._COPYING_
2019-02-21 13:59:14,722 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742027_1203, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile9._COPYING_
2019-02-21 13:59:14,917 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742028_1204, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile19._COPYING_
2019-02-21 13:59:15,135 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742029_1205, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile17._COPYING_
2019-02-21 13:59:15,152 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742030_1206, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile10._COPYING_
2019-02-21 13:59:15,215 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742031_1207, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-02-21 13:59:15,301 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742032_1208, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile9._COPYING_
2019-02-21 13:59:15,417 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile19._COPYING_ is closed by DFSClient_NONMAPREDUCE_1782864203_1
2019-02-21 13:59:15,653 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742033_1209, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile17._COPYING_
2019-02-21 13:59:15,678 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742034_1210, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile10._COPYING_
2019-02-21 13:59:15,795 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742035_1211, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile9._COPYING_
2019-02-21 13:59:16,037 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742036_1212, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-02-21 13:59:16,057 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742037_1213, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile13._COPYING_
2019-02-21 13:59:16,153 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742038_1214, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile17._COPYING_
2019-02-21 13:59:16,223 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742039_1215, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile10._COPYING_
2019-02-21 13:59:16,301 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742040_1216, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile9._COPYING_
2019-02-21 13:59:16,694 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742041_1217, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile17._COPYING_
2019-02-21 13:59:16,701 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742042_1218, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-02-21 13:59:16,796 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742043_1219, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile10._COPYING_
2019-02-21 13:59:16,816 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742044_1220, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile13._COPYING_
2019-02-21 13:59:16,884 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742045_1221, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile9._COPYING_
2019-02-21 13:59:17,291 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742046_1222, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-02-21 13:59:17,293 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile17._COPYING_ is closed by DFSClient_NONMAPREDUCE_1312279281_1
2019-02-21 13:59:17,388 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile10._COPYING_ is closed by DFSClient_NONMAPREDUCE_1565015334_1
2019-02-21 13:59:17,404 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742047_1223, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-02-21 13:59:17,425 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742048_1224, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile12._COPYING_
2019-02-21 13:59:17,445 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742049_1225, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile13._COPYING_
2019-02-21 13:59:17,535 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile9._COPYING_ is closed by DFSClient_NONMAPREDUCE_-737241898_1
2019-02-21 13:59:17,818 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742050_1226, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-02-21 13:59:17,961 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742051_1227, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile13._COPYING_
2019-02-21 13:59:18,131 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742052_1228, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-02-21 13:59:18,251 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742053_1229, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile12._COPYING_
2019-02-21 13:59:18,366 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742054_1230, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-02-21 13:59:18,516 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742055_1231, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile13._COPYING_
2019-02-21 13:59:18,633 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742056_1232, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-02-21 13:59:18,772 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742057_1233, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile12._COPYING_
2019-02-21 13:59:18,932 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742058_1234, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-02-21 13:59:19,066 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742059_1235, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile13._COPYING_
2019-02-21 13:59:19,209 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742060_1236, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-02-21 13:59:19,351 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742061_1237, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile12._COPYING_
2019-02-21 13:59:19,532 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742062_1238, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-02-21 13:59:19,611 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742063_1239, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile13._COPYING_
2019-02-21 13:59:19,797 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742064_1240, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-02-21 13:59:19,902 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742065_1241, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile12._COPYING_
2019-02-21 13:59:20,062 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile5._COPYING_ is closed by DFSClient_NONMAPREDUCE_166281776_1
2019-02-21 13:59:20,184 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742066_1242, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile13._COPYING_
2019-02-21 13:59:20,316 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742067_1243, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-02-21 13:59:20,477 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742068_1244, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile12._COPYING_
2019-02-21 13:59:20,709 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile13._COPYING_ is closed by DFSClient_NONMAPREDUCE_149950650_1
2019-02-21 13:59:20,828 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742069_1245, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-02-21 13:59:20,951 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742070_1246, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile12._COPYING_
2019-02-21 13:59:21,009 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742071_1247, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile8._COPYING_
2019-02-21 13:59:21,141 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742072_1248, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile18._COPYING_
2019-02-21 13:59:21,280 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742073_1249, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-02-21 13:59:21,498 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742074_1250, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile12._COPYING_
2019-02-21 13:59:21,838 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742075_1251, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile8._COPYING_
2019-02-21 13:59:21,890 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile3._COPYING_ is closed by DFSClient_NONMAPREDUCE_839763946_1
2019-02-21 13:59:21,924 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742076_1252, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile18._COPYING_
2019-02-21 13:59:22,106 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile12._COPYING_ is closed by DFSClient_NONMAPREDUCE_1838758248_1
2019-02-21 13:59:22,369 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742077_1253, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile8._COPYING_
2019-02-21 13:59:22,445 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742078_1254, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile18._COPYING_
2019-02-21 13:59:24,068 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742079_1255, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-02-21 13:59:25,956 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742080_1256, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile16._COPYING_
2019-02-21 13:59:26,743 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742081_1257, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile20._COPYING_
2019-02-21 13:59:26,802 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742082_1258, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile15._COPYING_
2019-02-21 13:59:27,802 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742083_1259, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-02-21 13:59:28,326 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742084_1260, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile8._COPYING_
2019-02-21 13:59:29,932 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742085_1261, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile11._COPYING_
2019-02-21 13:59:30,571 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742086_1262, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile18._COPYING_
2019-02-21 13:59:30,572 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742087_1263, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-02-21 13:59:30,573 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742088_1264, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile20._COPYING_
2019-02-21 13:59:30,573 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742089_1265, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile16._COPYING_
2019-02-21 13:59:30,573 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742090_1266, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile15._COPYING_
2019-02-21 13:59:31,424 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742091_1267, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile20._COPYING_
2019-02-21 13:59:31,492 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742092_1268, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-02-21 13:59:31,512 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742093_1269, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile16._COPYING_
2019-02-21 13:59:31,871 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742094_1270, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile8._COPYING_
2019-02-21 13:59:31,874 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742095_1271, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-02-21 13:59:31,887 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742096_1272, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile15._COPYING_
2019-02-21 13:59:31,891 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742097_1273, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile11._COPYING_
2019-02-21 13:59:31,891 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742098_1274, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile18._COPYING_
2019-02-21 13:59:32,126 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742099_1275, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile20._COPYING_
2019-02-21 13:59:32,363 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742100_1276, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-02-21 13:59:32,403 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742101_1277, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile16._COPYING_
2019-02-21 13:59:32,673 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742102_1278, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile15._COPYING_
2019-02-21 13:59:32,807 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742103_1279, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile8._COPYING_
2019-02-21 13:59:32,964 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742104_1280, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-02-21 13:59:33,000 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742105_1281, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile11._COPYING_
2019-02-21 13:59:33,218 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742106_1282, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile18._COPYING_
2019-02-21 13:59:33,262 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742107_1283, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-02-21 13:59:33,479 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742108_1284, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile15._COPYING_
2019-02-21 13:59:33,693 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742109_1285, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile11._COPYING_
2019-02-21 13:59:33,741 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742110_1286, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile16._COPYING_
2019-02-21 13:59:33,811 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742111_1287, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile20._COPYING_
2019-02-21 13:59:33,953 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742112_1288, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-02-21 13:59:33,969 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742113_1289, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile18._COPYING_
2019-02-21 13:59:36,285 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742114_1290, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile7._COPYING_
2019-02-21 13:59:39,645 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742115_1291, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile8._COPYING_
2019-02-21 13:59:39,646 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742116_1292, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile11._COPYING_
2019-02-21 13:59:39,646 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742117_1293, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-02-21 13:59:39,646 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742118_1294, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile16._COPYING_
2019-02-21 13:59:39,647 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742119_1295, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile18._COPYING_
2019-02-21 13:59:39,647 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742120_1296, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile15._COPYING_
2019-02-21 13:59:39,647 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742121_1297, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-02-21 13:59:39,648 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742122_1298, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile20._COPYING_
2019-02-21 13:59:44,934 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742123_1299, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile11._COPYING_
2019-02-21 13:59:44,976 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742124_1300, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile7._COPYING_
2019-02-21 13:59:45,058 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742125_1301, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile15._COPYING_
2019-02-21 13:59:45,215 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile18._COPYING_ is closed by DFSClient_NONMAPREDUCE_-33027544_1
2019-02-21 13:59:45,227 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742126_1302, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile16._COPYING_
2019-02-21 13:59:45,232 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742127_1303, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-02-21 13:59:45,242 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742128_1304, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile8._COPYING_
2019-02-21 13:59:45,243 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742129_1305, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile20._COPYING_
2019-02-21 13:59:45,330 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742130_1306, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-02-21 13:59:45,636 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742131_1307, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile11._COPYING_
2019-02-21 13:59:45,952 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742132_1308, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile15._COPYING_
2019-02-21 13:59:46,003 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742133_1309, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile7._COPYING_
2019-02-21 13:59:46,077 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile8._COPYING_ is closed by DFSClient_NONMAPREDUCE_1219108409_1
2019-02-21 13:59:46,241 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742134_1310, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-02-21 13:59:46,242 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742135_1311, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-02-21 13:59:46,252 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742136_1312, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile20._COPYING_
2019-02-21 13:59:46,334 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742137_1313, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile11._COPYING_
2019-02-21 13:59:46,344 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742138_1314, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile16._COPYING_
2019-02-21 13:59:48,988 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile15._COPYING_ is closed by DFSClient_NONMAPREDUCE_-233204621_1
2019-02-21 13:59:48,989 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile11._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1409302409_1
2019-02-21 13:59:48,990 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile16._COPYING_ is closed by DFSClient_NONMAPREDUCE_-722021489_1
2019-02-21 13:59:50,284 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742139_1315, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-02-21 13:59:51,126 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742140_1316, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile7._COPYING_
2019-02-21 13:59:51,130 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile20._COPYING_ is closed by DFSClient_NONMAPREDUCE_2031068512_1
2019-02-21 13:59:51,130 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile4._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2049433748_1
2019-02-21 13:59:51,699 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742141_1317, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile7._COPYING_
2019-02-21 13:59:51,720 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile2._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1033073883_1
2019-02-21 13:59:57,411 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742142_1318, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile7._COPYING_
2019-02-21 14:00:15,840 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742143_1319, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile7._COPYING_
2019-02-21 14:00:15,840 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 535 Total time for transactions(ms): 20 Number of transactions batched in Syncs: 807 Number of syncs: 300 SyncTimes(ms): 2405 2069 
2019-02-21 14:00:23,249 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 10.10.1.1
2019-02-21 14:00:23,249 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-02-21 14:00:23,250 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 575, 1110
2019-02-21 14:00:23,274 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 537 Total time for transactions(ms): 20 Number of transactions batched in Syncs: 808 Number of syncs: 303 SyncTimes(ms): 2432 2103 
2019-02-21 14:00:23,278 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000000575 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000000575-0000000000000001111
2019-02-21 14:00:23,278 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1112
2019-02-21 14:00:33,834 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742144_1320, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile7._COPYING_
2019-02-21 14:00:47,567 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile7._COPYING_ is closed by DFSClient_NONMAPREDUCE_896905388_1
2019-02-21 14:02:23,514 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 10.10.1.1
2019-02-21 14:02:23,514 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-02-21 14:02:23,514 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1112, 1117
2019-02-21 14:02:23,514 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 7 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 1 Number of syncs: 5 SyncTimes(ms): 46 100 
2019-02-21 14:02:23,537 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 7 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 1 Number of syncs: 6 SyncTimes(ms): 62 107 
2019-02-21 14:02:23,540 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000001112 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000001112-0000000000000001118
2019-02-21 14:02:23,541 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1119
2019-02-21 14:03:02,380 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742145_1321, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-02-21 14:03:02,397 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742146_1322, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile14._COPYING_
2019-02-21 14:03:02,480 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742147_1323, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile13._COPYING_
2019-02-21 14:03:03,083 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742148_1324, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile14._COPYING_
2019-02-21 14:03:03,150 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742149_1325, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile1._COPYING_
2019-02-21 14:03:03,290 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742150_1326, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile13._COPYING_
2019-02-21 14:03:03,674 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742151_1327, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-02-21 14:03:03,681 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742152_1328, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile14._COPYING_
2019-02-21 14:03:03,810 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742153_1329, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile13._COPYING_
2019-02-21 14:03:04,242 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742154_1330, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile1._COPYING_
2019-02-21 14:03:04,250 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742155_1331, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile14._COPYING_
2019-02-21 14:03:04,450 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742156_1332, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile13._COPYING_
2019-02-21 14:03:04,713 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742157_1333, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-02-21 14:03:04,732 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742158_1334, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile14._COPYING_
2019-02-21 14:03:04,911 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742159_1335, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile13._COPYING_
2019-02-21 14:03:05,168 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742160_1336, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-02-21 14:03:05,203 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742161_1337, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile14._COPYING_
2019-02-21 14:03:05,357 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742162_1338, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile19._COPYING_
2019-02-21 14:03:05,408 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742163_1339, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile13._COPYING_
2019-02-21 14:03:05,689 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742164_1340, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile14._COPYING_
2019-02-21 14:03:05,730 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742165_1341, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-02-21 14:03:05,905 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742166_1342, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile13._COPYING_
2019-02-21 14:03:06,054 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742167_1343, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile19._COPYING_
2019-02-21 14:03:06,256 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742168_1344, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile14._COPYING_
2019-02-21 14:03:06,274 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742169_1345, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile1._COPYING_
2019-02-21 14:03:06,454 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742170_1346, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile13._COPYING_
2019-02-21 14:03:06,656 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742171_1347, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile19._COPYING_
2019-02-21 14:03:06,814 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile1._COPYING_ is closed by DFSClient_NONMAPREDUCE_1295881532_1
2019-02-21 14:03:06,878 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile14._COPYING_ is closed by DFSClient_NONMAPREDUCE_1040555473_1
2019-02-21 14:03:06,990 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile13._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2107084881_1
2019-02-21 14:03:07,136 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742172_1348, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile19._COPYING_
2019-02-21 14:03:07,577 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742173_1349, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile19._COPYING_
2019-02-21 14:03:07,994 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742174_1350, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile19._COPYING_
2019-02-21 14:03:08,340 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742175_1351, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-02-21 14:03:08,390 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742176_1352, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile19._COPYING_
2019-02-21 14:03:08,836 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742177_1353, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile19._COPYING_
2019-02-21 14:03:08,953 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742178_1354, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-02-21 14:03:09,308 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile19._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1084570753_1
2019-02-21 14:03:09,443 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742179_1355, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-02-21 14:03:09,870 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742180_1356, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-02-21 14:03:10,291 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742181_1357, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-02-21 14:03:10,613 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742182_1358, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile6._COPYING_
2019-02-21 14:03:10,703 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742183_1359, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-02-21 14:03:11,163 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742184_1360, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-02-21 14:03:11,297 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742185_1361, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile6._COPYING_
2019-02-21 14:03:11,578 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742186_1362, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-02-21 14:03:11,823 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742187_1363, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile6._COPYING_
2019-02-21 14:03:12,027 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile5._COPYING_ is closed by DFSClient_NONMAPREDUCE_-465383520_1
2019-02-21 14:03:12,311 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742188_1364, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile6._COPYING_
2019-02-21 14:03:12,736 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742189_1365, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile6._COPYING_
2019-02-21 14:03:13,172 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742190_1366, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile6._COPYING_
2019-02-21 14:03:13,659 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742191_1367, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile6._COPYING_
2019-02-21 14:03:14,227 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742192_1368, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile6._COPYING_
2019-02-21 14:03:14,238 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742193_1369, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile10._COPYING_
2019-02-21 14:03:14,411 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742194_1370, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile12._COPYING_
2019-02-21 14:03:14,749 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile6._COPYING_ is closed by DFSClient_NONMAPREDUCE_-416146019_1
2019-02-21 14:03:15,039 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742195_1371, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile10._COPYING_
2019-02-21 14:03:15,221 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742196_1372, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile9._COPYING_
2019-02-21 14:03:15,275 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742197_1373, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile12._COPYING_
2019-02-21 14:03:15,644 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742198_1374, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile10._COPYING_
2019-02-21 14:03:16,296 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742199_1375, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile10._COPYING_
2019-02-21 14:03:16,296 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742200_1376, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile12._COPYING_
2019-02-21 14:03:16,297 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742201_1377, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile9._COPYING_
2019-02-21 14:03:16,951 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742202_1378, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile9._COPYING_
2019-02-21 14:03:17,159 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742203_1379, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile10._COPYING_
2019-02-21 14:03:17,211 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742204_1380, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile12._COPYING_
2019-02-21 14:03:17,761 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742205_1381, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile10._COPYING_
2019-02-21 14:03:17,767 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742206_1382, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile9._COPYING_
2019-02-21 14:03:17,980 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742207_1383, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile12._COPYING_
2019-02-21 14:03:18,251 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742208_1384, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile9._COPYING_
2019-02-21 14:03:18,385 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742209_1385, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile10._COPYING_
2019-02-21 14:03:18,568 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742210_1386, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile12._COPYING_
2019-02-21 14:03:19,117 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742211_1387, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile9._COPYING_
2019-02-21 14:03:19,337 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742212_1388, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile10._COPYING_
2019-02-21 14:03:19,417 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742213_1389, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile12._COPYING_
2019-02-21 14:03:19,730 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742214_1390, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile9._COPYING_
2019-02-21 14:03:19,869 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile10._COPYING_ is closed by DFSClient_NONMAPREDUCE_1516208384_1
2019-02-21 14:03:20,024 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742215_1391, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile12._COPYING_
2019-02-21 14:03:20,267 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742216_1392, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile9._COPYING_
2019-02-21 14:03:20,569 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile12._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1436484090_1
2019-02-21 14:03:20,798 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile9._COPYING_ is closed by DFSClient_NONMAPREDUCE_1625819193_1
2019-02-21 14:03:28,124 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 255 Total time for transactions(ms): 6 Number of transactions batched in Syncs: 89 Number of syncs: 165 SyncTimes(ms): 1300 1147 
2019-02-21 14:03:32,891 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742217_1393, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile18._COPYING_
2019-02-21 14:03:40,718 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742218_1394, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-02-21 14:03:45,504 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742219_1395, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile18._COPYING_
2019-02-21 14:03:50,722 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742220_1396, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile16._COPYING_
2019-02-21 14:03:50,883 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742221_1397, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile20._COPYING_
2019-02-21 14:03:51,495 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742222_1398, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-02-21 14:03:52,376 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742223_1399, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile18._COPYING_
2019-02-21 14:03:52,379 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742224_1400, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-02-21 14:03:54,753 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742225_1401, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-02-21 14:04:04,947 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742226_1402, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile18._COPYING_
2019-02-21 14:04:04,951 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742227_1403, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile16._COPYING_
2019-02-21 14:04:09,914 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742228_1404, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile18._COPYING_
2019-02-21 14:04:09,917 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742229_1405, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-02-21 14:04:09,917 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742230_1406, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile16._COPYING_
2019-02-21 14:04:09,918 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742231_1407, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-02-21 14:04:09,921 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742232_1408, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-02-21 14:04:09,921 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742233_1409, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile20._COPYING_
2019-02-21 14:04:13,310 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742234_1410, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile17._COPYING_
2019-02-21 14:04:17,965 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742235_1411, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile18._COPYING_
2019-02-21 14:04:21,267 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742236_1412, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-02-21 14:04:21,535 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742237_1413, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-02-21 14:04:21,535 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742238_1414, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile16._COPYING_
2019-02-21 14:04:21,535 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742239_1415, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile20._COPYING_
2019-02-21 14:04:21,538 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742240_1416, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-02-21 14:04:23,732 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 10.10.1.1
2019-02-21 14:04:23,732 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-02-21 14:04:23,732 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1119, 1457
2019-02-21 14:04:23,753 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 340 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 133 Number of syncs: 207 SyncTimes(ms): 1635 1401 
2019-02-21 14:04:23,756 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000001119 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000001119-0000000000000001458
2019-02-21 14:04:23,756 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1459
2019-02-21 14:04:29,690 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742241_1417, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile16._COPYING_
2019-02-21 14:04:29,690 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742242_1418, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-02-21 14:04:29,690 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742243_1419, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile18._COPYING_
2019-02-21 14:04:29,691 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742244_1420, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-02-21 14:04:29,696 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742245_1421, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile17._COPYING_
2019-02-21 14:04:34,514 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742246_1422, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile20._COPYING_
2019-02-21 14:04:35,769 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742247_1423, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-02-21 14:04:44,923 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742248_1424, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile15._COPYING_
2019-02-21 14:04:45,932 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742249_1425, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-02-21 14:04:45,933 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742250_1426, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-02-21 14:04:45,933 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742251_1427, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile20._COPYING_
2019-02-21 14:04:49,717 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742252_1428, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile11._COPYING_
2019-02-21 14:04:49,951 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742253_1429, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile8._COPYING_
2019-02-21 14:04:50,345 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742254_1430, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile17._COPYING_
2019-02-21 14:04:50,346 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742255_1431, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile18._COPYING_
2019-02-21 14:04:50,347 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742256_1432, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-02-21 14:04:50,347 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742257_1433, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile16._COPYING_
2019-02-21 14:05:02,920 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742258_1434, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-02-21 14:05:03,025 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742259_1435, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile15._COPYING_
2019-02-21 14:05:03,095 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742260_1436, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-02-21 14:05:03,171 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742261_1437, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile16._COPYING_
2019-02-21 14:05:03,217 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile18._COPYING_ is closed by DFSClient_NONMAPREDUCE_-412372771_1
2019-02-21 14:05:03,236 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742262_1438, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-02-21 14:05:03,257 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742263_1439, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile11._COPYING_
2019-02-21 14:05:03,317 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742264_1440, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile20._COPYING_
2019-02-21 14:05:03,357 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742265_1441, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile17._COPYING_
2019-02-21 14:05:03,431 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742266_1442, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile8._COPYING_
2019-02-21 14:05:03,746 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742267_1443, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile15._COPYING_
2019-02-21 14:05:03,746 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742268_1444, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-02-21 14:05:03,876 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742269_1445, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile16._COPYING_
2019-02-21 14:05:04,194 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742270_1446, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile11._COPYING_
2019-02-21 14:05:04,355 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742271_1447, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-02-21 14:05:04,395 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742272_1448, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile20._COPYING_
2019-02-21 14:05:04,447 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742273_1449, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-02-21 14:05:04,513 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile3._COPYING_ is closed by DFSClient_NONMAPREDUCE_1713856959_1
2019-02-21 14:05:04,531 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742274_1450, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile15._COPYING_
2019-02-21 14:05:04,708 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile16._COPYING_ is closed by DFSClient_NONMAPREDUCE_1777905855_1
2019-02-21 14:05:04,944 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742275_1451, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile17._COPYING_
2019-02-21 14:05:04,950 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742276_1452, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile11._COPYING_
2019-02-21 14:05:05,258 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742277_1453, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-02-21 14:05:05,271 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742278_1454, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile20._COPYING_
2019-02-21 14:05:05,277 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742279_1455, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile8._COPYING_
2019-02-21 14:05:05,574 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742280_1456, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-02-21 14:05:05,660 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742281_1457, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile17._COPYING_
2019-02-21 14:05:06,072 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742282_1458, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile11._COPYING_
2019-02-21 14:05:06,198 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile2._COPYING_ is closed by DFSClient_NONMAPREDUCE_-277419188_1
2019-02-21 14:05:06,216 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile4._COPYING_ is closed by DFSClient_NONMAPREDUCE_1681591258_1
2019-02-21 14:05:06,266 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742283_1459, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile17._COPYING_
2019-02-21 14:05:06,308 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742284_1460, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile15._COPYING_
2019-02-21 14:05:06,313 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742285_1461, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile8._COPYING_
2019-02-21 14:05:07,875 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742286_1462, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile7._COPYING_
2019-02-21 14:05:11,122 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742287_1463, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile11._COPYING_
2019-02-21 14:05:11,122 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742288_1464, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile17._COPYING_
2019-02-21 14:05:11,127 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile20._COPYING_ is closed by DFSClient_NONMAPREDUCE_1357207804_1
2019-02-21 14:05:12,979 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742289_1465, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile8._COPYING_
2019-02-21 14:05:17,080 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742290_1466, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile15._COPYING_
2019-02-21 14:05:17,081 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742291_1467, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile11._COPYING_
2019-02-21 14:05:17,082 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742292_1468, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile7._COPYING_
2019-02-21 14:05:17,083 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile17._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1841128546_1
2019-02-21 14:05:23,325 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742293_1469, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile11._COPYING_
2019-02-21 14:05:23,357 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742294_1470, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile8._COPYING_
2019-02-21 14:05:24,591 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742295_1471, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile15._COPYING_
2019-02-21 14:05:24,591 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 187 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 93 Number of syncs: 92 SyncTimes(ms): 652 480 
2019-02-21 14:05:24,591 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742296_1472, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile7._COPYING_
2019-02-21 14:05:29,848 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742297_1473, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile15._COPYING_
2019-02-21 14:05:29,848 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742298_1474, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile7._COPYING_
2019-02-21 14:05:30,458 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742299_1475, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile8._COPYING_
2019-02-21 14:05:30,460 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile11._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1627097943_1
2019-02-21 14:05:35,229 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742300_1476, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile7._COPYING_
2019-02-21 14:05:35,231 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742301_1477, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile8._COPYING_
2019-02-21 14:05:35,232 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile15._COPYING_ is closed by DFSClient_NONMAPREDUCE_1569235728_1
2019-02-21 14:05:35,940 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742302_1478, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile7._COPYING_
2019-02-21 14:05:35,946 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile8._COPYING_ is closed by DFSClient_NONMAPREDUCE_457939794_1
2019-02-21 14:05:36,504 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742303_1479, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile7._COPYING_
2019-02-21 14:05:36,989 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742304_1480, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile7._COPYING_
2019-02-21 14:05:37,476 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile7._COPYING_ is closed by DFSClient_NONMAPREDUCE_1973028756_1
2019-02-21 14:06:23,925 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 10.10.1.1
2019-02-21 14:06:23,925 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-02-21 14:06:23,925 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1459, 1681
2019-02-21 14:06:23,946 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 224 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 112 Number of syncs: 112 SyncTimes(ms): 833 605 
2019-02-21 14:06:23,949 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000001459 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000001459-0000000000000001682
2019-02-21 14:06:23,949 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1683
2019-02-21 14:07:12,014 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-02-21 14:07:12,014 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-02-21 14:07:12,014 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-02-21 14:07:12,015 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742305_1481, replicas=10.10.1.2:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-02-21 14:07:12,358 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Not enough replicas was chosen. Reason:{NODE_TOO_BUSY=1}
2019-02-21 14:07:12,358 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-02-21 14:07:12,358 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-02-21 14:07:12,358 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-02-21 14:07:12,358 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742306_1482, replicas=10.10.1.2:9866, 10.10.1.5:9866 for /myfile14._COPYING_
2019-02-21 14:07:12,528 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742307_1483, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-02-21 14:07:12,846 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742308_1484, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile14._COPYING_
2019-02-21 14:07:13,056 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742309_1485, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-02-21 14:07:13,381 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742310_1486, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile14._COPYING_
2019-02-21 14:07:13,549 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742311_1487, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-02-21 14:07:13,715 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Not enough replicas was chosen. Reason:{NODE_TOO_BUSY=1}
2019-02-21 14:07:13,715 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-02-21 14:07:13,716 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-02-21 14:07:13,716 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-02-21 14:07:13,716 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742312_1488, replicas=10.10.1.2:9866, 10.10.1.3:9866 for /myfile13._COPYING_
2019-02-21 14:07:13,900 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742313_1489, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile14._COPYING_
2019-02-21 14:07:13,992 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742314_1490, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-02-21 14:07:14,305 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742315_1491, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile13._COPYING_
2019-02-21 14:07:14,330 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742316_1492, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile14._COPYING_
2019-02-21 14:07:14,508 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742317_1493, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-02-21 14:07:14,743 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742318_1494, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile13._COPYING_
2019-02-21 14:07:14,805 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742319_1495, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile14._COPYING_
2019-02-21 14:07:15,011 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742320_1496, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-02-21 14:07:15,200 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742321_1497, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile13._COPYING_
2019-02-21 14:07:15,279 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742322_1498, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile14._COPYING_
2019-02-21 14:07:15,485 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742323_1499, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-02-21 14:07:15,676 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742324_1500, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile13._COPYING_
2019-02-21 14:07:15,791 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742325_1501, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile14._COPYING_
2019-02-21 14:07:15,952 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile5._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1791461964_1
2019-02-21 14:07:16,243 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile14._COPYING_ is closed by DFSClient_NONMAPREDUCE_-489594076_1
2019-02-21 14:07:16,247 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742326_1502, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile13._COPYING_
2019-02-21 14:07:16,741 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742327_1503, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile13._COPYING_
2019-02-21 14:07:17,183 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742328_1504, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile13._COPYING_
2019-02-21 14:07:17,730 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile13._COPYING_ is closed by DFSClient_NONMAPREDUCE_-607879055_1
2019-02-21 14:07:20,823 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742329_1505, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile10._COPYING_
2019-02-21 14:07:21,655 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742330_1506, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile10._COPYING_
2019-02-21 14:07:22,634 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742331_1507, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile10._COPYING_
2019-02-21 14:07:23,147 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742332_1508, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile1._COPYING_
2019-02-21 14:07:23,302 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742333_1509, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile10._COPYING_
2019-02-21 14:07:23,863 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742334_1510, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-02-21 14:07:23,902 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742335_1511, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile10._COPYING_
2019-02-21 14:07:24,410 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742336_1512, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-02-21 14:07:24,410 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 115 Total time for transactions(ms): 4 Number of transactions batched in Syncs: 33 Number of syncs: 80 SyncTimes(ms): 620 770 
2019-02-21 14:07:24,492 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742337_1513, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile10._COPYING_
2019-02-21 14:07:24,912 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742338_1514, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-02-21 14:07:24,998 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742339_1515, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile10._COPYING_
2019-02-21 14:07:25,150 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742340_1516, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile9._COPYING_
2019-02-21 14:07:25,493 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742341_1517, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-02-21 14:07:25,619 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742342_1518, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile10._COPYING_
2019-02-21 14:07:25,922 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742343_1519, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile9._COPYING_
2019-02-21 14:07:26,115 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742344_1520, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-02-21 14:07:26,213 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile10._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1400554646_1
2019-02-21 14:07:26,527 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742345_1521, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile9._COPYING_
2019-02-21 14:07:26,691 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742346_1522, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-02-21 14:07:27,161 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742347_1523, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile9._COPYING_
2019-02-21 14:07:27,175 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742348_1524, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-02-21 14:07:27,814 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile1._COPYING_ is closed by DFSClient_NONMAPREDUCE_1823017139_1
2019-02-21 14:07:27,844 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742349_1525, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile9._COPYING_
2019-02-21 14:07:28,381 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742350_1526, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile9._COPYING_
2019-02-21 14:07:28,912 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742351_1527, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile9._COPYING_
2019-02-21 14:07:29,490 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742352_1528, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile9._COPYING_
2019-02-21 14:07:30,038 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile9._COPYING_ is closed by DFSClient_NONMAPREDUCE_1269605092_1
2019-02-21 14:07:35,097 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742353_1529, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile6._COPYING_
2019-02-21 14:07:35,723 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742354_1530, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile6._COPYING_
2019-02-21 14:07:36,293 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742355_1531, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile19._COPYING_
2019-02-21 14:07:36,374 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742356_1532, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile6._COPYING_
2019-02-21 14:07:36,935 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742357_1533, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile6._COPYING_
2019-02-21 14:07:36,988 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742358_1534, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile19._COPYING_
2019-02-21 14:07:37,452 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742359_1535, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile6._COPYING_
2019-02-21 14:07:37,500 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742360_1536, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile19._COPYING_
2019-02-21 14:07:37,926 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742361_1537, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile6._COPYING_
2019-02-21 14:07:37,980 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742362_1538, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile19._COPYING_
2019-02-21 14:07:38,438 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742363_1539, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile19._COPYING_
2019-02-21 14:07:38,501 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742364_1540, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile6._COPYING_
2019-02-21 14:07:38,877 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742365_1541, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile19._COPYING_
2019-02-21 14:07:38,957 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742366_1542, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile6._COPYING_
2019-02-21 14:07:39,285 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742367_1543, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile19._COPYING_
2019-02-21 14:07:39,402 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile6._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1658342636_1
2019-02-21 14:07:39,684 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742368_1544, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile19._COPYING_
2019-02-21 14:07:40,081 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile19._COPYING_ is closed by DFSClient_NONMAPREDUCE_1871646077_1
2019-02-21 14:07:54,329 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742369_1545, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile12._COPYING_
2019-02-21 14:07:58,126 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742370_1546, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile12._COPYING_
2019-02-21 14:07:58,533 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742371_1547, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile12._COPYING_
2019-02-21 14:07:59,129 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742372_1548, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile12._COPYING_
2019-02-21 14:07:59,686 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742373_1549, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile12._COPYING_
2019-02-21 14:08:00,139 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742374_1550, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile12._COPYING_
2019-02-21 14:08:04,418 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742375_1551, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile12._COPYING_
2019-02-21 14:08:04,864 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742376_1552, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile12._COPYING_
2019-02-21 14:08:05,278 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile12._COPYING_ is closed by DFSClient_NONMAPREDUCE_601734692_1
2019-02-21 14:08:24,135 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 10.10.1.1
2019-02-21 14:08:24,135 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-02-21 14:08:24,135 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1683, 1935
2019-02-21 14:08:24,151 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 254 Total time for transactions(ms): 6 Number of transactions batched in Syncs: 77 Number of syncs: 177 SyncTimes(ms): 1432 1551 
2019-02-21 14:08:24,154 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000001683 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000001683-0000000000000001936
2019-02-21 14:08:24,154 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1937
2019-02-21 14:08:29,299 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2019-02-21 14:08:29,302 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at clnode042.clemson.cloudlab.us/130.127.133.51
************************************************************/
2019-02-21 14:08:46,331 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode042.clemson.cloudlab.us/130.127.133.51
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-02-19T02:28Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-02-21 14:08:46,341 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-21 14:08:46,346 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-02-21 14:08:46,634 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-21 14:08:46,738 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-02-21 14:08:46,738 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-02-21 14:08:46,794 INFO org.apache.hadoop.hdfs.server.namenode.NameNodeUtils: fs.defaultFS is hdfs://mycluster
2019-02-21 14:08:46,795 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients should use mycluster to access this namenode/service.
2019-02-21 14:08:46,958 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-02-21 14:08:46,986 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://node-1-link-0:9870
2019-02-21 14:08:47,003 INFO org.eclipse.jetty.util.log: Logging initialized @1189ms
2019-02-21 14:08:47,109 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-21 14:08:47,123 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-02-21 14:08:47,134 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-21 14:08:47,137 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-02-21 14:08:47,138 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-21 14:08:47,138 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-21 14:08:47,165 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-02-21 14:08:47,165 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-02-21 14:08:47,174 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 9870
2019-02-21 14:08:47,175 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2019-02-21 14:08:47,216 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c137fd5{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-02-21 14:08:47,216 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d9d0818{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-02-21 14:08:47,291 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@376a0d86{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-02-21 14:08:47,311 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@536dbea0{HTTP/1.1,[http/1.1]}{node-1-link-0:9870}
2019-02-21 14:08:47,311 INFO org.eclipse.jetty.server.Server: Started @1499ms
2019-02-21 14:08:47,638 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-21 14:08:47,698 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2019-02-21 14:08:47,712 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2019-02-21 14:08:47,714 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2019-02-21 14:08:47,716 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2019-02-21 14:08:47,723 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-02-21 14:08:47,723 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-02-21 14:08:47,723 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2019-02-21 14:08:47,724 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID: mycluster
2019-02-21 14:08:47,724 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: true
2019-02-21 14:08:47,767 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-02-21 14:08:47,779 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-02-21 14:08:47,780 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-02-21 14:08:47,784 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-02-21 14:08:47,785 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Feb 21 14:08:47
2019-02-21 14:08:47,787 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-02-21 14:08:47,787 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-21 14:08:47,788 INFO org.apache.hadoop.util.GSet: 2.0% max memory 26.7 GB = 546.1 MB
2019-02-21 14:08:47,788 INFO org.apache.hadoop.util.GSet: capacity      = 2^26 = 67108864 entries
2019-02-21 14:08:47,927 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable = false
2019-02-21 14:08:47,936 INFO org.apache.hadoop.conf.Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-02-21 14:08:47,936 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-02-21 14:08:47,936 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2019-02-21 14:08:47,936 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2019-02-21 14:08:47,936 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2019-02-21 14:08:47,936 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-02-21 14:08:47,936 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-02-21 14:08:47,936 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-02-21 14:08:47,936 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2019-02-21 14:08:47,936 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-02-21 14:08:47,937 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-02-21 14:08:48,020 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-02-21 14:08:48,021 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-21 14:08:48,021 INFO org.apache.hadoop.util.GSet: 1.0% max memory 26.7 GB = 273.0 MB
2019-02-21 14:08:48,021 INFO org.apache.hadoop.util.GSet: capacity      = 2^25 = 33554432 entries
2019-02-21 14:08:48,094 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-02-21 14:08:48,094 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: POSIX ACL inheritance enabled? true
2019-02-21 14:08:48,094 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-02-21 14:08:48,095 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2019-02-21 14:08:48,102 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-02-21 14:08:48,104 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: SkipList is disabled
2019-02-21 14:08:48,110 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-02-21 14:08:48,110 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-21 14:08:48,110 INFO org.apache.hadoop.util.GSet: 0.25% max memory 26.7 GB = 68.3 MB
2019-02-21 14:08:48,110 INFO org.apache.hadoop.util.GSet: capacity      = 2^23 = 8388608 entries
2019-02-21 14:08:48,138 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-02-21 14:08:48,138 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-02-21 14:08:48,138 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-02-21 14:08:48,142 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-02-21 14:08:48,142 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-02-21 14:08:48,145 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-02-21 14:08:48,145 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-21 14:08:48,145 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-02-21 14:08:48,145 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-02-21 14:08:48,207 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 101688@clnode042.clemson.cloudlab.us
2019-02-21 14:08:49,687 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1224ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=351ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1002ms
2019-02-21 14:08:49,863 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-02-21 14:08:49,928 INFO org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager: Enable the erasure coding policy RS-6-3-1024k
2019-02-21 14:08:49,930 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-02-21 14:08:49,963 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-02-21 14:08:49,964 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-02-21 14:08:49,969 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1a7288a3 expecting start txid #1
2019-02-21 14:08:49,969 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 14:08:49,973 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:49,973 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,031 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile17._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,043 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile14._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,044 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile10._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,045 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile13._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,049 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile19._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,050 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile1._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,051 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile11._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,051 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile8._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,052 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile6._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,053 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile18._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,053 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile15._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,054 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile16._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,055 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile2._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,056 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile5._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,057 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile9._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,058 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile3._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,058 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile4._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,059 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile12._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,062 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile7._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,062 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile20._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,088 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile7 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,092 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile18 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,093 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile9 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,094 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile4 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,094 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile13 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,095 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile17 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,096 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile15 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,096 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile2 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,097 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile14 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,098 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile19 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,098 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile3 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,099 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile5 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,100 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile10 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,100 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile12 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,101 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile1 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,102 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile8 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,103 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile16 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,103 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile11 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,104 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile20 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,109 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile6 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,110 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 1048576 edits # 541 loaded in 0 seconds
2019-02-21 14:08:50,110 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@2974f221 expecting start txid #542
2019-02-21 14:08:50,110 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 14:08:50,110 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,110 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,115 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=542&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-21 14:08:50,115 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@58fe0499 expecting start txid #543
2019-02-21 14:08:50,116 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 14:08:50,116 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,116 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,119 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=543&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 42 edits # 2 loaded in 0 seconds
2019-02-21 14:08:50,119 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@686449f9 expecting start txid #545
2019-02-21 14:08:50,119 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 14:08:50,119 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,119 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,139 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 1193 edits # 22 loaded in 0 seconds
2019-02-21 14:08:50,139 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@665df3c6 expecting start txid #567
2019-02-21 14:08:50,139 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=567&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=567&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=567&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 14:08:50,139 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=567&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=567&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=567&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,139 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=567&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,141 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=567&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=567&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=567&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 42 edits # 2 loaded in 0 seconds
2019-02-21 14:08:50,141 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@68b6f0d6 expecting start txid #569
2019-02-21 14:08:50,142 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=569&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=569&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=569&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 14:08:50,142 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=569&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=569&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=569&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,142 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=569&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,143 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=569&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=569&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=569&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 42 edits # 2 loaded in 0 seconds
2019-02-21 14:08:50,143 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4044fb95 expecting start txid #571
2019-02-21 14:08:50,143 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=571&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=571&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=571&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 14:08:50,143 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=571&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=571&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=571&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,144 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=571&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,145 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=571&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=571&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=571&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 42 edits # 2 loaded in 0 seconds
2019-02-21 14:08:50,145 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@aa549e5 expecting start txid #573
2019-02-21 14:08:50,145 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=573&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=573&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=573&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 14:08:50,145 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=573&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=573&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=573&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,145 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=573&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,149 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=573&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=573&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=573&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-21 14:08:50,149 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@36f48b4 expecting start txid #574
2019-02-21 14:08:50,149 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=574&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=574&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=574&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 14:08:50,149 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=574&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=574&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=574&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,149 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=574&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,154 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=574&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=574&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=574&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-21 14:08:50,154 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@5c00384f expecting start txid #575
2019-02-21 14:08:50,154 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=575&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=575&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=575&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 14:08:50,154 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=575&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=575&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=575&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,154 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=575&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,156 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile14._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,157 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile6._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,158 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile1._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,159 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile19._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,161 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile14 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,162 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile17._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,163 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile9._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,164 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile10._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,165 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile6 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,166 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile1 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,168 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile5._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,168 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile19 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,169 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile13._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,171 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile17 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,171 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile3._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,172 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile12._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,172 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile10 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,173 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile9 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,175 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile5 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,176 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile13 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,176 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile8._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,177 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile18._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,178 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile3 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,178 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile12 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,179 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile2._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,179 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile16._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,180 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile20._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,181 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile15._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,181 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile4._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,182 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile11._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,185 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile7._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,187 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile18 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,187 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile8 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,189 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile15 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,189 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile11 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,189 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile16 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,190 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile20 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,190 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile4 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,191 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile2 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,191 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=575&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=575&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=575&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 29827 edits # 537 loaded in 0 seconds
2019-02-21 14:08:50,191 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3b7ff809 expecting start txid #1112
2019-02-21 14:08:50,191 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1112&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1112&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1112&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 14:08:50,192 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1112&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1112&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1112&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,192 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1112&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,194 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile7 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,194 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1112&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1112&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1112&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 528 edits # 7 loaded in 0 seconds
2019-02-21 14:08:50,194 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1bb564e2 expecting start txid #1119
2019-02-21 14:08:50,194 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1119&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1119&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1119&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 14:08:50,194 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1119&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1119&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1119&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,194 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1119&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,197 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile1._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,198 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile14._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,198 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile13._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,200 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile19._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,201 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile1 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,202 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile14 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,202 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile13 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,203 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile5._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,203 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile19 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,204 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile6._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,205 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile5 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,205 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile10._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,206 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile12._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,207 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile6 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,207 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile9._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,209 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile10 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,210 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile12 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,210 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile9 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,211 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile18._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,212 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile3._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,213 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile16._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,213 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile20._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,214 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile2._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,214 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile4._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,216 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile17._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,217 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1119&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1119&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1119&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 18655 edits # 340 loaded in 0 seconds
2019-02-21 14:08:50,217 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@62e6b5c8 expecting start txid #1459
2019-02-21 14:08:50,217 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1459&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1459&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1459&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 14:08:50,217 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1459&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1459&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1459&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,217 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1459&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,220 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile15._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,221 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile11._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,221 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile8._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,223 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile18 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,224 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile3 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,225 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile16 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,226 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile2 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,226 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile4 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,226 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile7._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,227 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile20 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,228 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile17 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,229 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile11 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,229 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile15 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,230 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile8 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,231 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile7 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,231 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1459&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1459&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1459&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 12856 edits # 224 loaded in 0 seconds
2019-02-21 14:08:50,231 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3f792b9b expecting start txid #1683
2019-02-21 14:08:50,231 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1683&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1683&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1683&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 14:08:50,231 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1683&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1683&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1683&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,231 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1683&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,235 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile5._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,235 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile14._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,236 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile13._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,237 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile5 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,238 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile14 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,238 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile13 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,239 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile10._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,240 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile1._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,241 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile9._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,242 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile10 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,243 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile1 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,243 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile9 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,243 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile6._COPYING_ in directory / is exceeded: limit=1 length=17
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,244 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile19._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,245 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile6 in directory / is exceeded: limit=1 length=7
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,245 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile19 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,246 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile12._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,247 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: ERROR in FSDirectory.verifyINodeName
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile12 in directory / is exceeded: limit=1 length=8
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.verifyFsLimitsForRename(FSDirRenameOp.java:98)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameForEditLog(FSDirRenameOp.java:119)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:587)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:745)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1086)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2019-02-21 14:08:50,247 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1683&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1683&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1683&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 14185 edits # 254 loaded in 0 seconds
2019-02-21 14:08:50,247 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@7b8233cd expecting start txid #1937
2019-02-21 14:08:50,247 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1937&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1937&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1937&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 14:08:50,247 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1937&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1937&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1937&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,247 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1937&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1
2019-02-21 14:08:50,251 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1937&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1937&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1937&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-21 14:08:50,251 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-02-21 14:08:50,251 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-02-21 14:08:50,251 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 2099 msecs
2019-02-21 14:08:50,433 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to node-1-link-0:8020
2019-02-21 14:08:50,438 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-02-21 14:08:50,450 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2019-02-21 14:08:50,640 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-02-21 14:08:50,648 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2019-02-21 14:08:50,659 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2019-02-21 14:08:50,688 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-21 14:08:50,689 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2019-02-21 14:08:50,691 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: node-1-link-0/10.10.1.4:8020
2019-02-21 14:08:50,694 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for standby state
2019-02-21 14:08:50,699 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Will roll logs on active node every 120 seconds.
2019-02-21 14:08:50,706 INFO org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-0-link-0:9870]
Serving checkpoints at http://node-1-link-0:9870
2019-02-21 14:08:50,892 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=ace0929e-b1d4-4c28-bb36-3a8207830786, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-23e35154-5274-4de9-8d07-196c4382bd6c;nsid=1969679814;c=1550781885051) storage ace0929e-b1d4-4c28-bb36-3a8207830786
2019-02-21 14:08:50,893 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.3:9866
2019-02-21 14:08:50,893 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN ace0929e-b1d4-4c28-bb36-3a8207830786 (10.10.1.3:9866).
2019-02-21 14:08:50,895 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.5:9866, datanodeUuid=bbbd9e3d-a9c1-412c-8eea-c0eaa8156b8f, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-23e35154-5274-4de9-8d07-196c4382bd6c;nsid=1969679814;c=1550781885051) storage bbbd9e3d-a9c1-412c-8eea-c0eaa8156b8f
2019-02-21 14:08:50,895 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.5:9866
2019-02-21 14:08:50,895 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN bbbd9e3d-a9c1-412c-8eea-c0eaa8156b8f (10.10.1.5:9866).
2019-02-21 14:08:50,895 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=f0848aeb-442f-4329-ab0e-4f5304ca825c, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-23e35154-5274-4de9-8d07-196c4382bd6c;nsid=1969679814;c=1550781885051) storage f0848aeb-442f-4329-ab0e-4f5304ca825c
2019-02-21 14:08:50,895 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.2:9866
2019-02-21 14:08:50,895 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN f0848aeb-442f-4329-ab0e-4f5304ca825c (10.10.1.2:9866).
2019-02-21 14:08:50,906 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-b3edba74-952c-4bbb-8bd0-34ebe7c744f8 for DN 10.10.1.2:9866
2019-02-21 14:08:50,909 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-d7ea203f-77fb-4e5b-ae28-c6a58b3e0edc for DN 10.10.1.5:9866
2019-02-21 14:08:50,909 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-c5c2b850-4fc6-431b-b274-5d2834f0b52c for DN 10.10.1.3:9866
2019-02-21 14:08:50,924 INFO BlockStateChange: BLOCK* processReport 0x2ff0023e1a36ea44: Processing first storage report for DS-b3edba74-952c-4bbb-8bd0-34ebe7c744f8 from datanode f0848aeb-442f-4329-ab0e-4f5304ca825c
2019-02-21 14:08:50,934 INFO BlockStateChange: BLOCK* processReport 0x2ff0023e1a36ea44: from storage DS-b3edba74-952c-4bbb-8bd0-34ebe7c744f8 node DatanodeRegistration(10.10.1.2:9866, datanodeUuid=f0848aeb-442f-4329-ab0e-4f5304ca825c, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-23e35154-5274-4de9-8d07-196c4382bd6c;nsid=1969679814;c=1550781885051), blocks: 153, hasStaleStorage: false, processing time: 10 msecs, invalidatedBlocks: 0
2019-02-21 14:08:50,934 INFO BlockStateChange: BLOCK* processReport 0x8958ecc37eacf76b: Processing first storage report for DS-c5c2b850-4fc6-431b-b274-5d2834f0b52c from datanode ace0929e-b1d4-4c28-bb36-3a8207830786
2019-02-21 14:08:50,937 INFO BlockStateChange: BLOCK* processReport 0x8958ecc37eacf76b: from storage DS-c5c2b850-4fc6-431b-b274-5d2834f0b52c node DatanodeRegistration(10.10.1.3:9866, datanodeUuid=ace0929e-b1d4-4c28-bb36-3a8207830786, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-23e35154-5274-4de9-8d07-196c4382bd6c;nsid=1969679814;c=1550781885051), blocks: 153, hasStaleStorage: false, processing time: 4 msecs, invalidatedBlocks: 0
2019-02-21 14:08:50,937 INFO BlockStateChange: BLOCK* processReport 0xdf611fe4e7a4d6b6: Processing first storage report for DS-d7ea203f-77fb-4e5b-ae28-c6a58b3e0edc from datanode bbbd9e3d-a9c1-412c-8eea-c0eaa8156b8f
2019-02-21 14:08:50,940 INFO BlockStateChange: BLOCK* processReport 0xdf611fe4e7a4d6b6: from storage DS-d7ea203f-77fb-4e5b-ae28-c6a58b3e0edc node DatanodeRegistration(10.10.1.5:9866, datanodeUuid=bbbd9e3d-a9c1-412c-8eea-c0eaa8156b8f, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-23e35154-5274-4de9-8d07-196c4382bd6c;nsid=1969679814;c=1550781885051), blocks: 153, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0
2019-02-21 14:09:08,132 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for standby state
2019-02-21 14:09:08,133 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Edit log tailer interrupted
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:479)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
2019-02-21 14:09:08,135 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2019-02-21 14:09:08,142 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Starting recovery process for unclosed journal segments...
2019-02-21 14:09:08,185 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Successfully started new epoch 7
2019-02-21 14:09:08,185 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Beginning recovery of unclosed segment starting at txid 1938
2019-02-21 14:09:08,207 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Recovery prepare phase complete. Responses:
10.10.1.5:8485: segmentState { startTxId: 1938 endTxId: 1971 isInProgress: true } lastWriterEpoch: 6 lastCommittedTxId: 1970
10.10.1.3:8485: segmentState { startTxId: 1938 endTxId: 1971 isInProgress: true } lastWriterEpoch: 6 lastCommittedTxId: 1970
2019-02-21 14:09:08,209 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Using longest log: 10.10.1.5:8485=segmentState {
  startTxId: 1938
  endTxId: 1971
  isInProgress: true
}
lastWriterEpoch: 6
lastCommittedTxId: 1970

2019-02-21 14:09:08,251 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-root/dfs/name/current
2019-02-21 14:09:08,277 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000001937 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000001937-0000000000000001937
2019-02-21 14:09:08,293 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Catching up to latest edits from old active before taking over writer role in edits logs
2019-02-21 14:09:08,298 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@7cc607ac expecting start txid #1938
2019-02-21 14:09:08,298 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1938&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1938&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-21 14:09:08,298 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1938&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1938&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1938
2019-02-21 14:09:08,298 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1938&storageInfo=-64%3A1969679814%3A1550781885051%3ACID-23e35154-5274-4de9-8d07-196c4382bd6c&inProgressOk=true' to transaction ID 1938
2019-02-21 14:09:08,303 WARN org.apache.hadoop.hdfs.StateChange: DIR* FSDirectory.unprotectedAddFile: exception when add / to the file system
org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException: The maximum path component name limit of myfile16._COPYING_ in directory / is exceeded: limit=1 length=18
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1184)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1286)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1111)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog(FSDirWriteFileOp.java:452)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:320)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$1.run(EditLogTailer.java:271)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$1.run(EditLogTailer.java:265)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.security.SecurityUtil.doAsUser(SecurityUtil.java:515)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUser(SecurityUtil.java:496)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.catchupDuringFailover(EditLogTailer.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1216)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1888)
	at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)
	at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:64)
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1746)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:1723)
	at org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)
	at org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:4460)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
2019-02-21 14:09:08,304 ERROR org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: Encountered exception on operation AddOp [length=0, inodeId=16455, path=/myfile16._COPYING_, replication=3, mtime=1550783329588, atime=1550783329588, blockSize=134217728, blocks=[], permissions=root:supergroup:rw-r--r--, aclEntries=null, clientName=DFSClient_NONMAPREDUCE_833799401_1, clientMachine=10.10.1.6, overwrite=true, RpcClientId=e0ad91dd-d3c5-4fca-909e-c2de982e449d, RpcCallId=3, storagePolicyId=0, erasureCodingPolicyId=0, opCode=OP_ADD, txid=1940]
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:406)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:320)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$1.run(EditLogTailer.java:271)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$1.run(EditLogTailer.java:265)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.security.SecurityUtil.doAsUser(SecurityUtil.java:515)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUser(SecurityUtil.java:496)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.catchupDuringFailover(EditLogTailer.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1216)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1888)
	at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)
	at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:64)
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1746)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:1723)
	at org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)
	at org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:4460)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
2019-02-21 14:09:08,305 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 152 has reached the threshold 0.9990 of total blocks 152. The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 30 seconds.
2019-02-21 14:09:08,305 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Error encountered requiring NN shutdown. Shutting down immediately.
java.io.IOException: java.lang.IllegalStateException: Cannot skip to less than the current value (=16455), where newValue=16454
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.resetLastInodeId(FSDirectory.java:1945)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:309)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:160)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:890)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:320)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$1.run(EditLogTailer.java:271)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$1.run(EditLogTailer.java:265)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.security.SecurityUtil.doAsUser(SecurityUtil.java:515)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUser(SecurityUtil.java:496)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.catchupDuringFailover(EditLogTailer.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1216)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1888)
	at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)
	at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:64)
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1746)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:1723)
	at org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)
	at org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:4460)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: java.lang.IllegalStateException: Cannot skip to less than the current value (=16455), where newValue=16454
	at org.apache.hadoop.util.SequentialNumber.skipTo(SequentialNumber.java:58)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.resetLastInodeId(FSDirectory.java:1943)
	... 29 more
2019-02-21 14:09:08,306 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1: java.io.IOException: java.lang.IllegalStateException: Cannot skip to less than the current value (=16455), where newValue=16454
2019-02-21 14:09:08,308 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at clnode042.clemson.cloudlab.us/130.127.133.51
************************************************************/
