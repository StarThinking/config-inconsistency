2019-02-11 20:41:55,691 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode024.clemson.cloudlab.us/130.127.133.33
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-02-05T02:43Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-02-11 20:41:55,701 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-11 20:41:55,706 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-02-11 20:41:55,992 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-11 20:41:56,097 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-02-11 20:41:56,097 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-02-11 20:41:56,148 INFO org.apache.hadoop.hdfs.server.namenode.NameNodeUtils: fs.defaultFS is hdfs://mycluster
2019-02-11 20:41:56,148 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients should use mycluster to access this namenode/service.
2019-02-11 20:41:56,308 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-02-11 20:41:56,337 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://node-1-link-0:9870
2019-02-11 20:41:56,355 INFO org.eclipse.jetty.util.log: Logging initialized @1167ms
2019-02-11 20:41:56,463 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-11 20:41:56,478 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-02-11 20:41:56,488 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-11 20:41:56,491 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-02-11 20:41:56,491 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-11 20:41:56,491 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-11 20:41:56,518 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-02-11 20:41:56,518 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-02-11 20:41:56,527 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 9870
2019-02-11 20:41:56,529 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2019-02-11 20:41:56,564 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c137fd5{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-02-11 20:41:56,565 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d9d0818{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-02-11 20:41:56,641 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@376a0d86{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-02-11 20:41:56,661 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@536dbea0{HTTP/1.1,[http/1.1]}{node-1-link-0:9870}
2019-02-11 20:41:56,661 INFO org.eclipse.jetty.server.Server: Started @1476ms
2019-02-11 20:41:57,009 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-11 20:41:57,060 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2019-02-11 20:41:57,074 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2019-02-11 20:41:57,075 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2019-02-11 20:41:57,078 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2019-02-11 20:41:57,085 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-02-11 20:41:57,085 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-02-11 20:41:57,085 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2019-02-11 20:41:57,085 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID: mycluster
2019-02-11 20:41:57,085 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: true
2019-02-11 20:41:57,128 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-02-11 20:41:57,140 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-02-11 20:41:57,140 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-02-11 20:41:57,145 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-02-11 20:41:57,145 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Feb 11 20:41:57
2019-02-11 20:41:57,147 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-02-11 20:41:57,147 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 20:41:57,149 INFO org.apache.hadoop.util.GSet: 2.0% max memory 26.7 GB = 546.1 MB
2019-02-11 20:41:57,149 INFO org.apache.hadoop.util.GSet: capacity      = 2^26 = 67108864 entries
2019-02-11 20:41:57,282 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable = false
2019-02-11 20:41:57,291 INFO org.apache.hadoop.conf.Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-02-11 20:41:57,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-02-11 20:41:57,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2019-02-11 20:41:57,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2019-02-11 20:41:57,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2019-02-11 20:41:57,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-02-11 20:41:57,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-02-11 20:41:57,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-02-11 20:41:57,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2019-02-11 20:41:57,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-02-11 20:41:57,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-02-11 20:41:57,375 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-02-11 20:41:57,375 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 20:41:57,375 INFO org.apache.hadoop.util.GSet: 1.0% max memory 26.7 GB = 273.0 MB
2019-02-11 20:41:57,375 INFO org.apache.hadoop.util.GSet: capacity      = 2^25 = 33554432 entries
2019-02-11 20:41:57,449 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-02-11 20:41:57,449 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: POSIX ACL inheritance enabled? true
2019-02-11 20:41:57,449 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-02-11 20:41:57,449 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2019-02-11 20:41:57,456 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-02-11 20:41:57,458 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: SkipList is disabled
2019-02-11 20:41:57,463 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-02-11 20:41:57,464 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 20:41:57,464 INFO org.apache.hadoop.util.GSet: 0.25% max memory 26.7 GB = 68.3 MB
2019-02-11 20:41:57,464 INFO org.apache.hadoop.util.GSet: capacity      = 2^23 = 8388608 entries
2019-02-11 20:41:57,491 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-02-11 20:41:57,491 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-02-11 20:41:57,491 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-02-11 20:41:57,496 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-02-11 20:41:57,496 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-02-11 20:41:57,498 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-02-11 20:41:57,498 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 20:41:57,498 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-02-11 20:41:57,498 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-02-11 20:41:57,521 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 12394@clnode024.clemson.cloudlab.us
2019-02-11 20:41:59,810 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-11 20:41:59,810 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-11 20:41:59,810 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-4-link-0/10.10.1.2:8485. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-11 20:42:00,811 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-11 20:42:00,811 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-11 20:42:00,811 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-4-link-0/10.10.1.2:8485. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-11 20:42:01,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-11 20:42:01,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-4-link-0/10.10.1.2:8485. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-11 20:42:01,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-11 20:42:02,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-11 20:42:02,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-11 20:42:02,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-4-link-0/10.10.1.2:8485. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-11 20:42:03,626 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Waited 6002 ms (timeout=20000 ms) for a response for selectInputStreams. No responses yet.
2019-02-11 20:42:03,813 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-11 20:42:03,813 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-11 20:42:03,813 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node-4-link-0/10.10.1.2:8485. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-02-11 20:42:04,401 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2019-02-11 20:42:04,401 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-02-11 20:42:04,465 INFO org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager: Enable the erasure coding policy RS-6-3-1024k
2019-02-11 20:42:04,467 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-02-11 20:42:04,498 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-02-11 20:42:04,498 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-02-11 20:42:04,503 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-02-11 20:42:04,504 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-02-11 20:42:04,504 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 6998 msecs
2019-02-11 20:42:04,683 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to node-1-link-0:8020
2019-02-11 20:42:04,688 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-02-11 20:42:04,700 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2019-02-11 20:42:04,885 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-02-11 20:42:04,894 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2019-02-11 20:42:04,904 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2019-02-11 20:42:04,904 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2019-02-11 20:42:04,904 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2019-02-11 20:42:04,938 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-11 20:42:04,938 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2019-02-11 20:42:04,942 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: node-1-link-0/10.10.1.4:8020
2019-02-11 20:42:04,945 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for standby state
2019-02-11 20:42:04,950 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Will roll logs on active node every 120 seconds.
2019-02-11 20:42:04,956 INFO org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-0-link-0:9870]
Serving checkpoints at http://node-1-link-0:9870
2019-02-11 20:42:05,446 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471) storage ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9
2019-02-11 20:42:05,448 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.2:9866
2019-02-11 20:42:05,449 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9 (10.10.1.2:9866).
2019-02-11 20:42:05,534 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-4b7709e7-36c2-44bc-88d1-dad941c45a98 for DN 10.10.1.2:9866
2019-02-11 20:42:06,133 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=d66d91c4-7ca3-4cf6-836a-6150d649a368, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471) storage d66d91c4-7ca3-4cf6-836a-6150d649a368
2019-02-11 20:42:06,134 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.3:9866
2019-02-11 20:42:06,135 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN d66d91c4-7ca3-4cf6-836a-6150d649a368 (10.10.1.3:9866).
2019-02-11 20:42:06,135 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.5:9866, datanodeUuid=008b8be5-6168-40fe-8b3e-5a79eadd8b91, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471) storage 008b8be5-6168-40fe-8b3e-5a79eadd8b91
2019-02-11 20:42:06,136 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.5:9866
2019-02-11 20:42:06,136 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 008b8be5-6168-40fe-8b3e-5a79eadd8b91 (10.10.1.5:9866).
2019-02-11 20:42:06,138 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-fb2079a0-a561-41d3-a48a-27ad3ed0c749 for DN 10.10.1.3:9866
2019-02-11 20:42:06,140 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-1f221937-520c-4c45-bfe8-cacf6a0d23c9 for DN 10.10.1.5:9866
2019-02-11 20:42:06,160 INFO BlockStateChange: BLOCK* processReport 0x49bd66b2c05e192f: Processing first storage report for DS-4b7709e7-36c2-44bc-88d1-dad941c45a98 from datanode ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9
2019-02-11 20:42:06,162 INFO BlockStateChange: BLOCK* processReport 0x49bd66b2c05e192f: from storage DS-4b7709e7-36c2-44bc-88d1-dad941c45a98 node DatanodeRegistration(10.10.1.2:9866, datanodeUuid=ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471), blocks: 0, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0
2019-02-11 20:42:06,162 INFO BlockStateChange: BLOCK* processReport 0x30da62cabdb53944: Processing first storage report for DS-1f221937-520c-4c45-bfe8-cacf6a0d23c9 from datanode 008b8be5-6168-40fe-8b3e-5a79eadd8b91
2019-02-11 20:42:06,162 INFO BlockStateChange: BLOCK* processReport 0x30da62cabdb53944: from storage DS-1f221937-520c-4c45-bfe8-cacf6a0d23c9 node DatanodeRegistration(10.10.1.5:9866, datanodeUuid=008b8be5-6168-40fe-8b3e-5a79eadd8b91, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-02-11 20:42:06,162 INFO BlockStateChange: BLOCK* processReport 0xffffd5969f308132: Processing first storage report for DS-fb2079a0-a561-41d3-a48a-27ad3ed0c749 from datanode d66d91c4-7ca3-4cf6-836a-6150d649a368
2019-02-11 20:42:06,162 INFO BlockStateChange: BLOCK* processReport 0xffffd5969f308132: from storage DS-fb2079a0-a561-41d3-a48a-27ad3ed0c749 node DatanodeRegistration(10.10.1.3:9866, datanodeUuid=d66d91c4-7ca3-4cf6-836a-6150d649a368, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-02-11 20:43:13,481 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for standby state
2019-02-11 20:43:13,482 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Edit log tailer interrupted
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:479)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
2019-02-11 20:43:13,585 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2019-02-11 20:43:13,594 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Starting recovery process for unclosed journal segments...
2019-02-11 20:43:13,668 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Successfully started new epoch 2
2019-02-11 20:43:13,668 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Beginning recovery of unclosed segment starting at txid 1
2019-02-11 20:43:13,700 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Recovery prepare phase complete. Responses:
10.10.1.5:8485: segmentState { startTxId: 1 endTxId: 511 isInProgress: true } lastWriterEpoch: 1 lastCommittedTxId: 510
10.10.1.2:8485: segmentState { startTxId: 1 endTxId: 511 isInProgress: true } lastWriterEpoch: 1 lastCommittedTxId: 510
2019-02-11 20:43:13,703 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Using longest log: 10.10.1.5:8485=segmentState {
  startTxId: 1
  endTxId: 511
  isInProgress: true
}
lastWriterEpoch: 1
lastCommittedTxId: 510

2019-02-11 20:43:13,779 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-root/dfs/name/current
2019-02-11 20:43:13,779 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Catching up to latest edits from old active before taking over writer role in edits logs
2019-02-11 20:43:13,795 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@504536f1 expecting start txid #1
2019-02-11 20:43:13,795 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 20:43:13,798 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 20:43:13,798 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 20:43:14,011 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 511 loaded in 0 seconds
2019-02-11 20:43:14,011 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Marking all datanodes as stale
2019-02-11 20:43:14,012 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Reprocessing replication and invalidation queues
2019-02-11 20:43:14,012 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: initializing replication queues
2019-02-11 20:43:14,012 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Will take over writing edit logs at txnid 512
2019-02-11 20:43:14,015 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 512
2019-02-11 20:43:14,288 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 0 SyncTimes(ms): 0 86 
2019-02-11 20:43:14,307 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Initializing quota with 4 thread(s)
2019-02-11 20:43:14,311 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Quota initialization completed in 4 milliseconds
name space=11
storage space=64424509440
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2019-02-11 20:43:14,316 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-02-11 20:43:14,379 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 160
2019-02-11 20:43:14,379 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2019-02-11 20:43:14,379 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2019-02-11 20:43:14,380 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2019-02-11 20:43:14,380 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2019-02-11 20:43:14,380 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 367 msec
2019-02-11 20:43:47,712 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2019-02-11 20:43:47,713 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at clnode024.clemson.cloudlab.us/130.127.133.33
************************************************************/
2019-02-11 20:44:04,751 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode024.clemson.cloudlab.us/130.127.133.33
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-02-05T02:43Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-02-11 20:44:04,761 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-11 20:44:04,766 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-02-11 20:44:05,056 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-11 20:44:05,162 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-02-11 20:44:05,162 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-02-11 20:44:05,214 INFO org.apache.hadoop.hdfs.server.namenode.NameNodeUtils: fs.defaultFS is hdfs://mycluster
2019-02-11 20:44:05,214 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients should use mycluster to access this namenode/service.
2019-02-11 20:44:05,381 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-02-11 20:44:05,409 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://node-1-link-0:9870
2019-02-11 20:44:05,426 INFO org.eclipse.jetty.util.log: Logging initialized @1194ms
2019-02-11 20:44:05,534 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-11 20:44:05,548 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-02-11 20:44:05,559 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-11 20:44:05,562 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-02-11 20:44:05,562 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-11 20:44:05,562 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-11 20:44:05,588 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-02-11 20:44:05,588 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-02-11 20:44:05,598 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 9870
2019-02-11 20:44:05,599 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2019-02-11 20:44:05,634 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c137fd5{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-02-11 20:44:05,635 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d9d0818{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-02-11 20:44:05,710 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@376a0d86{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-02-11 20:44:05,737 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@536dbea0{HTTP/1.1,[http/1.1]}{node-1-link-0:9870}
2019-02-11 20:44:05,737 INFO org.eclipse.jetty.server.Server: Started @1507ms
2019-02-11 20:44:06,086 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-11 20:44:06,144 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2019-02-11 20:44:06,159 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2019-02-11 20:44:06,160 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2019-02-11 20:44:06,163 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2019-02-11 20:44:06,170 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-02-11 20:44:06,170 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-02-11 20:44:06,170 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = false
2019-02-11 20:44:06,170 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID: mycluster
2019-02-11 20:44:06,170 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: true
2019-02-11 20:44:06,213 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-02-11 20:44:06,227 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-02-11 20:44:06,228 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-02-11 20:44:06,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-02-11 20:44:06,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Feb 11 20:44:06
2019-02-11 20:44:06,235 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-02-11 20:44:06,235 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 20:44:06,236 INFO org.apache.hadoop.util.GSet: 2.0% max memory 26.7 GB = 546.1 MB
2019-02-11 20:44:06,236 INFO org.apache.hadoop.util.GSet: capacity      = 2^26 = 67108864 entries
2019-02-11 20:44:06,375 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable = true
2019-02-11 20:44:06,375 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.key.update.interval=600 min(s), dfs.block.access.token.lifetime=600 min(s), dfs.encrypt.data.transfer.algorithm=null
2019-02-11 20:44:06,436 INFO org.apache.hadoop.conf.Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-02-11 20:44:06,436 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-02-11 20:44:06,437 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2019-02-11 20:44:06,437 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2019-02-11 20:44:06,437 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2019-02-11 20:44:06,437 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-02-11 20:44:06,437 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-02-11 20:44:06,437 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-02-11 20:44:06,437 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2019-02-11 20:44:06,437 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-02-11 20:44:06,437 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-02-11 20:44:06,480 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-02-11 20:44:06,480 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 20:44:06,480 INFO org.apache.hadoop.util.GSet: 1.0% max memory 26.7 GB = 273.0 MB
2019-02-11 20:44:06,480 INFO org.apache.hadoop.util.GSet: capacity      = 2^25 = 33554432 entries
2019-02-11 20:44:06,555 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-02-11 20:44:06,555 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: POSIX ACL inheritance enabled? true
2019-02-11 20:44:06,555 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-02-11 20:44:06,555 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2019-02-11 20:44:06,562 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-02-11 20:44:06,565 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: SkipList is disabled
2019-02-11 20:44:06,571 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-02-11 20:44:06,571 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 20:44:06,571 INFO org.apache.hadoop.util.GSet: 0.25% max memory 26.7 GB = 68.3 MB
2019-02-11 20:44:06,571 INFO org.apache.hadoop.util.GSet: capacity      = 2^23 = 8388608 entries
2019-02-11 20:44:06,599 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-02-11 20:44:06,599 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-02-11 20:44:06,599 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-02-11 20:44:06,603 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-02-11 20:44:06,604 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-02-11 20:44:06,606 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-02-11 20:44:06,606 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 20:44:06,606 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-02-11 20:44:06,606 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-02-11 20:44:06,679 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 12837@clnode024.clemson.cloudlab.us
2019-02-11 20:44:08,031 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-02-11 20:44:08,093 INFO org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager: Enable the erasure coding policy RS-6-3-1024k
2019-02-11 20:44:08,095 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-02-11 20:44:08,126 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-02-11 20:44:08,126 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-02-11 20:44:08,131 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1cbf6e72 expecting start txid #1
2019-02-11 20:44:08,131 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 20:44:08,135 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 20:44:08,135 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 20:44:08,255 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 511 loaded in 0 seconds
2019-02-11 20:44:08,255 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@6aecbb8d expecting start txid #512
2019-02-11 20:44:08,256 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 20:44:08,256 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 20:44:08,256 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 20:44:08,262 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 20:44:08,263 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-02-11 20:44:08,263 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-02-11 20:44:08,263 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 1648 msecs
2019-02-11 20:44:08,449 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to node-1-link-0:8020
2019-02-11 20:44:08,453 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-02-11 20:44:08,466 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2019-02-11 20:44:08,651 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-02-11 20:44:08,660 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2019-02-11 20:44:08,668 INFO org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Updating block keys
2019-02-11 20:44:08,671 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2019-02-11 20:44:08,709 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-11 20:44:08,709 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2019-02-11 20:44:08,712 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: node-1-link-0/10.10.1.4:8020
2019-02-11 20:44:08,715 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for standby state
2019-02-11 20:44:08,721 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Will roll logs on active node every 120 seconds.
2019-02-11 20:44:08,728 INFO org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-0-link-0:9870]
Serving checkpoints at http://node-1-link-0:9870
2019-02-11 20:44:09,551 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.5:9866, datanodeUuid=008b8be5-6168-40fe-8b3e-5a79eadd8b91, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471) storage 008b8be5-6168-40fe-8b3e-5a79eadd8b91
2019-02-11 20:44:09,552 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.5:9866
2019-02-11 20:44:09,552 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 008b8be5-6168-40fe-8b3e-5a79eadd8b91 (10.10.1.5:9866).
2019-02-11 20:44:09,554 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=d66d91c4-7ca3-4cf6-836a-6150d649a368, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471) storage d66d91c4-7ca3-4cf6-836a-6150d649a368
2019-02-11 20:44:09,554 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.3:9866
2019-02-11 20:44:09,554 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN d66d91c4-7ca3-4cf6-836a-6150d649a368 (10.10.1.3:9866).
2019-02-11 20:44:09,554 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471) storage ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9
2019-02-11 20:44:09,554 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.2:9866
2019-02-11 20:44:09,554 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9 (10.10.1.2:9866).
2019-02-11 20:44:14,565 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-1f221937-520c-4c45-bfe8-cacf6a0d23c9 for DN 10.10.1.5:9866
2019-02-11 20:44:14,569 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-4b7709e7-36c2-44bc-88d1-dad941c45a98 for DN 10.10.1.2:9866
2019-02-11 20:44:14,569 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-fb2079a0-a561-41d3-a48a-27ad3ed0c749 for DN 10.10.1.3:9866
2019-02-11 20:46:08,740 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 20:46:08,793 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 20:46:38,583 INFO BlockStateChange: BLOCK* processReport 0xffffd5969f308133: Processing first storage report for DS-fb2079a0-a561-41d3-a48a-27ad3ed0c749 from datanode d66d91c4-7ca3-4cf6-836a-6150d649a368
2019-02-11 20:46:38,588 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 1 needs additional 158 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2019-02-11 20:46:38,593 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 159 has reached the threshold 0.9990 of total blocks 160. The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2019-02-11 20:46:38,594 INFO BlockStateChange: BLOCK* processReport 0xffffd5969f308133: from storage DS-fb2079a0-a561-41d3-a48a-27ad3ed0c749 node DatanodeRegistration(10.10.1.3:9866, datanodeUuid=d66d91c4-7ca3-4cf6-836a-6150d649a368, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471), blocks: 160, hasStaleStorage: false, processing time: 11 msecs, invalidatedBlocks: 0
2019-02-11 20:46:58,598 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 160 has reached the threshold 0.9990 of total blocks 160. The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2019-02-11 20:47:08,600 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2019-02-11 20:47:08,600 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 179 secs
2019-02-11 20:47:08,600 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 3 datanodes
2019-02-11 20:47:08,600 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2019-02-11 20:47:08,801 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 20:47:08,804 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 20:48:08,811 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 20:48:08,815 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 20:49:08,821 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 20:49:08,825 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 20:50:08,832 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 20:50:08,836 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 20:51:08,842 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 20:51:08,845 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 20:52:08,854 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 20:52:08,858 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 20:53:08,864 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 20:53:08,867 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 20:54:08,873 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 20:54:08,877 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 20:54:16,933 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for standby state
2019-02-11 20:54:16,934 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Edit log tailer interrupted
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:479)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
2019-02-11 20:54:16,935 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2019-02-11 20:54:16,943 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Starting recovery process for unclosed journal segments...
2019-02-11 20:54:16,986 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Successfully started new epoch 4
2019-02-11 20:54:16,986 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Beginning recovery of unclosed segment starting at txid 513
2019-02-11 20:54:17,009 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Recovery prepare phase complete. Responses:
10.10.1.5:8485: segmentState { startTxId: 513 endTxId: 513 isInProgress: true } lastWriterEpoch: 3 lastCommittedTxId: 510
10.10.1.3:8485: segmentState { startTxId: 513 endTxId: 513 isInProgress: true } lastWriterEpoch: 3 lastCommittedTxId: 510
2019-02-11 20:54:17,011 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Using longest log: 10.10.1.5:8485=segmentState {
  startTxId: 513
  endTxId: 513
  isInProgress: true
}
lastWriterEpoch: 3
lastCommittedTxId: 510

2019-02-11 20:54:17,049 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-root/dfs/name/current
2019-02-11 20:54:17,076 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000000512 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000000512-0000000000000000512
2019-02-11 20:54:17,092 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Catching up to latest edits from old active before taking over writer role in edits logs
2019-02-11 20:54:17,096 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@667b9d76 expecting start txid #513
2019-02-11 20:54:17,096 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 20:54:17,097 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 513
2019-02-11 20:54:17,097 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 513
2019-02-11 20:54:17,104 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 20:54:17,104 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Marking all datanodes as stale
2019-02-11 20:54:17,105 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Reprocessing replication and invalidation queues
2019-02-11 20:54:17,105 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: initializing replication queues
2019-02-11 20:54:17,105 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Will take over writing edit logs at txnid 514
2019-02-11 20:54:17,108 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 514
2019-02-11 20:54:17,347 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 0 SyncTimes(ms): 0 50 
2019-02-11 20:54:17,370 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Initializing quota with 4 thread(s)
2019-02-11 20:54:17,374 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Quota initialization completed in 4 milliseconds
name space=11
storage space=64424509440
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2019-02-11 20:54:17,379 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-02-11 20:54:17,445 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 160
2019-02-11 20:54:17,445 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2019-02-11 20:54:17,445 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 160
2019-02-11 20:54:17,445 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2019-02-11 20:54:17,445 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2019-02-11 20:54:17,446 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 341 msec
2019-02-11 20:54:51,769 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2019-02-11 20:54:51,770 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at clnode024.clemson.cloudlab.us/130.127.133.33
************************************************************/
2019-02-11 20:55:08,798 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode024.clemson.cloudlab.us/130.127.133.33
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-02-05T02:43Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-02-11 20:55:08,808 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-11 20:55:08,813 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-02-11 20:55:09,130 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-11 20:55:09,233 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-02-11 20:55:09,233 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-02-11 20:55:09,286 INFO org.apache.hadoop.hdfs.server.namenode.NameNodeUtils: fs.defaultFS is hdfs://mycluster
2019-02-11 20:55:09,286 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients should use mycluster to access this namenode/service.
2019-02-11 20:55:09,456 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-02-11 20:55:09,483 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://node-1-link-0:9870
2019-02-11 20:55:09,501 INFO org.eclipse.jetty.util.log: Logging initialized @1216ms
2019-02-11 20:55:09,608 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-11 20:55:09,621 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-02-11 20:55:09,632 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-11 20:55:09,635 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-02-11 20:55:09,635 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-11 20:55:09,635 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-11 20:55:09,662 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-02-11 20:55:09,662 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-02-11 20:55:09,671 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 9870
2019-02-11 20:55:09,672 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2019-02-11 20:55:09,708 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c137fd5{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-02-11 20:55:09,708 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d9d0818{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-02-11 20:55:09,784 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@376a0d86{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-02-11 20:55:09,809 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@536dbea0{HTTP/1.1,[http/1.1]}{node-1-link-0:9870}
2019-02-11 20:55:09,809 INFO org.eclipse.jetty.server.Server: Started @1526ms
2019-02-11 20:55:10,138 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-11 20:55:10,188 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2019-02-11 20:55:10,203 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2019-02-11 20:55:10,204 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2019-02-11 20:55:10,207 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2019-02-11 20:55:10,213 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-02-11 20:55:10,213 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-02-11 20:55:10,214 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2019-02-11 20:55:10,214 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID: mycluster
2019-02-11 20:55:10,214 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: true
2019-02-11 20:55:10,257 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-02-11 20:55:10,269 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-02-11 20:55:10,269 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-02-11 20:55:10,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-02-11 20:55:10,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Feb 11 20:55:10
2019-02-11 20:55:10,276 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-02-11 20:55:10,276 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 20:55:10,278 INFO org.apache.hadoop.util.GSet: 2.0% max memory 26.7 GB = 546.1 MB
2019-02-11 20:55:10,278 INFO org.apache.hadoop.util.GSet: capacity      = 2^26 = 67108864 entries
2019-02-11 20:55:10,408 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable = false
2019-02-11 20:55:10,417 INFO org.apache.hadoop.conf.Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-02-11 20:55:10,417 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-02-11 20:55:10,417 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2019-02-11 20:55:10,417 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2019-02-11 20:55:10,417 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2019-02-11 20:55:10,418 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-02-11 20:55:10,418 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-02-11 20:55:10,418 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-02-11 20:55:10,418 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2019-02-11 20:55:10,418 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-02-11 20:55:10,418 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-02-11 20:55:10,501 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-02-11 20:55:10,502 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 20:55:10,502 INFO org.apache.hadoop.util.GSet: 1.0% max memory 26.7 GB = 273.0 MB
2019-02-11 20:55:10,502 INFO org.apache.hadoop.util.GSet: capacity      = 2^25 = 33554432 entries
2019-02-11 20:55:10,576 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-02-11 20:55:10,576 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: POSIX ACL inheritance enabled? true
2019-02-11 20:55:10,576 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-02-11 20:55:10,576 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2019-02-11 20:55:10,583 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-02-11 20:55:10,586 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: SkipList is disabled
2019-02-11 20:55:10,591 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-02-11 20:55:10,591 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 20:55:10,591 INFO org.apache.hadoop.util.GSet: 0.25% max memory 26.7 GB = 68.3 MB
2019-02-11 20:55:10,591 INFO org.apache.hadoop.util.GSet: capacity      = 2^23 = 8388608 entries
2019-02-11 20:55:10,619 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-02-11 20:55:10,619 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-02-11 20:55:10,619 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-02-11 20:55:10,624 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-02-11 20:55:10,624 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-02-11 20:55:10,626 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-02-11 20:55:10,626 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 20:55:10,626 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-02-11 20:55:10,626 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-02-11 20:55:10,689 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 13286@clnode024.clemson.cloudlab.us
2019-02-11 20:55:12,278 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1316ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=362ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1098ms
2019-02-11 20:55:12,449 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-02-11 20:55:12,512 INFO org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager: Enable the erasure coding policy RS-6-3-1024k
2019-02-11 20:55:12,514 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-02-11 20:55:12,545 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-02-11 20:55:12,545 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-02-11 20:55:12,550 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@abf688e expecting start txid #1
2019-02-11 20:55:12,550 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 20:55:12,554 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 20:55:12,554 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 20:55:12,667 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 511 loaded in 0 seconds
2019-02-11 20:55:12,667 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@478ee483 expecting start txid #512
2019-02-11 20:55:12,667 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 20:55:12,668 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 20:55:12,668 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 20:55:12,672 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 20:55:12,672 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1a7288a3 expecting start txid #513
2019-02-11 20:55:12,672 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 20:55:12,672 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 20:55:12,672 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 20:55:12,678 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 20:55:12,678 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@2974f221 expecting start txid #514
2019-02-11 20:55:12,678 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 20:55:12,678 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 20:55:12,678 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 20:55:12,685 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 20:55:12,685 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-02-11 20:55:12,685 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-02-11 20:55:12,685 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 2052 msecs
2019-02-11 20:55:12,870 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to node-1-link-0:8020
2019-02-11 20:55:12,875 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-02-11 20:55:12,887 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2019-02-11 20:55:13,072 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-02-11 20:55:13,081 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2019-02-11 20:55:13,092 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 159 blocks to reach the threshold 0.9990 of total blocks 160.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2019-02-11 20:55:13,123 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-11 20:55:13,123 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2019-02-11 20:55:13,125 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: node-1-link-0/10.10.1.4:8020
2019-02-11 20:55:13,128 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for standby state
2019-02-11 20:55:13,133 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Will roll logs on active node every 120 seconds.
2019-02-11 20:55:13,139 INFO org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-0-link-0:9870]
Serving checkpoints at http://node-1-link-0:9870
2019-02-11 20:55:13,718 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471) storage ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9
2019-02-11 20:55:13,720 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.2:9866
2019-02-11 20:55:13,720 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9 (10.10.1.2:9866).
2019-02-11 20:55:13,722 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.5:9866, datanodeUuid=008b8be5-6168-40fe-8b3e-5a79eadd8b91, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471) storage 008b8be5-6168-40fe-8b3e-5a79eadd8b91
2019-02-11 20:55:13,722 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.5:9866
2019-02-11 20:55:13,722 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 008b8be5-6168-40fe-8b3e-5a79eadd8b91 (10.10.1.5:9866).
2019-02-11 20:55:13,722 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=d66d91c4-7ca3-4cf6-836a-6150d649a368, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471) storage d66d91c4-7ca3-4cf6-836a-6150d649a368
2019-02-11 20:55:13,722 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.3:9866
2019-02-11 20:55:13,722 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN d66d91c4-7ca3-4cf6-836a-6150d649a368 (10.10.1.3:9866).
2019-02-11 20:55:13,732 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-4b7709e7-36c2-44bc-88d1-dad941c45a98 for DN 10.10.1.2:9866
2019-02-11 20:55:13,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-1f221937-520c-4c45-bfe8-cacf6a0d23c9 for DN 10.10.1.5:9866
2019-02-11 20:55:13,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-fb2079a0-a561-41d3-a48a-27ad3ed0c749 for DN 10.10.1.3:9866
2019-02-11 20:55:13,749 INFO BlockStateChange: BLOCK* processReport 0xffffd5969f308134: Processing first storage report for DS-fb2079a0-a561-41d3-a48a-27ad3ed0c749 from datanode d66d91c4-7ca3-4cf6-836a-6150d649a368
2019-02-11 20:55:13,761 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 159 has reached the threshold 0.9990 of total blocks 160. The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 30 seconds.
2019-02-11 20:55:13,761 INFO BlockStateChange: BLOCK* processReport 0xffffd5969f308134: from storage DS-fb2079a0-a561-41d3-a48a-27ad3ed0c749 node DatanodeRegistration(10.10.1.3:9866, datanodeUuid=d66d91c4-7ca3-4cf6-836a-6150d649a368, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471), blocks: 160, hasStaleStorage: false, processing time: 12 msecs, invalidatedBlocks: 0
2019-02-11 20:55:13,762 INFO BlockStateChange: BLOCK* processReport 0x49bd66b2c05e1930: Processing first storage report for DS-4b7709e7-36c2-44bc-88d1-dad941c45a98 from datanode ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9
2019-02-11 20:55:13,764 INFO BlockStateChange: BLOCK* processReport 0x49bd66b2c05e1930: from storage DS-4b7709e7-36c2-44bc-88d1-dad941c45a98 node DatanodeRegistration(10.10.1.2:9866, datanodeUuid=ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471), blocks: 128, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0
2019-02-11 20:55:13,764 INFO BlockStateChange: BLOCK* processReport 0x30da62cabdb53945: Processing first storage report for DS-1f221937-520c-4c45-bfe8-cacf6a0d23c9 from datanode 008b8be5-6168-40fe-8b3e-5a79eadd8b91
2019-02-11 20:55:13,767 INFO BlockStateChange: BLOCK* processReport 0x30da62cabdb53945: from storage DS-1f221937-520c-4c45-bfe8-cacf6a0d23c9 node DatanodeRegistration(10.10.1.5:9866, datanodeUuid=008b8be5-6168-40fe-8b3e-5a79eadd8b91, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471), blocks: 144, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0
2019-02-11 20:55:33,766 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 160 has reached the threshold 0.9990 of total blocks 160. The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2019-02-11 20:55:43,768 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2019-02-11 20:55:43,768 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 30 secs
2019-02-11 20:55:43,768 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 3 datanodes
2019-02-11 20:55:43,768 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2019-02-11 20:57:13,150 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 20:57:13,417 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@13503cad expecting start txid #515
2019-02-11 20:57:13,417 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 20:57:13,418 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 515
2019-02-11 20:57:13,418 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 515
2019-02-11 20:57:13,468 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 22104 edits # 419 loaded in 0 seconds
2019-02-11 20:59:13,477 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 20:59:13,667 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@5b949b2f expecting start txid #934
2019-02-11 20:59:13,668 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 20:59:13,668 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 934
2019-02-11 20:59:13,668 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 934
2019-02-11 20:59:13,677 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 5481 edits # 105 loaded in 0 seconds
2019-02-11 21:01:13,685 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:01:13,864 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@15a0c03f expecting start txid #1039
2019-02-11 21:01:13,864 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:01:13,864 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1039
2019-02-11 21:01:13,864 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1039
2019-02-11 21:01:13,877 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 8402 edits # 160 loaded in 0 seconds
2019-02-11 21:03:13,884 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:03:14,079 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@6c114c6b expecting start txid #1199
2019-02-11 21:03:14,079 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:03:14,079 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1199
2019-02-11 21:03:14,080 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1199
2019-02-11 21:03:14,097 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 13730 edits # 261 loaded in 0 seconds
2019-02-11 21:05:14,104 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:05:14,299 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@68c33f0c expecting start txid #1460
2019-02-11 21:05:14,299 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:05:14,299 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1460
2019-02-11 21:05:14,299 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1460
2019-02-11 21:05:14,313 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 13746 edits # 261 loaded in 0 seconds
2019-02-11 21:05:20,613 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for standby state
2019-02-11 21:05:20,614 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Edit log tailer interrupted
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:479)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
2019-02-11 21:05:20,616 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2019-02-11 21:05:20,625 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Starting recovery process for unclosed journal segments...
2019-02-11 21:05:20,668 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Successfully started new epoch 6
2019-02-11 21:05:20,668 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Beginning recovery of unclosed segment starting at txid 1721
2019-02-11 21:05:20,689 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Recovery prepare phase complete. Responses:
10.10.1.5:8485: segmentState { startTxId: 1721 endTxId: 1721 isInProgress: true } lastWriterEpoch: 5 lastCommittedTxId: 1720
10.10.1.3:8485: segmentState { startTxId: 1721 endTxId: 1721 isInProgress: true } lastWriterEpoch: 5 lastCommittedTxId: 1720
2019-02-11 21:05:20,691 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Using longest log: 10.10.1.5:8485=segmentState {
  startTxId: 1721
  endTxId: 1721
  isInProgress: true
}
lastWriterEpoch: 5
lastCommittedTxId: 1720

2019-02-11 21:05:20,729 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-root/dfs/name/current
2019-02-11 21:05:20,758 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000000514 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000000514-0000000000000000514
2019-02-11 21:05:20,775 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Catching up to latest edits from old active before taking over writer role in edits logs
2019-02-11 21:05:20,779 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4aedcfba expecting start txid #1721
2019-02-11 21:05:20,779 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:05:20,780 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1721
2019-02-11 21:05:20,780 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1721
2019-02-11 21:05:20,786 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:05:20,786 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Marking all datanodes as stale
2019-02-11 21:05:20,787 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Reprocessing replication and invalidation queues
2019-02-11 21:05:20,787 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: initializing replication queues
2019-02-11 21:05:20,788 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Will take over writing edit logs at txnid 1722
2019-02-11 21:05:20,790 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1722
2019-02-11 21:05:21,042 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 0 SyncTimes(ms): 0 61 
2019-02-11 21:05:21,059 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Initializing quota with 4 thread(s)
2019-02-11 21:05:21,063 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Quota initialization completed in 4 milliseconds
name space=11
storage space=64424509440
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2019-02-11 21:05:21,068 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-02-11 21:05:21,134 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 160
2019-02-11 21:05:21,134 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2019-02-11 21:05:21,134 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2019-02-11 21:05:21,134 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2019-02-11 21:05:21,135 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2019-02-11 21:05:21,135 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 347 msec
2019-02-11 21:05:41,734 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742353_1529, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile1._COPYING_
2019-02-11 21:05:44,920 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742354_1530, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-02-11 21:05:48,222 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742355_1531, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-02-11 21:05:48,747 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742356_1532, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-02-11 21:05:49,223 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742357_1533, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-02-11 21:05:49,668 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742358_1534, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-02-11 21:05:53,315 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742359_1535, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-02-11 21:05:53,787 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742360_1536, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-02-11 21:05:54,226 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742361_1537, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-02-11 21:05:54,869 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742362_1538, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-02-11 21:05:55,788 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2019-02-11 21:05:55,790 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at clnode024.clemson.cloudlab.us/130.127.133.33
************************************************************/
2019-02-11 21:06:12,823 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode024.clemson.cloudlab.us/130.127.133.33
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-02-05T02:43Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-02-11 21:06:12,833 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-11 21:06:12,838 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-02-11 21:06:13,126 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-11 21:06:13,230 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-02-11 21:06:13,230 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-02-11 21:06:13,282 INFO org.apache.hadoop.hdfs.server.namenode.NameNodeUtils: fs.defaultFS is hdfs://mycluster
2019-02-11 21:06:13,282 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients should use mycluster to access this namenode/service.
2019-02-11 21:06:13,442 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-02-11 21:06:13,470 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://node-1-link-0:9870
2019-02-11 21:06:13,487 INFO org.eclipse.jetty.util.log: Logging initialized @1174ms
2019-02-11 21:06:13,595 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-11 21:06:13,609 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-02-11 21:06:13,619 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-11 21:06:13,622 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-02-11 21:06:13,622 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-11 21:06:13,623 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-11 21:06:13,649 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-02-11 21:06:13,649 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-02-11 21:06:13,658 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 9870
2019-02-11 21:06:13,660 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2019-02-11 21:06:13,695 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c137fd5{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-02-11 21:06:13,696 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d9d0818{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-02-11 21:06:13,770 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@376a0d86{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-02-11 21:06:13,793 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@536dbea0{HTTP/1.1,[http/1.1]}{node-1-link-0:9870}
2019-02-11 21:06:13,793 INFO org.eclipse.jetty.server.Server: Started @1482ms
2019-02-11 21:06:14,140 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-11 21:06:14,197 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2019-02-11 21:06:14,212 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2019-02-11 21:06:14,214 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2019-02-11 21:06:14,216 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2019-02-11 21:06:14,223 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-02-11 21:06:14,223 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-02-11 21:06:14,223 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = false
2019-02-11 21:06:14,223 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID: mycluster
2019-02-11 21:06:14,223 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: true
2019-02-11 21:06:14,266 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-02-11 21:06:14,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-02-11 21:06:14,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-02-11 21:06:14,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-02-11 21:06:14,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Feb 11 21:06:14
2019-02-11 21:06:14,285 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-02-11 21:06:14,285 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 21:06:14,287 INFO org.apache.hadoop.util.GSet: 2.0% max memory 26.7 GB = 546.1 MB
2019-02-11 21:06:14,287 INFO org.apache.hadoop.util.GSet: capacity      = 2^26 = 67108864 entries
2019-02-11 21:06:14,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable = true
2019-02-11 21:06:14,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.key.update.interval=600 min(s), dfs.block.access.token.lifetime=600 min(s), dfs.encrypt.data.transfer.algorithm=null
2019-02-11 21:06:14,485 INFO org.apache.hadoop.conf.Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-02-11 21:06:14,486 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-02-11 21:06:14,486 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2019-02-11 21:06:14,486 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2019-02-11 21:06:14,486 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2019-02-11 21:06:14,486 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-02-11 21:06:14,486 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-02-11 21:06:14,486 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-02-11 21:06:14,486 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2019-02-11 21:06:14,486 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-02-11 21:06:14,486 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-02-11 21:06:14,528 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-02-11 21:06:14,528 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 21:06:14,529 INFO org.apache.hadoop.util.GSet: 1.0% max memory 26.7 GB = 273.0 MB
2019-02-11 21:06:14,529 INFO org.apache.hadoop.util.GSet: capacity      = 2^25 = 33554432 entries
2019-02-11 21:06:14,602 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-02-11 21:06:14,602 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: POSIX ACL inheritance enabled? true
2019-02-11 21:06:14,602 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-02-11 21:06:14,602 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2019-02-11 21:06:14,609 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-02-11 21:06:14,612 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: SkipList is disabled
2019-02-11 21:06:14,617 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-02-11 21:06:14,617 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 21:06:14,617 INFO org.apache.hadoop.util.GSet: 0.25% max memory 26.7 GB = 68.3 MB
2019-02-11 21:06:14,617 INFO org.apache.hadoop.util.GSet: capacity      = 2^23 = 8388608 entries
2019-02-11 21:06:14,645 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-02-11 21:06:14,645 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-02-11 21:06:14,645 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-02-11 21:06:14,649 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-02-11 21:06:14,649 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-02-11 21:06:14,652 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-02-11 21:06:14,652 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 21:06:14,652 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-02-11 21:06:14,652 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-02-11 21:06:14,724 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 13742@clnode024.clemson.cloudlab.us
2019-02-11 21:06:15,998 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-02-11 21:06:16,061 INFO org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager: Enable the erasure coding policy RS-6-3-1024k
2019-02-11 21:06:16,063 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-02-11 21:06:16,093 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-02-11 21:06:16,093 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-02-11 21:06:16,098 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1cbf6e72 expecting start txid #1
2019-02-11 21:06:16,099 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:06:16,102 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:06:16,102 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:06:16,216 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 511 loaded in 0 seconds
2019-02-11 21:06:16,216 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@6aecbb8d expecting start txid #512
2019-02-11 21:06:16,216 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:06:16,217 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:06:16,217 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:06:16,221 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:06:16,221 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1af146 expecting start txid #513
2019-02-11 21:06:16,221 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:06:16,221 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:06:16,221 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:06:16,226 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:06:16,226 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4da602fc expecting start txid #514
2019-02-11 21:06:16,226 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:06:16,226 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:06:16,226 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:06:16,229 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:06:16,230 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@2a8d39c4 expecting start txid #515
2019-02-11 21:06:16,230 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:06:16,230 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:06:16,230 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:06:16,269 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 22104 edits # 419 loaded in 0 seconds
2019-02-11 21:06:16,269 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@25b2cfcb expecting start txid #934
2019-02-11 21:06:16,269 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:06:16,269 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:06:16,269 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:06:16,276 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 5481 edits # 105 loaded in 0 seconds
2019-02-11 21:06:16,276 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@72758afa expecting start txid #1039
2019-02-11 21:06:16,276 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:06:16,276 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:06:16,276 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:06:16,284 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 8402 edits # 160 loaded in 0 seconds
2019-02-11 21:06:16,284 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@fb9c7aa expecting start txid #1199
2019-02-11 21:06:16,284 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:06:16,284 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:06:16,284 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:06:16,297 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 13730 edits # 261 loaded in 0 seconds
2019-02-11 21:06:16,297 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4c398c80 expecting start txid #1460
2019-02-11 21:06:16,298 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:06:16,298 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:06:16,298 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:06:16,307 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 13746 edits # 261 loaded in 0 seconds
2019-02-11 21:06:16,307 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@7fc6de5b expecting start txid #1721
2019-02-11 21:06:16,307 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:06:16,308 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:06:16,308 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:06:16,311 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:06:16,311 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@21baa903 expecting start txid #1722
2019-02-11 21:06:16,311 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:06:16,311 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:06:16,311 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:06:16,316 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 33 loaded in 0 seconds
2019-02-11 21:06:16,316 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-02-11 21:06:16,316 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-02-11 21:06:16,316 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 1656 msecs
2019-02-11 21:06:16,501 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to node-1-link-0:8020
2019-02-11 21:06:16,506 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-02-11 21:06:16,518 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2019-02-11 21:06:16,710 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-02-11 21:06:16,721 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 1
2019-02-11 21:06:16,730 INFO org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Updating block keys
2019-02-11 21:06:16,734 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2019-02-11 21:06:16,763 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-11 21:06:16,763 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2019-02-11 21:06:16,767 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: node-1-link-0/10.10.1.4:8020
2019-02-11 21:06:16,769 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for standby state
2019-02-11 21:06:16,777 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Will roll logs on active node every 120 seconds.
2019-02-11 21:06:16,783 INFO org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-0-link-0:9870]
Serving checkpoints at http://node-1-link-0:9870
2019-02-11 21:06:17,488 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.5:9866, datanodeUuid=008b8be5-6168-40fe-8b3e-5a79eadd8b91, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471) storage 008b8be5-6168-40fe-8b3e-5a79eadd8b91
2019-02-11 21:06:17,490 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.5:9866
2019-02-11 21:06:17,490 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 008b8be5-6168-40fe-8b3e-5a79eadd8b91 (10.10.1.5:9866).
2019-02-11 21:06:17,491 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471) storage ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9
2019-02-11 21:06:17,492 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.2:9866
2019-02-11 21:06:17,492 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9 (10.10.1.2:9866).
2019-02-11 21:06:17,492 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=d66d91c4-7ca3-4cf6-836a-6150d649a368, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471) storage d66d91c4-7ca3-4cf6-836a-6150d649a368
2019-02-11 21:06:17,492 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.3:9866
2019-02-11 21:06:17,492 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN d66d91c4-7ca3-4cf6-836a-6150d649a368 (10.10.1.3:9866).
2019-02-11 21:06:22,503 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-4b7709e7-36c2-44bc-88d1-dad941c45a98 for DN 10.10.1.2:9866
2019-02-11 21:06:22,506 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-1f221937-520c-4c45-bfe8-cacf6a0d23c9 for DN 10.10.1.5:9866
2019-02-11 21:06:22,506 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-fb2079a0-a561-41d3-a48a-27ad3ed0c749 for DN 10.10.1.3:9866
2019-02-11 21:08:16,796 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:08:16,849 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 21:09:16,857 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:09:16,861 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 21:10:16,867 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:10:16,870 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 21:11:16,876 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:11:16,880 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 21:12:16,887 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:12:16,892 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 21:13:16,898 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:13:16,901 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 21:14:16,907 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:14:16,910 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 21:15:16,916 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:15:16,919 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 21:16:16,926 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:16:16,929 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 9 needs additional 143 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 21:16:24,584 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for standby state
2019-02-11 21:16:24,584 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Edit log tailer interrupted
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:479)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
2019-02-11 21:16:24,586 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2019-02-11 21:16:24,594 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Starting recovery process for unclosed journal segments...
2019-02-11 21:16:24,633 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Successfully started new epoch 8
2019-02-11 21:16:24,633 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Beginning recovery of unclosed segment starting at txid 1755
2019-02-11 21:16:24,653 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Recovery prepare phase complete. Responses:
10.10.1.3:8485: segmentState { startTxId: 1755 endTxId: 1755 isInProgress: true } lastWriterEpoch: 7 lastCommittedTxId: 1753
10.10.1.2:8485: segmentState { startTxId: 1755 endTxId: 1755 isInProgress: true } lastWriterEpoch: 7 lastCommittedTxId: 1753
10.10.1.5:8485: segmentState { startTxId: 1755 endTxId: 1755 isInProgress: true } lastWriterEpoch: 7 lastCommittedTxId: 1753
2019-02-11 21:16:24,655 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Using longest log: 10.10.1.3:8485=segmentState {
  startTxId: 1755
  endTxId: 1755
  isInProgress: true
}
lastWriterEpoch: 7
lastCommittedTxId: 1753

2019-02-11 21:16:24,694 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-root/dfs/name/current
2019-02-11 21:16:24,723 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000001722 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000001722-0000000000000001754
2019-02-11 21:16:24,739 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Catching up to latest edits from old active before taking over writer role in edits logs
2019-02-11 21:16:24,743 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@62be7d13 expecting start txid #1755
2019-02-11 21:16:24,743 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:16:24,743 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1755
2019-02-11 21:16:24,743 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1755
2019-02-11 21:16:24,749 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:16:24,749 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Marking all datanodes as stale
2019-02-11 21:16:24,750 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Will take over writing edit logs at txnid 1756
2019-02-11 21:16:24,753 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1756
2019-02-11 21:16:25,000 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 0 SyncTimes(ms): 0 55 
2019-02-11 21:16:25,021 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Initializing quota with 4 thread(s)
2019-02-11 21:16:25,025 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Quota initialization completed in 4 milliseconds
name space=11
storage space=62008590336
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2019-02-11 21:16:25,030 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2019-02-11 21:16:25,030 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-02-11 21:16:25,725 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020, call Call#1 Retry#7 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56532
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile5. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile5. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:26,833 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call Call#1 Retry#6 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56536
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile9. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile9. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:27,542 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 8020, call Call#1 Retry#8 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56546
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile3. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile3. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:29,076 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 8020, call Call#1 Retry#9 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56550
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile6. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile6. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:29,512 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 8020, call Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.delete from 10.10.1.6:56554
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /myfile7. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:2986)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1095)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:692)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /myfile7. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more
2019-02-11 21:16:29,520 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 8020, call Call#3 Retry#1 org.apache.hadoop.hdfs.protocol.ClientProtocol.delete from 10.10.1.6:56554
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /myfile7. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:2986)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1095)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:692)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /myfile7. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more
2019-02-11 21:16:29,571 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 8020, call Call#1 Retry#10 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56558
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile10. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile10. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:30,576 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 8020, call Call#3 Retry#2 org.apache.hadoop.hdfs.protocol.ClientProtocol.delete from 10.10.1.6:56554
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /myfile7. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:2986)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1095)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:692)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /myfile7. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more
2019-02-11 21:16:32,432 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 8020, call Call#3 Retry#3 org.apache.hadoop.hdfs.protocol.ClientProtocol.delete from 10.10.1.6:56554
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /myfile7. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:2986)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1095)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:692)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /myfile7. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more
2019-02-11 21:16:33,685 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 8020, call Call#1 Retry#7 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56566
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile2. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile2. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:34,841 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 8020, call Call#3 Retry#4 org.apache.hadoop.hdfs.protocol.ClientProtocol.delete from 10.10.1.6:56554
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /myfile7. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:2986)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1095)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:692)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /myfile7. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more
2019-02-11 21:16:35,855 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 8020, call Call#1 Retry#9 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56546
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile3. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile3. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:37,082 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 8020, call Call#1 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56574
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile4. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile4. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:37,087 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 8020, call Call#1 Retry#1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56574
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile4. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile4. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:37,983 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020, call Call#1 Retry#2 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56574
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile4. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile4. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:39,193 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 8020, call Call#1 Retry#3 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56574
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile4. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile4. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:40,058 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call Call#1 Retry#11 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56580
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile10. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile10. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:41,311 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 8020, call Call#1 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56584
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile8. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile8. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:41,316 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 8020, call Call#1 Retry#1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56584
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile8. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile8. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:42,426 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 8020, call Call#1 Retry#10 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56586
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile6. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile6. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:42,741 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 8020, call Call#1 Retry#2 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56584
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile8. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile8. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:43,702 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020, call Call#3 Retry#5 org.apache.hadoop.hdfs.protocol.ClientProtocol.delete from 10.10.1.6:56554
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /myfile7. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:2986)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1095)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:692)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /myfile7. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more
2019-02-11 21:16:44,091 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 8020, call Call#1 Retry#4 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56574
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile4. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile4. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:44,207 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020, call Call#1 Retry#7 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56590
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile9. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile9. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:44,535 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call Call#1 Retry#3 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56584
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile8. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile8. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:46,842 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 8020, call Call#1 Retry#8 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56594
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile5. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile5. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:49,085 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 8020, call Call#1 Retry#4 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56584
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile8. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile8. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:51,736 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 8020, call Call#1 Retry#8 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56596
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile2. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile2. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:53,669 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 8020, call Call#1 Retry#12 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56598
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile10. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile10. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:54,358 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 8020, call Call#1 Retry#5 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56600
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile4. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile4. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:54,964 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020, call Call#1 Retry#8 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56602
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile9. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile9. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:55,618 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 8020, call Call#1 Retry#10 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56604
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile3. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile3. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:59,255 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 8020, call Call#1 Retry#11 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:56610
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile6. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile6. Name node is in safe mode.
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:16:59,830 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2019-02-11 21:16:59,832 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at clnode024.clemson.cloudlab.us/130.127.133.33
************************************************************/
2019-02-11 21:17:16,875 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode024.clemson.cloudlab.us/130.127.133.33
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-02-05T02:43Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-02-11 21:17:16,886 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-11 21:17:16,891 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-02-11 21:17:17,179 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-11 21:17:17,284 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-02-11 21:17:17,284 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-02-11 21:17:17,336 INFO org.apache.hadoop.hdfs.server.namenode.NameNodeUtils: fs.defaultFS is hdfs://mycluster
2019-02-11 21:17:17,336 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients should use mycluster to access this namenode/service.
2019-02-11 21:17:17,502 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-02-11 21:17:17,529 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://node-1-link-0:9870
2019-02-11 21:17:17,546 INFO org.eclipse.jetty.util.log: Logging initialized @1183ms
2019-02-11 21:17:17,654 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-11 21:17:17,668 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-02-11 21:17:17,678 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-11 21:17:17,681 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-02-11 21:17:17,682 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-11 21:17:17,682 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-11 21:17:17,708 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-02-11 21:17:17,708 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-02-11 21:17:17,717 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 9870
2019-02-11 21:17:17,718 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2019-02-11 21:17:17,754 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c137fd5{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-02-11 21:17:17,754 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d9d0818{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-02-11 21:17:17,830 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@376a0d86{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-02-11 21:17:17,853 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@536dbea0{HTTP/1.1,[http/1.1]}{node-1-link-0:9870}
2019-02-11 21:17:17,853 INFO org.eclipse.jetty.server.Server: Started @1491ms
2019-02-11 21:17:18,188 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-11 21:17:18,247 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2019-02-11 21:17:18,262 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2019-02-11 21:17:18,263 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2019-02-11 21:17:18,266 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2019-02-11 21:17:18,272 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-02-11 21:17:18,272 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-02-11 21:17:18,272 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2019-02-11 21:17:18,273 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID: mycluster
2019-02-11 21:17:18,273 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: true
2019-02-11 21:17:18,316 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-02-11 21:17:18,328 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-02-11 21:17:18,328 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-02-11 21:17:18,333 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-02-11 21:17:18,333 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Feb 11 21:17:18
2019-02-11 21:17:18,335 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-02-11 21:17:18,335 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 21:17:18,337 INFO org.apache.hadoop.util.GSet: 2.0% max memory 26.7 GB = 546.1 MB
2019-02-11 21:17:18,337 INFO org.apache.hadoop.util.GSet: capacity      = 2^26 = 67108864 entries
2019-02-11 21:17:18,479 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable = false
2019-02-11 21:17:18,489 INFO org.apache.hadoop.conf.Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-02-11 21:17:18,489 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-02-11 21:17:18,489 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2019-02-11 21:17:18,489 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2019-02-11 21:17:18,489 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2019-02-11 21:17:18,489 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-02-11 21:17:18,490 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-02-11 21:17:18,490 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-02-11 21:17:18,490 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2019-02-11 21:17:18,490 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-02-11 21:17:18,490 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-02-11 21:17:18,574 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-02-11 21:17:18,574 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 21:17:18,574 INFO org.apache.hadoop.util.GSet: 1.0% max memory 26.7 GB = 273.0 MB
2019-02-11 21:17:18,574 INFO org.apache.hadoop.util.GSet: capacity      = 2^25 = 33554432 entries
2019-02-11 21:17:18,647 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-02-11 21:17:18,647 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: POSIX ACL inheritance enabled? true
2019-02-11 21:17:18,648 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-02-11 21:17:18,648 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2019-02-11 21:17:18,655 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-02-11 21:17:18,657 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: SkipList is disabled
2019-02-11 21:17:18,663 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-02-11 21:17:18,663 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 21:17:18,663 INFO org.apache.hadoop.util.GSet: 0.25% max memory 26.7 GB = 68.3 MB
2019-02-11 21:17:18,663 INFO org.apache.hadoop.util.GSet: capacity      = 2^23 = 8388608 entries
2019-02-11 21:17:18,690 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-02-11 21:17:18,691 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-02-11 21:17:18,691 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-02-11 21:17:18,695 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-02-11 21:17:18,695 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-02-11 21:17:18,697 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-02-11 21:17:18,697 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 21:17:18,697 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-02-11 21:17:18,698 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-02-11 21:17:18,776 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 14175@clnode024.clemson.cloudlab.us
2019-02-11 21:17:20,255 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1249ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=346ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1009ms
2019-02-11 21:17:20,431 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-02-11 21:17:20,495 INFO org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager: Enable the erasure coding policy RS-6-3-1024k
2019-02-11 21:17:20,497 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-02-11 21:17:20,528 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-02-11 21:17:20,528 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-02-11 21:17:20,533 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@abf688e expecting start txid #1
2019-02-11 21:17:20,533 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:17:20,537 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,537 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,650 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 511 loaded in 0 seconds
2019-02-11 21:17:20,650 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@478ee483 expecting start txid #512
2019-02-11 21:17:20,650 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:17:20,650 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,651 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,654 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:17:20,654 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1a7288a3 expecting start txid #513
2019-02-11 21:17:20,654 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:17:20,654 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,654 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,659 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:17:20,660 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@2974f221 expecting start txid #514
2019-02-11 21:17:20,660 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:17:20,660 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,660 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,663 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:17:20,663 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@58fe0499 expecting start txid #515
2019-02-11 21:17:20,663 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:17:20,663 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,663 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,701 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 22104 edits # 419 loaded in 0 seconds
2019-02-11 21:17:20,701 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@686449f9 expecting start txid #934
2019-02-11 21:17:20,701 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:17:20,702 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,702 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,710 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 5481 edits # 105 loaded in 0 seconds
2019-02-11 21:17:20,710 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@665df3c6 expecting start txid #1039
2019-02-11 21:17:20,710 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:17:20,710 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,710 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,718 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 8402 edits # 160 loaded in 0 seconds
2019-02-11 21:17:20,719 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@68b6f0d6 expecting start txid #1199
2019-02-11 21:17:20,719 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:17:20,719 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,719 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,731 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 13730 edits # 261 loaded in 0 seconds
2019-02-11 21:17:20,731 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4044fb95 expecting start txid #1460
2019-02-11 21:17:20,731 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:17:20,731 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,731 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,741 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 13746 edits # 261 loaded in 0 seconds
2019-02-11 21:17:20,741 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@aa549e5 expecting start txid #1721
2019-02-11 21:17:20,741 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:17:20,741 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,741 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,744 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:17:20,745 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@36f48b4 expecting start txid #1722
2019-02-11 21:17:20,745 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:17:20,745 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,745 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,749 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 33 loaded in 0 seconds
2019-02-11 21:17:20,749 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@5c00384f expecting start txid #1755
2019-02-11 21:17:20,749 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:17:20,750 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,750 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,753 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:17:20,753 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3b7ff809 expecting start txid #1756
2019-02-11 21:17:20,753 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:17:20,754 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,754 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:17:20,757 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:17:20,757 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-02-11 21:17:20,757 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-02-11 21:17:20,757 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 2052 msecs
2019-02-11 21:17:20,939 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to node-1-link-0:8020
2019-02-11 21:17:20,944 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-02-11 21:17:20,956 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2019-02-11 21:17:21,145 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-02-11 21:17:21,154 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 1
2019-02-11 21:17:21,166 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 152 blocks to reach the threshold 0.9990 of total blocks 153.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2019-02-11 21:17:21,196 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-11 21:17:21,196 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2019-02-11 21:17:21,199 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: node-1-link-0/10.10.1.4:8020
2019-02-11 21:17:21,202 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for standby state
2019-02-11 21:17:21,207 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Will roll logs on active node every 120 seconds.
2019-02-11 21:17:21,213 INFO org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-0-link-0:9870]
Serving checkpoints at http://node-1-link-0:9870
2019-02-11 21:17:21,671 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.5:9866, datanodeUuid=008b8be5-6168-40fe-8b3e-5a79eadd8b91, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471) storage 008b8be5-6168-40fe-8b3e-5a79eadd8b91
2019-02-11 21:17:21,672 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.5:9866
2019-02-11 21:17:21,673 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 008b8be5-6168-40fe-8b3e-5a79eadd8b91 (10.10.1.5:9866).
2019-02-11 21:17:21,674 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=d66d91c4-7ca3-4cf6-836a-6150d649a368, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471) storage d66d91c4-7ca3-4cf6-836a-6150d649a368
2019-02-11 21:17:21,674 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.3:9866
2019-02-11 21:17:21,674 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN d66d91c4-7ca3-4cf6-836a-6150d649a368 (10.10.1.3:9866).
2019-02-11 21:17:21,674 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471) storage ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9
2019-02-11 21:17:21,675 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.2:9866
2019-02-11 21:17:21,675 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9 (10.10.1.2:9866).
2019-02-11 21:17:21,685 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-1f221937-520c-4c45-bfe8-cacf6a0d23c9 for DN 10.10.1.5:9866
2019-02-11 21:17:21,688 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-fb2079a0-a561-41d3-a48a-27ad3ed0c749 for DN 10.10.1.3:9866
2019-02-11 21:17:21,688 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-4b7709e7-36c2-44bc-88d1-dad941c45a98 for DN 10.10.1.2:9866
2019-02-11 21:17:21,702 INFO BlockStateChange: BLOCK* processReport 0xffffd5969f308135: Processing first storage report for DS-fb2079a0-a561-41d3-a48a-27ad3ed0c749 from datanode d66d91c4-7ca3-4cf6-836a-6150d649a368
2019-02-11 21:17:21,713 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 152 has reached the threshold 0.9990 of total blocks 153. The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2019-02-11 21:17:21,713 INFO BlockStateChange: BLOCK* processReport 0xffffd5969f308135: from storage DS-fb2079a0-a561-41d3-a48a-27ad3ed0c749 node DatanodeRegistration(10.10.1.3:9866, datanodeUuid=d66d91c4-7ca3-4cf6-836a-6150d649a368, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471), blocks: 154, hasStaleStorage: false, processing time: 11 msecs, invalidatedBlocks: 0
2019-02-11 21:17:21,714 INFO BlockStateChange: BLOCK* processReport 0x49bd66b2c05e1931: Processing first storage report for DS-4b7709e7-36c2-44bc-88d1-dad941c45a98 from datanode ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9
2019-02-11 21:17:21,717 INFO BlockStateChange: BLOCK* processReport 0x49bd66b2c05e1931: from storage DS-4b7709e7-36c2-44bc-88d1-dad941c45a98 node DatanodeRegistration(10.10.1.2:9866, datanodeUuid=ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471), blocks: 154, hasStaleStorage: false, processing time: 3 msecs, invalidatedBlocks: 0
2019-02-11 21:17:21,717 INFO BlockStateChange: BLOCK* processReport 0x30da62cabdb53946: Processing first storage report for DS-1f221937-520c-4c45-bfe8-cacf6a0d23c9 from datanode 008b8be5-6168-40fe-8b3e-5a79eadd8b91
2019-02-11 21:17:21,720 INFO BlockStateChange: BLOCK* processReport 0x30da62cabdb53946: from storage DS-1f221937-520c-4c45-bfe8-cacf6a0d23c9 node DatanodeRegistration(10.10.1.5:9866, datanodeUuid=008b8be5-6168-40fe-8b3e-5a79eadd8b91, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471), blocks: 154, hasStaleStorage: false, processing time: 3 msecs, invalidatedBlocks: 0
2019-02-11 21:17:41,718 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 153 has reached the threshold 0.9990 of total blocks 153. The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2019-02-11 21:17:51,720 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2019-02-11 21:17:51,720 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 30 secs
2019-02-11 21:17:51,720 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 3 datanodes
2019-02-11 21:17:51,720 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2019-02-11 21:19:21,225 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:19:21,477 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@55cd2196 expecting start txid #1757
2019-02-11 21:19:21,477 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:19:21,478 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1757
2019-02-11 21:19:21,478 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1757
2019-02-11 21:19:21,504 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 15817 edits # 308 loaded in 0 seconds
2019-02-11 21:21:21,512 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:21:21,701 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@7db201f2 expecting start txid #2065
2019-02-11 21:21:21,701 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:21:21,702 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 2065
2019-02-11 21:21:21,702 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 2065
2019-02-11 21:21:21,716 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 11712 edits # 215 loaded in 0 seconds
2019-02-11 21:23:21,723 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:23:21,929 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@670481be expecting start txid #2280
2019-02-11 21:23:21,929 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:23:21,929 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 2280
2019-02-11 21:23:21,929 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 2280
2019-02-11 21:23:21,943 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 11723 edits # 220 loaded in 0 seconds
2019-02-11 21:25:21,950 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:25:22,145 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@44571204 expecting start txid #2500
2019-02-11 21:25:22,145 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:25:22,145 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 2500
2019-02-11 21:25:22,145 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 2500
2019-02-11 21:25:22,159 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 13450 edits # 257 loaded in 0 seconds
2019-02-11 21:27:22,167 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:27:22,338 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@7d016039 expecting start txid #2757
2019-02-11 21:27:22,339 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:27:22,339 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 2757
2019-02-11 21:27:22,339 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 2757
2019-02-11 21:27:22,352 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 12666 edits # 249 loaded in 0 seconds
2019-02-11 21:27:28,988 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for standby state
2019-02-11 21:27:28,990 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Edit log tailer interrupted
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:479)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
2019-02-11 21:27:28,992 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2019-02-11 21:27:29,000 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Starting recovery process for unclosed journal segments...
2019-02-11 21:27:29,047 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Successfully started new epoch 10
2019-02-11 21:27:29,047 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Beginning recovery of unclosed segment starting at txid 3006
2019-02-11 21:27:29,067 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Recovery prepare phase complete. Responses:
10.10.1.3:8485: segmentState { startTxId: 3006 endTxId: 3009 isInProgress: true } lastWriterEpoch: 9 lastCommittedTxId: 3006
10.10.1.2:8485: segmentState { startTxId: 3006 endTxId: 3009 isInProgress: true } lastWriterEpoch: 9 lastCommittedTxId: 3006
10.10.1.5:8485: segmentState { startTxId: 3006 endTxId: 3009 isInProgress: true } lastWriterEpoch: 9 lastCommittedTxId: 3006
2019-02-11 21:27:29,069 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Using longest log: 10.10.1.3:8485=segmentState {
  startTxId: 3006
  endTxId: 3009
  isInProgress: true
}
lastWriterEpoch: 9
lastCommittedTxId: 3006

2019-02-11 21:27:29,110 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-root/dfs/name/current
2019-02-11 21:27:29,129 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000001756 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000001756-0000000000000001756
2019-02-11 21:27:29,147 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Catching up to latest edits from old active before taking over writer role in edits logs
2019-02-11 21:27:29,150 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@2f739595 expecting start txid #3006
2019-02-11 21:27:29,150 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:27:29,150 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 3006
2019-02-11 21:27:29,151 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 3006
2019-02-11 21:27:29,157 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 4 loaded in 0 seconds
2019-02-11 21:27:29,157 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Marking all datanodes as stale
2019-02-11 21:27:29,158 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Reprocessing replication and invalidation queues
2019-02-11 21:27:29,158 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: initializing replication queues
2019-02-11 21:27:29,158 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Will take over writing edit logs at txnid 3010
2019-02-11 21:27:29,160 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3010
2019-02-11 21:27:29,420 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 0 SyncTimes(ms): 0 53 
2019-02-11 21:27:29,440 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Initializing quota with 4 thread(s)
2019-02-11 21:27:29,444 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Quota initialization completed in 3 milliseconds
name space=11
storage space=64021856256
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2019-02-11 21:27:29,450 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-02-11 21:27:29,513 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 159
2019-02-11 21:27:29,513 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2019-02-11 21:27:29,513 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2019-02-11 21:27:29,513 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2019-02-11 21:27:29,513 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 1
2019-02-11 21:27:29,513 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 355 msec
2019-02-11 21:27:31,057 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742746_1922, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile9._COPYING_
2019-02-11 21:27:32,008 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /myfile9._COPYING_ is closed by DFSClient_NONMAPREDUCE_1253649323_1
2019-02-11 21:27:44,753 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742747_1923, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-02-11 21:27:45,664 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742748_1924, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile6._COPYING_
2019-02-11 21:27:54,606 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742749_1925, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-02-11 21:27:54,676 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742750_1926, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile6._COPYING_
2019-02-11 21:27:58,417 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742751_1927, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile6._COPYING_
2019-02-11 21:27:58,417 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742752_1928, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-02-11 21:27:59,070 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742753_1929, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile6._COPYING_
2019-02-11 21:27:59,097 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742754_1930, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-02-11 21:27:59,563 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742755_1931, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile6._COPYING_
2019-02-11 21:27:59,580 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742756_1932, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-02-11 21:28:01,828 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742757_1933, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-02-11 21:28:01,830 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742758_1934, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile6._COPYING_
2019-02-11 21:28:02,531 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742759_1935, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile6._COPYING_
2019-02-11 21:28:02,531 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742760_1936, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-02-11 21:28:03,894 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2019-02-11 21:28:03,896 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at clnode024.clemson.cloudlab.us/130.127.133.33
************************************************************/
2019-02-11 21:28:20,947 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode024.clemson.cloudlab.us/130.127.133.33
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-02-05T02:43Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-02-11 21:28:20,957 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-11 21:28:20,962 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-02-11 21:28:21,249 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-11 21:28:21,355 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-02-11 21:28:21,355 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-02-11 21:28:21,410 INFO org.apache.hadoop.hdfs.server.namenode.NameNodeUtils: fs.defaultFS is hdfs://mycluster
2019-02-11 21:28:21,411 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients should use mycluster to access this namenode/service.
2019-02-11 21:28:21,570 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-02-11 21:28:21,600 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://node-1-link-0:9870
2019-02-11 21:28:21,617 INFO org.eclipse.jetty.util.log: Logging initialized @1190ms
2019-02-11 21:28:21,724 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-11 21:28:21,738 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-02-11 21:28:21,748 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-11 21:28:21,751 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-02-11 21:28:21,751 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-11 21:28:21,752 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-11 21:28:21,778 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-02-11 21:28:21,778 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-02-11 21:28:21,787 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 9870
2019-02-11 21:28:21,788 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2019-02-11 21:28:21,824 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c137fd5{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-02-11 21:28:21,824 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d9d0818{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-02-11 21:28:21,900 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@376a0d86{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-02-11 21:28:21,921 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@536dbea0{HTTP/1.1,[http/1.1]}{node-1-link-0:9870}
2019-02-11 21:28:21,921 INFO org.eclipse.jetty.server.Server: Started @1496ms
2019-02-11 21:28:22,287 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-11 21:28:22,341 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2019-02-11 21:28:22,356 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2019-02-11 21:28:22,357 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2019-02-11 21:28:22,360 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2019-02-11 21:28:22,366 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-02-11 21:28:22,366 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-02-11 21:28:22,366 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = false
2019-02-11 21:28:22,367 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID: mycluster
2019-02-11 21:28:22,367 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: true
2019-02-11 21:28:22,409 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-02-11 21:28:22,422 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-02-11 21:28:22,422 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-02-11 21:28:22,426 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-02-11 21:28:22,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Feb 11 21:28:22
2019-02-11 21:28:22,429 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-02-11 21:28:22,429 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 21:28:22,431 INFO org.apache.hadoop.util.GSet: 2.0% max memory 26.7 GB = 546.1 MB
2019-02-11 21:28:22,431 INFO org.apache.hadoop.util.GSet: capacity      = 2^26 = 67108864 entries
2019-02-11 21:28:22,570 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable = true
2019-02-11 21:28:22,570 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.key.update.interval=600 min(s), dfs.block.access.token.lifetime=600 min(s), dfs.encrypt.data.transfer.algorithm=null
2019-02-11 21:28:22,629 INFO org.apache.hadoop.conf.Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-02-11 21:28:22,629 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-02-11 21:28:22,629 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2019-02-11 21:28:22,629 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2019-02-11 21:28:22,629 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2019-02-11 21:28:22,629 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-02-11 21:28:22,629 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-02-11 21:28:22,629 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-02-11 21:28:22,629 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2019-02-11 21:28:22,629 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-02-11 21:28:22,629 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-02-11 21:28:22,672 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-02-11 21:28:22,672 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 21:28:22,672 INFO org.apache.hadoop.util.GSet: 1.0% max memory 26.7 GB = 273.0 MB
2019-02-11 21:28:22,673 INFO org.apache.hadoop.util.GSet: capacity      = 2^25 = 33554432 entries
2019-02-11 21:28:22,747 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-02-11 21:28:22,747 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: POSIX ACL inheritance enabled? true
2019-02-11 21:28:22,747 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-02-11 21:28:22,747 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2019-02-11 21:28:22,755 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-02-11 21:28:22,757 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: SkipList is disabled
2019-02-11 21:28:22,763 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-02-11 21:28:22,763 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 21:28:22,763 INFO org.apache.hadoop.util.GSet: 0.25% max memory 26.7 GB = 68.3 MB
2019-02-11 21:28:22,763 INFO org.apache.hadoop.util.GSet: capacity      = 2^23 = 8388608 entries
2019-02-11 21:28:22,791 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-02-11 21:28:22,791 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-02-11 21:28:22,791 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-02-11 21:28:22,796 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-02-11 21:28:22,796 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-02-11 21:28:22,798 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-02-11 21:28:22,798 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 21:28:22,798 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-02-11 21:28:22,798 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-02-11 21:28:22,853 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 14623@clnode024.clemson.cloudlab.us
2019-02-11 21:28:24,364 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1289ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=352ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1034ms
2019-02-11 21:28:24,549 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-02-11 21:28:24,614 INFO org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager: Enable the erasure coding policy RS-6-3-1024k
2019-02-11 21:28:24,616 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-02-11 21:28:24,648 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-02-11 21:28:24,648 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-02-11 21:28:24,653 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1cbf6e72 expecting start txid #1
2019-02-11 21:28:24,653 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:28:24,657 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,657 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,772 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 511 loaded in 0 seconds
2019-02-11 21:28:24,772 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@6aecbb8d expecting start txid #512
2019-02-11 21:28:24,772 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:28:24,772 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,772 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,775 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:28:24,775 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1af146 expecting start txid #513
2019-02-11 21:28:24,775 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:28:24,776 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,776 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,780 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:28:24,780 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4da602fc expecting start txid #514
2019-02-11 21:28:24,780 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:28:24,780 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,781 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,784 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:28:24,784 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@2a8d39c4 expecting start txid #515
2019-02-11 21:28:24,784 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:28:24,784 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,784 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,820 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 22104 edits # 419 loaded in 0 seconds
2019-02-11 21:28:24,820 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@25b2cfcb expecting start txid #934
2019-02-11 21:28:24,820 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:28:24,820 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,820 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,827 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 5481 edits # 105 loaded in 0 seconds
2019-02-11 21:28:24,827 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@72758afa expecting start txid #1039
2019-02-11 21:28:24,827 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:28:24,827 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,828 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,836 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 8402 edits # 160 loaded in 0 seconds
2019-02-11 21:28:24,836 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@fb9c7aa expecting start txid #1199
2019-02-11 21:28:24,836 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:28:24,836 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,836 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,847 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 13730 edits # 261 loaded in 0 seconds
2019-02-11 21:28:24,847 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4c398c80 expecting start txid #1460
2019-02-11 21:28:24,847 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:28:24,847 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,847 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,857 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 13746 edits # 261 loaded in 0 seconds
2019-02-11 21:28:24,857 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@7fc6de5b expecting start txid #1721
2019-02-11 21:28:24,857 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:28:24,857 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,857 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,860 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:28:24,860 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@21baa903 expecting start txid #1722
2019-02-11 21:28:24,860 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:28:24,860 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,860 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,864 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 33 loaded in 0 seconds
2019-02-11 21:28:24,864 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@607fbe09 expecting start txid #1755
2019-02-11 21:28:24,864 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:28:24,865 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,865 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,870 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:28:24,870 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@60a2630a expecting start txid #1756
2019-02-11 21:28:24,870 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:28:24,870 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,870 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,874 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:28:24,874 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@29df4d43 expecting start txid #1757
2019-02-11 21:28:24,874 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:28:24,874 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,874 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,885 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 15817 edits # 308 loaded in 0 seconds
2019-02-11 21:28:24,885 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@5dd91bca expecting start txid #2065
2019-02-11 21:28:24,885 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:28:24,885 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,885 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,893 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 11712 edits # 215 loaded in 0 seconds
2019-02-11 21:28:24,893 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@40cb698e expecting start txid #2280
2019-02-11 21:28:24,893 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:28:24,893 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,893 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,901 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 11723 edits # 220 loaded in 0 seconds
2019-02-11 21:28:24,901 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3382f8ae expecting start txid #2500
2019-02-11 21:28:24,901 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:28:24,902 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,902 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,910 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 13450 edits # 257 loaded in 0 seconds
2019-02-11 21:28:24,910 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@60641ec8 expecting start txid #2757
2019-02-11 21:28:24,910 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:28:24,910 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,910 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,918 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 12666 edits # 249 loaded in 0 seconds
2019-02-11 21:28:24,918 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@75f65e45 expecting start txid #3006
2019-02-11 21:28:24,918 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:28:24,918 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,918 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,921 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 4 loaded in 0 seconds
2019-02-11 21:28:24,921 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@6eeade6c expecting start txid #3010
2019-02-11 21:28:24,921 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3010&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3010&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3010&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:28:24,922 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3010&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3010&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3010&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,922 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3010&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:28:24,926 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3010&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3010&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3010&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 52 loaded in 0 seconds
2019-02-11 21:28:24,926 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-02-11 21:28:24,926 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-02-11 21:28:24,926 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 2119 msecs
2019-02-11 21:28:25,112 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to node-1-link-0:8020
2019-02-11 21:28:25,117 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-02-11 21:28:25,130 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2019-02-11 21:28:25,318 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-02-11 21:28:25,327 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 2
2019-02-11 21:28:25,336 INFO org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Updating block keys
2019-02-11 21:28:25,339 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2019-02-11 21:28:25,369 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-11 21:28:25,369 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2019-02-11 21:28:25,372 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: node-1-link-0/10.10.1.4:8020
2019-02-11 21:28:25,375 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for standby state
2019-02-11 21:28:25,380 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Will roll logs on active node every 120 seconds.
2019-02-11 21:28:25,386 INFO org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-0-link-0:9870]
Serving checkpoints at http://node-1-link-0:9870
2019-02-11 21:28:25,758 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471) storage ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9
2019-02-11 21:28:25,759 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.2:9866
2019-02-11 21:28:25,760 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9 (10.10.1.2:9866).
2019-02-11 21:28:25,761 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=d66d91c4-7ca3-4cf6-836a-6150d649a368, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471) storage d66d91c4-7ca3-4cf6-836a-6150d649a368
2019-02-11 21:28:25,761 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.3:9866
2019-02-11 21:28:25,761 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN d66d91c4-7ca3-4cf6-836a-6150d649a368 (10.10.1.3:9866).
2019-02-11 21:28:25,761 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.5:9866, datanodeUuid=008b8be5-6168-40fe-8b3e-5a79eadd8b91, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471) storage 008b8be5-6168-40fe-8b3e-5a79eadd8b91
2019-02-11 21:28:25,761 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.5:9866
2019-02-11 21:28:25,762 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 008b8be5-6168-40fe-8b3e-5a79eadd8b91 (10.10.1.5:9866).
2019-02-11 21:28:30,773 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-fb2079a0-a561-41d3-a48a-27ad3ed0c749 for DN 10.10.1.3:9866
2019-02-11 21:28:30,776 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-4b7709e7-36c2-44bc-88d1-dad941c45a98 for DN 10.10.1.2:9866
2019-02-11 21:28:30,777 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-1f221937-520c-4c45-bfe8-cacf6a0d23c9 for DN 10.10.1.5:9866
2019-02-11 21:30:25,399 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:30:25,453 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 21:31:25,461 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:31:25,465 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 21:32:25,472 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:32:25,475 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 21:33:25,481 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:33:25,485 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 21:34:25,493 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:34:25,497 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 21:35:25,503 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:35:25,506 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 21:36:25,513 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:36:25,517 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 21:37:25,523 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:37:25,527 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 21:38:25,533 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:38:25,537 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:381)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:440)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4644)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1292)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 14 needs additional 125 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy19.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:366)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:363)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:513)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-02-11 21:38:33,156 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for standby state
2019-02-11 21:38:33,156 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Edit log tailer interrupted
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:479)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
2019-02-11 21:38:33,158 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2019-02-11 21:38:33,166 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Starting recovery process for unclosed journal segments...
2019-02-11 21:38:33,206 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Successfully started new epoch 12
2019-02-11 21:38:33,206 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Beginning recovery of unclosed segment starting at txid 3062
2019-02-11 21:38:33,227 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Recovery prepare phase complete. Responses:
10.10.1.3:8485: segmentState { startTxId: 3062 endTxId: 3062 isInProgress: true } lastWriterEpoch: 11 lastCommittedTxId: 3057
10.10.1.2:8485: segmentState { startTxId: 3062 endTxId: 3062 isInProgress: true } lastWriterEpoch: 11 lastCommittedTxId: 3057
10.10.1.5:8485: segmentState { startTxId: 3062 endTxId: 3062 isInProgress: true } lastWriterEpoch: 11 lastCommittedTxId: 3057
2019-02-11 21:38:33,229 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Using longest log: 10.10.1.3:8485=segmentState {
  startTxId: 3062
  endTxId: 3062
  isInProgress: true
}
lastWriterEpoch: 11
lastCommittedTxId: 3057

2019-02-11 21:38:33,263 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-root/dfs/name/current
2019-02-11 21:38:33,282 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000003010 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000003010-0000000000000003061
2019-02-11 21:38:33,300 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Catching up to latest edits from old active before taking over writer role in edits logs
2019-02-11 21:38:33,304 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@302032d6 expecting start txid #3062
2019-02-11 21:38:33,304 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3062&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3062&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:38:33,304 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3062&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3062&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 3062
2019-02-11 21:38:33,304 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3062&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 3062
2019-02-11 21:38:33,312 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3062&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3062&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:38:33,312 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Marking all datanodes as stale
2019-02-11 21:38:33,313 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Will take over writing edit logs at txnid 3063
2019-02-11 21:38:33,315 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3063
2019-02-11 21:38:33,570 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 0 SyncTimes(ms): 0 59 
2019-02-11 21:38:33,594 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Initializing quota with 4 thread(s)
2019-02-11 21:38:33,598 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Quota initialization completed in 4 milliseconds
name space=11
storage space=57176752128
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2019-02-11 21:38:33,604 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2019-02-11 21:38:33,604 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-02-11 21:38:36,167 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call Call#1 Retry#10 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60690
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile4. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile4. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:38:36,541 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 8020, call Call#1 Retry#8 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60682
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile9. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile9. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:38:39,625 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 8020, call Call#1 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60698
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile7. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile7. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:38:39,631 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 8020, call Call#1 Retry#1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60698
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile7. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile7. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:38:40,658 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020, call Call#1 Retry#7 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60702
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile2. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile2. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:38:41,121 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 8020, call Call#1 Retry#2 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60698
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile7. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile7. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:38:43,243 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 8020, call Call#1 Retry#7 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60706
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile5. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile5. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:38:43,793 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 8020, call Call#1 Retry#3 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60698
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile7. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile7. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:38:44,279 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 8020, call Call#1 Retry#8 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60710
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile10. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile10. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:38:46,589 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 8020, call Call#1 Retry#9 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60712
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile9. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile9. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:38:47,830 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 8020, call Call#1 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60716
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile1. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile1. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:38:47,836 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 8020, call Call#1 Retry#1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60716
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile1. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile1. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:38:48,994 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 8020, call Call#1 Retry#4 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60698
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile7. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile7. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:38:49,141 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020, call Call#1 Retry#2 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60716
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile1. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile1. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:38:49,176 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call Call#1 Retry#8 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60702
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile2. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile2. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:38:50,136 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020, call Call#1 Retry#11 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60724
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile4. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile4. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:38:51,712 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 8020, call Call#1 Retry#3 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60716
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile1. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile1. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:38:55,005 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call Call#1 Retry#9 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60730
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile10. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile10. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:38:55,628 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020, call Call#1 Retry#4 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60716
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile1. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile1. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:38:55,812 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call Call#1 Retry#5 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60698
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile7. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile7. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:39:00,122 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 8020, call Call#1 Retry#10 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60732
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile9. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile9. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:39:02,135 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 8020, call Call#1 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60740
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile8. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile8. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:39:02,140 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 8020, call Call#1 Retry#1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60740
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile8. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile8. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:39:03,457 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 8020, call Call#1 Retry#2 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60740
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile8. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile8. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:39:03,947 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 8020, call Call#1 Retry#8 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60746
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile5. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile5. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:39:04,616 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020, call Call#1 Retry#3 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60740
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile8. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile8. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:39:05,161 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020, call Call#1 Retry#12 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60748
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile4. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile4. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:39:07,036 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 8020, call Call#1 Retry#5 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60750
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile1. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile1. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:39:07,630 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 8020, call Call#1 Retry#9 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.10.1.6:60752
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile2. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:738)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:426)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /myfile2. Name node is in safe mode.
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node-1-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1933)
	... 11 more
2019-02-11 21:39:07,960 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2019-02-11 21:39:07,962 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at clnode024.clemson.cloudlab.us/130.127.133.33
************************************************************/
2019-02-11 21:39:24,991 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode024.clemson.cloudlab.us/130.127.133.33
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-02-05T02:43Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-02-11 21:39:25,001 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-11 21:39:25,006 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-02-11 21:39:25,291 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-11 21:39:25,394 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-02-11 21:39:25,394 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-02-11 21:39:25,446 INFO org.apache.hadoop.hdfs.server.namenode.NameNodeUtils: fs.defaultFS is hdfs://mycluster
2019-02-11 21:39:25,446 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients should use mycluster to access this namenode/service.
2019-02-11 21:39:25,612 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-02-11 21:39:25,639 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://node-1-link-0:9870
2019-02-11 21:39:25,657 INFO org.eclipse.jetty.util.log: Logging initialized @1176ms
2019-02-11 21:39:25,763 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-11 21:39:25,777 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-02-11 21:39:25,788 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-11 21:39:25,791 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-02-11 21:39:25,791 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-11 21:39:25,791 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-11 21:39:25,818 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-02-11 21:39:25,818 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-02-11 21:39:25,827 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 9870
2019-02-11 21:39:25,829 INFO org.eclipse.jetty.server.Server: jetty-9.3.19.v20170502
2019-02-11 21:39:25,864 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c137fd5{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-02-11 21:39:25,865 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d9d0818{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-02-11 21:39:25,941 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@376a0d86{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-02-11 21:39:25,961 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@536dbea0{HTTP/1.1,[http/1.1]}{node-1-link-0:9870}
2019-02-11 21:39:25,961 INFO org.eclipse.jetty.server.Server: Started @1482ms
2019-02-11 21:39:26,296 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-11 21:39:26,346 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2019-02-11 21:39:26,360 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2019-02-11 21:39:26,361 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2019-02-11 21:39:26,364 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2019-02-11 21:39:26,371 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-02-11 21:39:26,371 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-02-11 21:39:26,371 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2019-02-11 21:39:26,371 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID: mycluster
2019-02-11 21:39:26,371 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: true
2019-02-11 21:39:26,413 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-02-11 21:39:26,426 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-02-11 21:39:26,426 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-02-11 21:39:26,430 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-02-11 21:39:26,431 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Feb 11 21:39:26
2019-02-11 21:39:26,433 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-02-11 21:39:26,433 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 21:39:26,434 INFO org.apache.hadoop.util.GSet: 2.0% max memory 26.7 GB = 546.1 MB
2019-02-11 21:39:26,434 INFO org.apache.hadoop.util.GSet: capacity      = 2^26 = 67108864 entries
2019-02-11 21:39:26,571 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable = false
2019-02-11 21:39:26,580 INFO org.apache.hadoop.conf.Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-02-11 21:39:26,581 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-02-11 21:39:26,581 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2019-02-11 21:39:26,581 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2019-02-11 21:39:26,581 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2019-02-11 21:39:26,581 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-02-11 21:39:26,581 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-02-11 21:39:26,581 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-02-11 21:39:26,581 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2019-02-11 21:39:26,581 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-02-11 21:39:26,581 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-02-11 21:39:26,665 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-02-11 21:39:26,665 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 21:39:26,666 INFO org.apache.hadoop.util.GSet: 1.0% max memory 26.7 GB = 273.0 MB
2019-02-11 21:39:26,666 INFO org.apache.hadoop.util.GSet: capacity      = 2^25 = 33554432 entries
2019-02-11 21:39:26,739 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-02-11 21:39:26,739 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: POSIX ACL inheritance enabled? true
2019-02-11 21:39:26,739 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-02-11 21:39:26,739 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2019-02-11 21:39:26,746 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-02-11 21:39:26,749 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: SkipList is disabled
2019-02-11 21:39:26,754 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-02-11 21:39:26,754 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 21:39:26,754 INFO org.apache.hadoop.util.GSet: 0.25% max memory 26.7 GB = 68.3 MB
2019-02-11 21:39:26,754 INFO org.apache.hadoop.util.GSet: capacity      = 2^23 = 8388608 entries
2019-02-11 21:39:26,782 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-02-11 21:39:26,782 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-02-11 21:39:26,782 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-02-11 21:39:26,786 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-02-11 21:39:26,786 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-02-11 21:39:26,788 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-02-11 21:39:26,789 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-11 21:39:26,789 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-02-11 21:39:26,789 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-02-11 21:39:26,879 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 15061@clnode024.clemson.cloudlab.us
2019-02-11 21:39:28,413 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1297ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=307ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1097ms
2019-02-11 21:39:28,584 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-02-11 21:39:28,647 INFO org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager: Enable the erasure coding policy RS-6-3-1024k
2019-02-11 21:39:28,650 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-02-11 21:39:28,681 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-02-11 21:39:28,681 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-02-11 21:39:28,686 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@abf688e expecting start txid #1
2019-02-11 21:39:28,686 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:39:28,690 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,690 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,803 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 511 loaded in 0 seconds
2019-02-11 21:39:28,803 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@478ee483 expecting start txid #512
2019-02-11 21:39:28,803 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:39:28,804 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,804 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,806 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=512&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:39:28,806 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1a7288a3 expecting start txid #513
2019-02-11 21:39:28,806 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:39:28,807 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,807 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,812 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=513&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:39:28,812 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@2974f221 expecting start txid #514
2019-02-11 21:39:28,812 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:39:28,812 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,812 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,815 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=514&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:39:28,815 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@58fe0499 expecting start txid #515
2019-02-11 21:39:28,815 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:39:28,815 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,815 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,864 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=515&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 22104 edits # 419 loaded in 0 seconds
2019-02-11 21:39:28,864 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@686449f9 expecting start txid #934
2019-02-11 21:39:28,864 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:39:28,864 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,864 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,870 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=934&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 5481 edits # 105 loaded in 0 seconds
2019-02-11 21:39:28,870 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@665df3c6 expecting start txid #1039
2019-02-11 21:39:28,870 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:39:28,870 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,870 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,878 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1039&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 8402 edits # 160 loaded in 0 seconds
2019-02-11 21:39:28,878 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@68b6f0d6 expecting start txid #1199
2019-02-11 21:39:28,878 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:39:28,878 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,878 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,892 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 13730 edits # 261 loaded in 0 seconds
2019-02-11 21:39:28,892 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4044fb95 expecting start txid #1460
2019-02-11 21:39:28,892 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:39:28,892 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,892 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,902 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1460&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 13746 edits # 261 loaded in 0 seconds
2019-02-11 21:39:28,902 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@aa549e5 expecting start txid #1721
2019-02-11 21:39:28,902 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:39:28,902 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,903 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,907 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1721&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:39:28,907 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@36f48b4 expecting start txid #1722
2019-02-11 21:39:28,907 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:39:28,908 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,908 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,912 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1722&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 33 loaded in 0 seconds
2019-02-11 21:39:28,912 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@5c00384f expecting start txid #1755
2019-02-11 21:39:28,912 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:39:28,912 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,912 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,915 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1755&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:39:28,916 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3b7ff809 expecting start txid #1756
2019-02-11 21:39:28,916 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:39:28,916 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,916 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,919 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1756&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:39:28,919 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1bb564e2 expecting start txid #1757
2019-02-11 21:39:28,919 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:39:28,919 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,919 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,930 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 15817 edits # 308 loaded in 0 seconds
2019-02-11 21:39:28,930 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@62e6b5c8 expecting start txid #2065
2019-02-11 21:39:28,930 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:39:28,930 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,930 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,938 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2065&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 11712 edits # 215 loaded in 0 seconds
2019-02-11 21:39:28,938 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3f792b9b expecting start txid #2280
2019-02-11 21:39:28,938 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:39:28,938 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,938 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,947 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2280&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 11723 edits # 220 loaded in 0 seconds
2019-02-11 21:39:28,947 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@7b8233cd expecting start txid #2500
2019-02-11 21:39:28,947 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:39:28,947 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,947 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,955 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2500&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 13450 edits # 257 loaded in 0 seconds
2019-02-11 21:39:28,955 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4b20ca2b expecting start txid #2757
2019-02-11 21:39:28,955 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:39:28,955 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,955 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,964 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2757&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 12666 edits # 249 loaded in 0 seconds
2019-02-11 21:39:28,964 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1cbf6e72 expecting start txid #3006
2019-02-11 21:39:28,964 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:39:28,964 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,964 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,967 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3006&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 4 loaded in 0 seconds
2019-02-11 21:39:28,967 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@6aecbb8d expecting start txid #3010
2019-02-11 21:39:28,967 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3010&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3010&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3010&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:39:28,967 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3010&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3010&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3010&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,967 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3010&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,973 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3010&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3010&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3010&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 52 loaded in 0 seconds
2019-02-11 21:39:28,973 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1af146 expecting start txid #3062
2019-02-11 21:39:28,973 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3062&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3062&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3062&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:39:28,973 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3062&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3062&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3062&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,973 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3062&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,976 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3062&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3062&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3062&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:39:28,976 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4da602fc expecting start txid #3063
2019-02-11 21:39:28,976 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3063&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3063&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3063&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:39:28,976 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3063&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3063&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3063&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,976 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3063&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 1
2019-02-11 21:39:28,979 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3063&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3063&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3063&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-02-11 21:39:28,979 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-02-11 21:39:28,979 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-02-11 21:39:28,980 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 2183 msecs
2019-02-11 21:39:29,162 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to node-1-link-0:8020
2019-02-11 21:39:29,167 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-02-11 21:39:29,179 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2019-02-11 21:39:29,365 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-02-11 21:39:29,374 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 2
2019-02-11 21:39:29,385 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 139 blocks to reach the threshold 0.9990 of total blocks 140.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2019-02-11 21:39:29,415 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-11 21:39:29,415 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2019-02-11 21:39:29,418 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: node-1-link-0/10.10.1.4:8020
2019-02-11 21:39:29,420 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for standby state
2019-02-11 21:39:29,425 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Will roll logs on active node every 120 seconds.
2019-02-11 21:39:29,431 INFO org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-0-link-0:9870]
Serving checkpoints at http://node-1-link-0:9870
2019-02-11 21:39:29,931 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471) storage ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9
2019-02-11 21:39:29,932 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.2:9866
2019-02-11 21:39:29,933 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9 (10.10.1.2:9866).
2019-02-11 21:39:29,934 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=d66d91c4-7ca3-4cf6-836a-6150d649a368, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471) storage d66d91c4-7ca3-4cf6-836a-6150d649a368
2019-02-11 21:39:29,934 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.3:9866
2019-02-11 21:39:29,934 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN d66d91c4-7ca3-4cf6-836a-6150d649a368 (10.10.1.3:9866).
2019-02-11 21:39:29,934 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.5:9866, datanodeUuid=008b8be5-6168-40fe-8b3e-5a79eadd8b91, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471) storage 008b8be5-6168-40fe-8b3e-5a79eadd8b91
2019-02-11 21:39:29,934 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.1.5:9866
2019-02-11 21:39:29,935 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 008b8be5-6168-40fe-8b3e-5a79eadd8b91 (10.10.1.5:9866).
2019-02-11 21:39:29,945 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-1f221937-520c-4c45-bfe8-cacf6a0d23c9 for DN 10.10.1.5:9866
2019-02-11 21:39:29,948 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-4b7709e7-36c2-44bc-88d1-dad941c45a98 for DN 10.10.1.2:9866
2019-02-11 21:39:29,949 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-fb2079a0-a561-41d3-a48a-27ad3ed0c749 for DN 10.10.1.3:9866
2019-02-11 21:39:29,962 INFO BlockStateChange: BLOCK* processReport 0xffffd5969f308136: Processing first storage report for DS-fb2079a0-a561-41d3-a48a-27ad3ed0c749 from datanode d66d91c4-7ca3-4cf6-836a-6150d649a368
2019-02-11 21:39:29,972 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 139 has reached the threshold 0.9990 of total blocks 140. The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2019-02-11 21:39:29,973 INFO BlockStateChange: BLOCK* processReport 0xffffd5969f308136: from storage DS-fb2079a0-a561-41d3-a48a-27ad3ed0c749 node DatanodeRegistration(10.10.1.3:9866, datanodeUuid=d66d91c4-7ca3-4cf6-836a-6150d649a368, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471), blocks: 142, hasStaleStorage: false, processing time: 11 msecs, invalidatedBlocks: 0
2019-02-11 21:39:29,973 INFO BlockStateChange: BLOCK* processReport 0x30da62cabdb53947: Processing first storage report for DS-1f221937-520c-4c45-bfe8-cacf6a0d23c9 from datanode 008b8be5-6168-40fe-8b3e-5a79eadd8b91
2019-02-11 21:39:29,976 INFO BlockStateChange: BLOCK* processReport 0x30da62cabdb53947: from storage DS-1f221937-520c-4c45-bfe8-cacf6a0d23c9 node DatanodeRegistration(10.10.1.5:9866, datanodeUuid=008b8be5-6168-40fe-8b3e-5a79eadd8b91, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471), blocks: 142, hasStaleStorage: false, processing time: 3 msecs, invalidatedBlocks: 0
2019-02-11 21:39:29,976 INFO BlockStateChange: BLOCK* processReport 0x49bd66b2c05e1932: Processing first storage report for DS-4b7709e7-36c2-44bc-88d1-dad941c45a98 from datanode ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9
2019-02-11 21:39:29,978 INFO BlockStateChange: BLOCK* processReport 0x49bd66b2c05e1932: from storage DS-4b7709e7-36c2-44bc-88d1-dad941c45a98 node DatanodeRegistration(10.10.1.2:9866, datanodeUuid=ac3fcfe1-03a0-4ae6-a231-4f5366c5f9f9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1493796c-f26b-48a8-b0de-69b284a061ef;nsid=979741834;c=1549942889471), blocks: 142, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0
2019-02-11 21:39:49,977 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 140 has reached the threshold 0.9990 of total blocks 140. The number of live datanodes 3 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2019-02-11 21:39:59,978 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2019-02-11 21:39:59,978 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 30 secs
2019-02-11 21:39:59,979 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 3 datanodes
2019-02-11 21:39:59,979 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2019-02-11 21:41:29,442 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:41:29,684 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@7ac8582b expecting start txid #3064
2019-02-11 21:41:29,684 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3064&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3064&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:41:29,686 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3064&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3064&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 3064
2019-02-11 21:41:29,686 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3064&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 3064
2019-02-11 21:41:29,712 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3064&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3064&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 18506 edits # 359 loaded in 0 seconds
2019-02-11 21:43:29,720 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:43:29,891 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@fa86746 expecting start txid #3423
2019-02-11 21:43:29,891 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3423&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3423&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:43:29,892 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3423&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3423&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 3423
2019-02-11 21:43:29,892 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3423&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 3423
2019-02-11 21:43:29,903 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3423&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3423&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 8965 edits # 163 loaded in 0 seconds
2019-02-11 21:45:29,910 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:45:30,092 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@2c6340ab expecting start txid #3586
2019-02-11 21:45:30,092 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3586&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3586&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:45:30,092 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3586&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3586&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 3586
2019-02-11 21:45:30,092 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3586&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 3586
2019-02-11 21:45:30,103 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3586&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3586&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 8401 edits # 160 loaded in 0 seconds
2019-02-11 21:47:30,110 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:47:30,284 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@540e247f expecting start txid #3746
2019-02-11 21:47:30,285 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3746&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3746&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3746&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:47:30,285 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3746&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3746&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3746&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 3746
2019-02-11 21:47:30,285 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3746&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 3746
2019-02-11 21:47:30,296 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3746&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3746&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3746&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 10940 edits # 208 loaded in 0 seconds
2019-02-11 21:49:30,304 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2019-02-11 21:49:30,486 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@2a16f65d expecting start txid #3954
2019-02-11 21:49:30,486 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3954&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3954&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3954&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-02-11 21:49:30,486 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3954&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3954&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3954&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 3954
2019-02-11 21:49:30,486 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3954&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true' to transaction ID 3954
2019-02-11 21:49:30,496 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=3954&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=3954&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=3954&storageInfo=-64%3A979741834%3A1549942889471%3ACID-1493796c-f26b-48a8-b0de-69b284a061ef&inProgressOk=true of size 8288 edits # 158 loaded in 0 seconds
